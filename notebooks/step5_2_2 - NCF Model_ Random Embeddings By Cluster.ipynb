{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOjVbO8CL/t++MbJRFHCgeo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Neural Collaborative Filtering with Randomly Intialized Embeddings By Cluster\n","\n","For each customer cluster (as defined in step4_customer_segmentation), we will create a NCF model for it."],"metadata":{"id":"sXYkhGwl2pby"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J19TNojDIEkx","executionInfo":{"status":"ok","timestamp":1744605346913,"user_tz":-480,"elapsed":1296,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"49b8064a-7541-4f47-a880-3cd47dcf9d33"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import itertools\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import copy\n","import os\n","from sklearn.metrics import f1_score\n","from collections import defaultdict\n","import torch.nn.functional as F\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"],"metadata":{"id":"cyvfGjxo_HkB","executionInfo":{"status":"ok","timestamp":1744605346919,"user_tz":-480,"elapsed":10,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Step 1: Load dataset"],"metadata":{"id":"dlowKi5Y-WfV"}},{"cell_type":"markdown","source":["Previously in step1_data_preprocessing.ipynb, We have split the df_reviews dataset into training, testing and validation samples for each user, following chronological order and using the early 70% of each user's interactions for training, followed by the next 15% for validation and the last 15% for testing."],"metadata":{"id":"5cbayT1uKOyM"}},{"cell_type":"code","source":["train_data = pd.read_csv(os.path.join(data_dir,\"train_data.csv\"))\n","test_data = pd.read_csv(os.path.join(data_dir,\"test_data.csv\"))\n","val_data = pd.read_csv(os.path.join(data_dir,\"val_data.csv\"))\n","\n","df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))\n","print(df_reviews.head())"],"metadata":{"id":"ew_y5LJ3J7ln","executionInfo":{"status":"ok","timestamp":1744605357561,"user_tz":-480,"elapsed":10650,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ee9ce396-cead-458e-a96c-a2ad37caa8ca"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n"]}]},{"cell_type":"markdown","source":["# Filtering Away Customers and Products in test and val set that do not appear in its training set\n","\n","Prepares cluster-specific training, validation, and test data by:\n","- Extracting the subset of each dataframe corresponding to the given cluster ID.\n","- Mapping customer_id and product_id to 0-based index values within the cluster.\n","- Filtering val/test sets to only include users/items present in the cluster's training set.\n","\n","Prevent Cold Start problems during validation and testing. If a customer or product appears only in the validation or test set appears only in the validataion or test set, the model has never seen it before and cannot generate a valid prediction. Thus we will remove any rows in the validation or test sets which does not belong to any user in training set or the product is absent in the training set.\n","\n","We do not remove any rows from the training set."],"metadata":{"id":"608Mm0TBSq7z"}},{"cell_type":"code","source":["def prepare_cluster_data(cluster_id, train_df, val_df=None, test_df=None):\n","    # Cluster-specific train set\n","    train_cluster = train_df[train_df['cluster'] == cluster_id].reset_index(drop=True)\n","\n","    # Cluster-local mapping\n","    user2idx = {uid: idx for idx, uid in enumerate(train_cluster['customer_id'].unique())}\n","    item2idx = {pid: idx for idx, pid in enumerate(train_cluster['product_id'].unique())}\n","\n","    # Map train cluster\n","    train_cluster['user_idx'] = train_cluster['customer_id'].map(user2idx)\n","    train_cluster['item_idx'] = train_cluster['product_id'].map(item2idx)\n","\n","    val_cluster, test_cluster = None, None\n","\n","    # Prepare val cluster\n","    if val_df is not None:\n","        val_cluster = val_df[val_df['cluster'] == cluster_id].reset_index(drop=True)\n","        val_cluster = val_cluster[\n","            val_cluster['customer_id'].isin(user2idx) &\n","            val_cluster['product_id'].isin(item2idx)\n","        ].reset_index(drop=True)\n","        val_cluster['user_idx'] = val_cluster['customer_id'].map(user2idx)\n","        val_cluster['item_idx'] = val_cluster['product_id'].map(item2idx)\n","\n","    # Prepare test cluster\n","    if test_df is not None:\n","        test_cluster = test_df[test_df['cluster'] == cluster_id].reset_index(drop=True)\n","        test_cluster = test_cluster[\n","            test_cluster['customer_id'].isin(user2idx) &\n","            test_cluster['product_id'].isin(item2idx)\n","        ].reset_index(drop=True)\n","        test_cluster['user_idx'] = test_cluster['customer_id'].map(user2idx)\n","        test_cluster['item_idx'] = test_cluster['product_id'].map(item2idx)\n","\n","    return train_cluster, val_cluster, test_cluster, user2idx, item2idx"],"metadata":{"id":"aXHM27wukqrs","executionInfo":{"status":"ok","timestamp":1744605357562,"user_tz":-480,"elapsed":3,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# Create Data Loader"],"metadata":{"id":"NWUDdw36IuUt"}},{"cell_type":"code","source":["class ReviewsDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'customer_id': torch.tensor(row['user_idx'], dtype=torch.long),\n","            'product_id': torch.tensor(row['item_idx'], dtype=torch.long),\n","            'rating': torch.tensor(row['star_rating'], dtype=torch.float)\n","        }"],"metadata":{"id":"Sf5-Tjc0ItiQ","executionInfo":{"status":"ok","timestamp":1744605357564,"user_tz":-480,"elapsed":1,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Functions\n","\n","- **ndcg_at_k**: Computes the Normalized Discounted Cumulative Gain (NDCG) at rank k for a single list of relevance. If the list contains fewer than k items, it will use actual_k = min(k, len(relevances)) to ensure fair computation.\n","\n","- **mean_ndcg_user_at_k**: Computes the mean NDCG@k across all users by grouping predicted scores and relevance labels per user, sorting by prediction, and applying ndcg_at_k. For each user, their items are sorted by predicted scores, and NDCG is computed using `ndcg_at_k` with actual_k = min(k, len(user_items)).\n","\n","- **mean_precision_user_at_k**: Computes the mean Precision@k across all users.\n","Precision@k is the proportion of relevant items (e.g., rating ≥ threshold) among the top-k predicted items for each user. For each user, top-k items are selected based on predicted scores. If the user has fewer than k items, actual_k = min(k, len(user_items)) is used.  \n","  Precision is calculated as:  \n","  `precision = (# of relevant items among top-k) / actual_k`  \n","  where an item is considered relevant if `rating ≥ threshold`.\n","\n","- **mean_recall_user_at_k**: Computes the mean Recall@k across all users.\n","Recall@k is the proportion of a user's relevant items (rating ≥ threshold) that are retrieved in the top-k predicted list. For each user, top-k items are selected based on predicted scores, and recall is calculated as:  \n","  `recall = (# of relevant items among top-k) / total number of relevant items for the user`  \n","  actual_k = min(k, len(user_items)) is used to handle users with fewer than k items.\n","\n","- **mean_f1_user_at_k**:  \n","  Computes the mean F1@k across all users, where F1 combines precision and recall.  \n","  For each user, top-k items are selected (using actual_k = min(k, len(user_items))), and F1 is calculated based on binarized relevance labels (`rating ≥ threshold`).  \n","  The predicted labels are assumed to be all 1s (e.g top-k are predicted as relevant)."],"metadata":{"id":"T-aa3NCcwrso"}},{"cell_type":"code","source":["def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)\n","    actual_k = min(k, len(relevances))\n","    if actual_k == 0:\n","        return 0.0\n","    relevances = relevances[:actual_k]\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for u, pred, rel in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, rel))\n","    ndcg_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [rel for _, rel in entries_sorted]\n","        ndcg_list.append(ndcg_at_k(relevances, k))\n","    return np.mean(ndcg_list) if ndcg_list else 0.0\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    precision_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","        rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        precision_list.append(np.sum(rels) / actual_k if actual_k > 0 else 0)\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    recall_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","\n","        all_rels = [1 if r >= threshold else 0 for _, r in entries]\n","        top_k_rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        total_relevant = np.sum(all_rels)\n","\n","        if total_relevant == 0:\n","            recall = 0.0\n","        else:\n","            recall = np.sum(top_k_rels) / total_relevant\n","        recall_list.append(recall)\n","    return np.mean(recall_list) if recall_list else 0.0\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, p, l in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((p, l))\n","\n","    f1_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        y_true = [int(l >= threshold) for _, l in entries_sorted[:actual_k]]\n","        y_pred = [1] * actual_k\n","        f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","    return np.mean(f1_list) if f1_list else 0.0"],"metadata":{"id":"R2okKje4wuBV","executionInfo":{"status":"ok","timestamp":1744605357565,"user_tz":-480,"elapsed":1,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Define the NCF model with GMF and MLP\n","\n","NCF class implements a Neural Collaborative Filtering model combining:\n","\n","- **GMF (Generalized Matrix Factorization)**: Element-wise product of user and item embeddings\n","\n","- **MLP (Multi-Layer Perceptron)**: Concatenated embeddings passed through FC layers\n","\n","- **Final prediction**: Merges GMF and MLP outputs to produce a predicted rating (1 to 5 scale)"],"metadata":{"id":"LIQJaTpPeCWj"}},{"cell_type":"code","source":["class NCF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_dim, dropout_rate=0.3):\n","        super(NCF, self).__init__()\n","        # Randomly initialized embedding layers\n","        self.customer_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n","        self.product_embeddings_gmf = nn.Embedding(num_items, embedding_dim)\n","\n","        self.customer_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n","        self.product_embeddings_mlp = nn.Embedding(num_items, embedding_dim)\n","\n","        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n","        self.bn1_mlp = nn.BatchNorm1d(128)\n","        self.dropout1_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc2_mlp = nn.Linear(128, 64)\n","        self.bn2_mlp = nn.BatchNorm1d(64)\n","        self.dropout2_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n","        self.bn1_combined = nn.BatchNorm1d(128)\n","        self.dropout1_combined = nn.Dropout(dropout_rate)\n","\n","        self.fc2_combined = nn.Linear(128, 1)\n","\n","    def forward(self, customer_id, product_id):\n","        customer_emb_gmf = self.customer_embeddings_gmf(customer_id)\n","        product_emb_gmf = self.product_embeddings_gmf(product_id)\n","        gmf_output = customer_emb_gmf * product_emb_gmf\n","\n","        customer_emb_mlp = self.customer_embeddings_mlp(customer_id)\n","        product_emb_mlp = self.product_embeddings_mlp(product_id)\n","        mlp_input = torch.cat([customer_emb_mlp, product_emb_mlp], dim=-1)\n","\n","        mlp_output = F.relu(self.bn1_mlp(self.fc1_mlp(mlp_input)))\n","        mlp_output = self.dropout1_mlp(mlp_output)\n","        mlp_output = F.relu(self.bn2_mlp(self.fc2_mlp(mlp_output)))\n","        mlp_output = self.dropout2_mlp(mlp_output)\n","\n","        combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n","        combined_output = F.relu(self.bn1_combined(self.fc1_combined(combined_input)))\n","        combined_output = self.dropout1_combined(combined_output)\n","\n","        output = self.fc2_combined(combined_output)\n","        return output.squeeze() * 4 + 1"],"metadata":{"id":"SIFqdHUTeIMY","executionInfo":{"status":"ok","timestamp":1744605357574,"user_tz":-480,"elapsed":8,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["## Model Training\n","\n","\n"],"metadata":{"id":"8UAUS1P9eRhN"}},{"cell_type":"markdown","source":["### Grid Search\n","\n","The grid search algorithm here will perform an exhaustive search to identify the best combination of hyperparameters (embedding_dim, learning_rate, batch_size, dropout_rate, num_epoches) for training the NCF model.\n","\n","For each configuration:\n","\n","1. A new NCF model is instantiated with the configuration parameters.\n","\n","2. The model is trained on the training set and evaluated on the validation set.\n","\n","3. The best model state (with lowest validation loss) is stored using early stopping.\n","\n","4. The configuration and model weights are saved if it performs better than all previous configurations.\n","\n","It will then report the best-performing configuration which we will use to train the final model on the combined training and validation data before evaluating it on our test data.\n","\n"],"metadata":{"id":"_-AdHtAYrbEI"}},{"cell_type":"markdown","source":["# Seperate NCF Models for Each Cluster\n","In step4_customer_segmentation.ipynb file, we have segmented customers in 4 categories, namely steady and satified customers, power buyers, engaged shoppers, casual buyers. Customers in each cluster have distinct purchasing behaviours and preferences thus we will be building a NCF model for each cluster. This will allow us to provided tailored recommendations to different customer groups based on their past purchasing behaviour.\n","\n","The functions that we will be using are:\n","\n","1. **train_model_for_cluster** : Trains a model on a single cluster's training data (cluster_id) and validates it on its validation split. It will return the trained model, best validation loss and the best model state.\n","\n","2. **train_all_clusters** : Trains an NCF model for each cluster using their own data splits. It returns a dictionary of models, dictionary of validation losses, dictionary of model weights whose key is the cluster_id.\n","\n","2. **grid_search_by_cluster** : Perform an exhaustive search to identify the best combination of hyperparameters (embedding_dim, learning_rate, batch_size, dropout_rate, num_epoches) for the 3 cluster (using batch-averaged MSE). It will return the dictionary of best hyperparameter configuration for all clusters and the dictionary of best model state for all clusters\n","\n","3. **evaluate_cluster_models** : Evaluates trained models per cluster"],"metadata":{"id":"B4aOz0uZcdV9"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def train_model_for_cluster(cluster_id, train_df, val_df, config):\n","    train_cluster, val_cluster, _, user2idx, item2idx = prepare_cluster_data(\n","        cluster_id, train_df, val_df, test_df=None\n","    )\n","\n","    num_users = len(user2idx)\n","    num_items = len(item2idx)\n","    model = NCF(num_users, num_items, config['embedding_dim'], config['dropout_rate']).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    train_loader = DataLoader(ReviewsDataset(train_cluster), batch_size=config['batch_size'], shuffle=True)\n","    val_loader = DataLoader(ReviewsDataset(val_cluster), batch_size=config['batch_size'])\n","\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","    patience_counter = 0\n","    best_val_metrics = {}\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        for batch in train_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","\n","            optimizer.zero_grad()\n","            preds = model(user, item)\n","            loss = criterion(preds, label)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            val_all_preds, val_all_labels, val_all_users = [], [], []\n","            for batch in val_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                label = batch['rating'].to(device)\n","\n","                preds = model(user, item)\n","                loss = criterion(preds, label)\n","                val_loss += loss.item()\n","\n","                val_all_preds.extend(preds.squeeze().detach().cpu().numpy())\n","                val_all_labels.extend(label.detach().cpu().numpy())\n","                val_all_users.extend(user.detach().cpu().numpy())\n","\n","        val_all_preds = np.array(val_all_preds)\n","        val_all_labels = np.array(val_all_labels)\n","        val_all_users = np.array(val_all_users)\n","\n","        rmse_val = np.sqrt(np.mean((val_all_preds - val_all_labels)**2))\n","        ndcg_val = mean_ndcg_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10)\n","        precision_val = mean_precision_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","        recall_val = mean_recall_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","        f1_val = mean_f1_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","\n","        val_loss /= len(val_loader)\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model_state = copy.deepcopy(model.state_dict())\n","            patience_counter = 0\n","            best_val_metrics = {\n","                'rmse': rmse_val,\n","                'mse': val_loss,\n","                'ndcg@10': ndcg_val,\n","                'precision@10': precision_val,\n","                'recall@10': recall_val,\n","                'f1@10': f1_val\n","            }\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= config.get('patience', 5):\n","                break\n","\n","    model.load_state_dict(best_model_state)\n","    return model, best_val_loss, best_model_state, best_val_metrics\n","\n","def train_all_clusters(train_data, val_data, config):\n","    models, losses, states, val_metrics = {}, {}, {}, {}\n","    for cid in sorted(train_data['cluster'].unique()):\n","        model, val_loss, state, metrics = train_model_for_cluster(cid, train_data, val_data, config)\n","        models[cid], losses[cid], states[cid], val_metrics[cid] = model, val_loss, state, metrics\n","    return models, losses, states, val_metrics\n","\n","def grid_search_by_cluster(train_data, val_data, param_grid):\n","    combos = list(itertools.product(*param_grid.values()))\n","    print(\"======================= GRID SEARCH =======================\")\n","    best_config, best_loss, best_states = None, float('inf'), None\n","    for vals in combos:\n","        config = dict(zip(param_grid.keys(), vals))\n","        print(\"Running Config:\",config)\n","        _, losses, states, val_metrics = train_all_clusters(train_data, val_data, config)\n","        avg = np.mean(list(losses.values()))\n","        if avg < best_loss:\n","            best_loss, best_config, best_states, best_metrics = avg, config, states, val_metrics\n","    return best_config, best_states, best_metrics\n","\n","def evaluate_cluster_models(models, test_data):\n","    criterion = nn.MSELoss()\n","    cluster_test_metrics = {}\n","    test_results = {}\n","    for cid, model in models.items():\n","        model.eval()\n","        train_cluster, _, test_cluster, user2idx, item2idx = prepare_cluster_data(\n","            cluster_id=cid,\n","            train_df=train_data,\n","            val_df=None,\n","            test_df=test_data\n","        )\n","        loader = DataLoader(ReviewsDataset(test_cluster), batch_size=512)\n","        test_loss = 0\n","        test_preds, test_labels, test_users = [], [], []\n","        with torch.no_grad():\n","            for batch in loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                label = batch['rating'].to(device)\n","\n","                preds = model(user, item)\n","                test_preds.extend(preds.cpu().numpy())\n","                test_labels.extend(label.cpu().numpy())\n","                test_users.extend(user.cpu().numpy())\n","                test_loss += criterion(preds, label).item()\n","\n","        test_preds = np.array(test_preds)\n","        test_labels = np.array(test_labels)\n","        test_users = np.array(test_users)\n","\n","        rmse = np.sqrt(np.mean((test_preds - test_labels) ** 2))\n","        ndcg = mean_ndcg_user_at_k(test_users, test_preds, test_labels, k=10)\n","        precision = mean_precision_user_at_k(test_users, test_preds, test_labels, k=10, threshold=4)\n","        recall = mean_recall_user_at_k(test_users, test_preds, test_labels, k=10, threshold=4)\n","        f1 = mean_f1_user_at_k(test_users, test_preds, test_labels, k=10, threshold=4)\n","\n","        test_results[cid] = test_loss / len(loader)\n","        cluster_test_metrics[cid] = {\n","            'rmse': rmse,\n","            'mse': test_loss / len(loader),\n","            'ndcg@10': ndcg,\n","            'precision@10': precision,\n","            'recall@10': recall,\n","            'f1@10': f1\n","        }\n","    return cluster_test_metrics\n"],"metadata":{"id":"U3UHke8EcliD","executionInfo":{"status":"ok","timestamp":1744605357627,"user_tz":-480,"elapsed":54,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["# Final Model Training\n","\n","We will retrain the NCF model using the optimal hyperparameters identified through Grid Search, this time on the combined training and validation data. Lastly, the model is then evaluated on the test data."],"metadata":{"id":"3ywwIJV8tEzp"}},{"cell_type":"code","source":["# Step 1: Define hyperparameter grid\n","param_grid = {\n","    'embedding_dim': [16, 64],          # keep both for low vs high capacity\n","    'learning_rate': [0.001],           # pick one reliable value\n","    'batch_size': [128, 512],           # small vs large batch\n","    'dropout_rate': [0.0, 0.3],         # low vs regular dropout\n","    'num_epochs': [20, 40]              # moderate vs longer training\n","}\n","\n","def run_final_pipeline_by_clusters(train_data, val_data, test_data, param_grid, cluster_col='cluster'):\n","    output_path = os.path.join(project_dir, \"Model Results/NCF Random Embedding/Clustered Model/results_by_cluster.txt\")\n","    with open(output_path, \"w\") as f:\n","        best_config, _, best_metrics = grid_search_by_cluster(train_data, val_data, param_grid)\n","\n","        # Validation Metrics\n","        print(\"======Validation Metrics By Cluster======\\n\")\n","        f.write(\"===== Validation Metrics For Each Cluster ======\\n\")\n","        val_results = []\n","        for cid, metrics in best_metrics.items():\n","            row = {\"cluster_id\":cid}\n","            row.update(metrics)\n","            row.update(best_config)\n","            val_results.append(row)\n","            print(f\"Cluster {cid}, MSE: {metrics['mse']:.4f}, RMSE: {metrics['rmse']:.4f}, NDCG@10: {metrics['ndcg@10']}, Precision@10: {metrics['precision@10']}, Recall@10: {metrics['recall@10']}, F1@10: {metrics['f1@10']}\\n\")\n","            f.write(f\"Cluster {cid}, MSE: {metrics['mse']:.4f}, RMSE: {metrics['rmse']:.4f}, NDCG@10: {metrics['ndcg@10']}, Precision@10: {metrics['precision@10']}, Recall@10: {metrics['recall@10']}, F1@10: {metrics['f1@10']}\\n\")\n","        df_val_results = pd.DataFrame(val_results)\n","        df_val_results.to_csv(os.path.join(project_dir,\"Model Results/NCF Random Embedding/Clustered Model/val_results_by_cluster.csv\"), index=False)\n","\n","        train_val_data = pd.concat([train_data, val_data]).reset_index(drop=True)\n","        final_models, _, _ ,_ = train_all_clusters(train_val_data, val_data, best_config)\n","        test_results = evaluate_cluster_models(final_models, test_data)\n","\n","        print(\"======Test Results By Cluster======\\n\")\n","        f.write(f\"===== Test Results For Each Cluster ======\\n\")\n","        test_results_list = []\n","        for cid, test_metrics in test_results.items():\n","            row = {\"cluster_id\":cid}\n","            row.update(test_metrics)\n","            row.update(best_config)\n","            test_results_list.append(row)\n","            print(f\"Cluster {cid}, MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']}, NDCG@10: {test_metrics['ndcg@10']}, Precision@10: {test_metrics['precision@10']}, Recall@10: {test_metrics['recall@10']}, F1@10: {test_metrics['f1@10']}\\n\")\n","            f.write(f\"Cluster {cid}, MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']}, NDCG@10: {test_metrics['ndcg@10']}, Precision@10: {test_metrics['precision@10']}, Recall@10: {test_metrics['recall@10']}, F1@10: {test_metrics['f1@10']}\\n\")\n","        df_test_results = pd.DataFrame(test_results_list)\n","        df_test_results.to_csv(os.path.join(project_dir,\"Model Results/NCF Random Embedding/Clustered Model/test_results_by_cluster.csv\"), index=False)"],"metadata":{"id":"R7YVplaHrdqT","executionInfo":{"status":"ok","timestamp":1744605357634,"user_tz":-480,"elapsed":5,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["run_final_pipeline_by_clusters(train_data, val_data, test_data, param_grid)"],"metadata":{"id":"OwxXz2oqF77Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744614501237,"user_tz":-480,"elapsed":9143605,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"91ac3d0f-1e8f-4cae-f2b5-51812fe2bcb2","collapsed":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["======Validation Metrics By Cluster======\n","\n","Cluster 0, MSE: 0.8315, RMSE: 0.9121, NDCG@10: 0.9968564784461629, Precision@10: 0.8886216619671587, Recall@10: 0.8968319787692818, F1@10: 0.8913584342345331\n","\n","Cluster 1, MSE: 2.1674, RMSE: 1.4816, NDCG@10: 1.0, Precision@10: 0.6421568627450981, Recall@10: 0.6421568627450981, F1@10: 0.6421568627450981\n","\n","Cluster 2, MSE: 0.7773, RMSE: 0.8881, NDCG@10: 0.976637795672143, Precision@10: 0.9067006147352968, Recall@10: 0.953757225433526, F1@10: 0.9226680973263769\n","\n","Cluster 3, MSE: 0.5483, RMSE: 0.7342, NDCG@10: 1.0, Precision@10: 0.9339485186983972, Recall@10: 0.9339485186983972, F1@10: 0.9339485186983972\n","\n","======Test Results By Cluster======\n","\n","Cluster 0, MSE: 1.0855, RMSE: 1.0414689779281616, NDCG@10: 0.974413732659043, Precision@10: 0.8862679119168612, Recall@10: 0.9528910863892323, F1@10: 0.9088338205767131\n","\n","Cluster 1, MSE: 2.4049, RMSE: 1.5416595935821533, NDCG@10: 0.9752309223515311, Precision@10: 0.6177800100452034, Recall@10: 0.694123556002009, F1@10: 0.6432278586974719\n","\n","Cluster 2, MSE: 0.8751, RMSE: 0.9388381242752075, NDCG@10: 0.971095531534662, Precision@10: 0.8969753373662168, Recall@10: 0.9565689467969598, F1@10: 0.918231014973686\n","\n","Cluster 3, MSE: 0.6084, RMSE: 0.7823400497436523, NDCG@10: 0.9873088830412949, Precision@10: 0.9334347660707494, Recall@10: 0.9617725370863446, F1@10: 0.9428806897426145\n","\n"]}]},{"cell_type":"markdown","source":["# Check Stored Validation and Testing Results"],"metadata":{"id":"X66-PR8L9LT8"}},{"cell_type":"code","source":["# step5_2_2 - NCF Model: Random Embeddings By Cluster\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","\n","rand_emb_clustered_data_val_results = pd.read_csv(os.path.join(results_dir,\"NCF Random Embedding/Clustered Model/val_results_by_cluster.csv\"))\n","print(rand_emb_clustered_data_val_results.shape)\n","print(rand_emb_clustered_data_val_results.head())\n","\n","\n","rand_emb_by_clustered_data_test_results = pd.read_csv(os.path.join(results_dir,\"NCF Random Embedding/Clustered Model/test_results_by_cluster.csv\"))\n","print(rand_emb_by_clustered_data_test_results.shape)\n","print(rand_emb_by_clustered_data_test_results.head())\n","\n","print(\"Average Validation Metrics:\")\n","print(rand_emb_clustered_data_val_results.mean(numeric_only=True))\n","\n","print(\"\\nAverage Test Metrics:\")\n","print(rand_emb_by_clustered_data_test_results.mean(numeric_only=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W_62zgR49JcE","executionInfo":{"status":"ok","timestamp":1744614730363,"user_tz":-480,"elapsed":94,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"c785024d-4c32-4382-bd82-1ebefd43f0d5"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 12)\n","   cluster_id      rmse       mse   ndcg@10  precision@10  recall@10  \\\n","0           0  0.912108  0.831512  0.996856      0.888622   0.896832   \n","1           1  1.481553  2.167412  1.000000      0.642157   0.642157   \n","2           2  0.888148  0.777309  0.976638      0.906701   0.953757   \n","3           3  0.734164  0.548259  1.000000      0.933949   0.933949   \n","\n","      f1@10  embedding_dim  learning_rate  batch_size  dropout_rate  \\\n","0  0.891358             16          0.001         128           0.3   \n","1  0.642157             16          0.001         128           0.3   \n","2  0.922668             16          0.001         128           0.3   \n","3  0.933949             16          0.001         128           0.3   \n","\n","   num_epochs  \n","0          20  \n","1          20  \n","2          20  \n","3          20  \n","(4, 12)\n","   cluster_id      rmse       mse   ndcg@10  precision@10  recall@10  \\\n","0           0  1.041469  1.085503  0.974414      0.886268   0.952891   \n","1           1  1.541660  2.404885  0.975231      0.617780   0.694124   \n","2           2  0.938838  0.875097  0.971096      0.896975   0.956569   \n","3           3  0.782340  0.608425  0.987309      0.933435   0.961773   \n","\n","      f1@10  embedding_dim  learning_rate  batch_size  dropout_rate  \\\n","0  0.908834             16          0.001         128           0.3   \n","1  0.643228             16          0.001         128           0.3   \n","2  0.918231             16          0.001         128           0.3   \n","3  0.942881             16          0.001         128           0.3   \n","\n","   num_epochs  \n","0          20  \n","1          20  \n","2          20  \n","3          20  \n","Average Validation Metrics:\n","cluster_id         1.500000\n","rmse               1.003993\n","mse                1.081123\n","ndcg@10            0.993374\n","precision@10       0.842857\n","recall@10          0.856674\n","f1@10              0.847533\n","embedding_dim     16.000000\n","learning_rate      0.001000\n","batch_size       128.000000\n","dropout_rate       0.300000\n","num_epochs        20.000000\n","dtype: float64\n","\n","Average Test Metrics:\n","cluster_id         1.500000\n","rmse               1.076077\n","mse                1.243478\n","ndcg@10            0.977012\n","precision@10       0.833615\n","recall@10          0.891339\n","f1@10              0.853293\n","embedding_dim     16.000000\n","learning_rate      0.001000\n","batch_size       128.000000\n","dropout_rate       0.300000\n","num_epochs        20.000000\n","dtype: float64\n"]}]}]}