{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyMFCRKeIEzoZf2ARz5KOdEc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1LeXbWR2qDe","executionInfo":{"status":"ok","timestamp":1744697329073,"user_tz":-480,"elapsed":17460,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"a19c6a04-d4ee-4582-975f-e685e4f1e95b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","import itertools\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import copy\n","import os\n","from collections import defaultdict\n","import random\n","import itertools\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import ndcg_score as sk_ndcg\n","\n","# Build Custom Customer and Product Embedding\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"],"metadata":{"id":"uaClfpS825_G","executionInfo":{"status":"ok","timestamp":1744697334175,"user_tz":-480,"elapsed":5100,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Loading and splitting the data\n","\n","Previously in step1_data_preprocessing.ipynb, We have split the df_reviews dataset into training, testing and validation samples for each user, by grouping customers together then following chronological order and using the early 70% of each user's interactions for training, followed by the next 15% for validation and the last 15% for testing."],"metadata":{"id":"GLjUWRG6OSiP"}},{"cell_type":"code","source":["df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))"],"metadata":{"id":"-OoDpp3D258O","executionInfo":{"status":"ok","timestamp":1744697342066,"user_tz":-480,"elapsed":7890,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(df_reviews.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TPwRNsKtO9-_","executionInfo":{"status":"ok","timestamp":1744697342089,"user_tz":-480,"elapsed":19,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"dfc6ab96-a2cb-46e1-8afd-a25b92c4cf4b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n"]}]},{"cell_type":"markdown","source":["# Mapping IDs in df_reviews\n","\n","ID mapping for users and products in the df_reviews DataFrame, creating new columns for numerical indices corresponding to each unique user and product."],"metadata":{"id":"gsVwxStn4Ykq"}},{"cell_type":"code","source":["user2idx = {user_id: idx + 1 for idx, user_id in enumerate(df_reviews['customer_id'].unique())}\n","item2idx = {item_id: idx + 1 for idx, item_id in enumerate(df_reviews['product_id'].unique())}\n","\n","df_reviews['user_idx'] = df_reviews['customer_id'].map(user2idx)\n","df_reviews['item_idx'] = df_reviews['product_id'].map(item2idx)"],"metadata":{"id":"A3F4zb3R4cSr","executionInfo":{"status":"ok","timestamp":1744697342246,"user_tz":-480,"elapsed":154,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Splitting the data for LSTM\n","\n","- **Padding**: Sequences are padded to ensure a fixed length.\n","- **Data Splitting**: Users are split into training, validation, and test sets (64%, 16%, 20%).\n","\n","#### Formation of Training, Validation, and Testing Data\n","1. **Training Data**: From the data, we select the 80% of the interactions to generate sequences. These sequences, along with positive samples (items the user interacted with), and negative samples (randomly selected items the user did not interact with), are created and added to the training data. These sequences allow the model to learn patterns in user behavior.\n","\n","2. **Validation Data**: 16% of the interactions are used for validation. Similar to training data, we generate sequences, positive and negative samples, to evaluate how well the model generalizes to unseen data during training.\n","\n","3. **Testing Data**: 24% of the data is used for testing. For each user in the testing data, the first 80% of interactions are used as model input and last 20% as positive labels. 10 negative items are also sampled. The model does not see this data during training. Positive samples are taken from the user's reviews that were held out for testing, and multiple negative samples are randomly chosen from items the user hasn't interacted with.\n","\n","This approach ensures that the model is trained on sequences of past interactions, with positive samples (items the user interacted with) and negative samples (items the user did not interact with), preventing the model from seeing any cold-start users or products in the validation and test sets. Also ensures that the model is trained, validated, and tested on distinct sets of data, avoiding overfitting and ensuring generalizability."],"metadata":{"id":"DhyR1BeXmQPX"}},{"cell_type":"code","source":["max_len = 10 # max seq length for user-product interactions\n","min_seq_length = 5 # min 5 sequences\n","max_interactions = 20 # limits to 20 interactions\n","\n","df_sorted = df_reviews.sort_values(by=['user_idx', 'review_date'])\n","grouped = df_sorted.groupby('user_idx')\n","\n","all_items = np.array(df_sorted['item_idx'].unique())\n","filtered_users = [uid for uid, grp in grouped if len(grp) >= min_seq_length]\n","\n","# 64% train, 16% val, 20% test\n","train_val_users, test_users = train_test_split(filtered_users, test_size=0.2, random_state=42)\n","train_users, val_users = train_test_split(train_val_users, test_size=0.2, random_state=42)\n","\n","def pad_left(seq, max_len, pad_value=0):\n","    return [pad_value] * (max_len - len(seq)) + seq\n","\n","train_data, val_data, test_data = [], [], []\n","\n","for uid in filtered_users: # for each user with at least 5 interactions\n","    group = grouped.get_group(uid).sort_values('review_date').iloc[:max_interactions]\n","    items = group['item_idx'].tolist()\n","    user_item_set = set(items)\n","    neg_pool = np.setdiff1d(all_items, list(user_item_set), assume_unique=True) # for negative sampling\n","\n","    split_point = int(0.8 * len(items))\n","\n","    for i in range(1, split_point):\n","        seq = items[max(0, i - max_len):i]\n","        if len(seq) < 4:\n","            continue\n","\n","        seq_padded = pad_left(seq, max_len)\n","        pos_item = items[i]\n","        neg_item = np.random.choice(neg_pool)\n","\n","        if uid in train_users: # training set\n","            train_data.extend([\n","                (uid, seq_padded, pos_item, 1),\n","                (uid, seq_padded, neg_item, 0)\n","            ])\n","        elif uid in val_users: # validation set\n","            val_data.extend([\n","                (uid, seq_padded, pos_item, 1),\n","                (uid, seq_padded, neg_item, 0)\n","            ])\n","\n","    if uid in test_users: # test set\n","        seq = items[max(0, split_point - max_len):split_point]\n","        if len(seq) < 4:\n","            continue\n","\n","        seq_padded = pad_left(seq, max_len)\n","        # test positives\n","        pos_items = items[split_point:]\n","\n","        for pos_item in pos_items:\n","            test_data.append((uid, seq_padded, pos_item, 1))\n","\n","        # 10 negative samples\n","        neg_items = np.random.choice(neg_pool, size=10, replace=False)\n","        for neg_item in neg_items:\n","            test_data.append((uid, seq_padded, neg_item, 0))\n","\n","print(f\"Train Samples: {len(train_data)}\")\n","print(f\"Val Samples: {len(val_data)}\")\n","print(f\"Test Samples: {len(test_data)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcW3rPL7mPHb","executionInfo":{"status":"ok","timestamp":1744697549170,"user_tz":-480,"elapsed":91458,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"f321831b-08cb-4f38-9e5a-81ee8f716035"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Samples: 77002\n","Val Samples: 19204\n","Test Samples: 111834\n"]}]},{"cell_type":"code","source":["import random\n","test_user_ids = list(set([row[0] for row in test_data]))\n","# Randomly sample 20% of test sample\n","sampled_users = set(random.sample(test_user_ids, int(len(test_user_ids) * 0.2)))\n","reduced_test_data = [row for row in test_data if row[0] in sampled_users]\n","test_data = reduced_test_data\n","\n","print(f\"Reduced to {len(test_data)} test samples from {len(sampled_users)} users.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGTpg9REykrK","executionInfo":{"status":"ok","timestamp":1744697549180,"user_tz":-480,"elapsed":13,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"dace2746-1138-4bc4-efc3-61557c3031e5"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Reduced to 22310 test samples from 1906 users.\n"]}]},{"cell_type":"markdown","source":["# Creating DataLoader"],"metadata":{"id":"sR4X2hXzpesz"}},{"cell_type":"code","source":["class PurchaseSequenceDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        uid, seq, candidate, label = self.data[idx][:4]\n","        return (\n","            torch.LongTensor(seq),\n","            torch.LongTensor([candidate]),\n","            torch.FloatTensor([label]),\n","            torch.LongTensor([uid])\n","        )\n","\n","batch_size = 64\n","\n","train_dataset = PurchaseSequenceDataset(train_data)\n","val_dataset   = PurchaseSequenceDataset(val_data)\n","test_dataset  = PurchaseSequenceDataset(test_data)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"Training Data Size: {len(train_loader.dataset)}\")\n","print(f\"Validation Data Size: {len(val_loader.dataset)}\")\n","print(f\"Testing Data Size: {len(test_loader.dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G6RTXQOUjRP9","executionInfo":{"status":"ok","timestamp":1744697549196,"user_tz":-480,"elapsed":15,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"a84495de-4bcf-4848-a50b-54c65ce84d24"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data Size: 77002\n","Validation Data Size: 19204\n","Testing Data Size: 22310\n"]}]},{"cell_type":"markdown","source":["# Defining the LSTM Model\n","  \n","- **Objective**: Predict whether a user will purchase a candidate product based on past interactions (purchase history)\n","- **Inputs**: User's past purchases; Candidate product\n","- **Architecture**:  \n","  - **Embedding Layers**: Convert user and candidate item indices into dense vectors  \n","  - **LSTM**: Processes the user's purchase sequence to capture sequential dependencies  \n","  - **Hidden State**: The output from LSTM is combined with the candidate's embedding  \n","  - **Fully Connected Layer**: Computes a single logit representing the purchase probability  \n","- **Activation**: **Sigmoid**: Converts the logit into a probability between 0 and 1  \n","- **Objective**: Binary classification (purchase or not purchase)\n"],"metadata":{"id":"qA3xSivE2Udn"}},{"cell_type":"code","source":["class PurchaseCandidateLSTM(nn.Module):\n","    def __init__(self, num_items, embed_dim, hidden_dim, dropout, num_layers):\n","        super(PurchaseCandidateLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embed_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(\n","            input_size=embed_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","        self.candidate_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embed_dim, padding_idx=0) # embedding for candidate items\n","        self.fc = nn.Linear(hidden_dim + embed_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, seq, candidate):\n","        seq_embedded = self.embedding(seq)                      # (batch_size, seq_len, embed_dim)\n","        lstm_out, _ = self.lstm(seq_embedded)                   # (batch_size, seq_len, hidden_dim)\n","        seq_repr = self.dropout(lstm_out[:, -1, :])             # (batch_size, hidden_dim)\n","        candidate_emb = self.candidate_embedding(candidate).squeeze(1)  # (batch_size, embed_dim)\n","        combined = torch.cat([seq_repr, candidate_emb], dim=1)  # (batch_size, hidden_dim + embed_dim)\n","        logits = self.fc(combined)                              # (batch_size, 1)\n","        return self.sigmoid(logits)                             # (batch_size, 1)\n","\n","    def init_hidden(self, batch_size, device):\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device) # hidden state\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device) # context state\n","        return (h0, c0)"],"metadata":{"id":"jxAqPqF82Ysv","executionInfo":{"status":"ok","timestamp":1744698505089,"user_tz":-480,"elapsed":9,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Metrics Function\n","\n","- ndcg_at_k: Computes the NDCG for a single list of relevance labels at rank k.\n","- mean_ndcg_user_at_k: Computes the mean NDCG@k across all users.\n","- mean_precision_user_at_k: Computes the mean Precision@k across users, where Precision@k is the proportion of relevant items in the top k predictions.\n","- mean_recall_user_at_k: Computes the mean Recall@k across users, where Recall@k is the proportion of relevant items retrieved in the top k predictions.\n","- mean_f1_user_at_k: Computes the mean F1 score at k across users, comparing the predicted top k items with the true labels."],"metadata":{"id":"aQkRlbQjRZ4e"}},{"cell_type":"code","source":["def acc(outputs, labels):\n","    predicted = (outputs.squeeze() >= 0.5).float()\n","    return (predicted == labels.squeeze()).sum().item()\n","\n","def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)[:k]\n","    if relevances.size == 0:\n","        return 0.0\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    ndcg_scores = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [label for _, label in ranked]\n","        dcg = ndcg_at_k(relevances, k)\n","        ndcg_scores.append(dcg)\n","\n","    return np.mean(ndcg_scores)\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    precision_list = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        precision = sum(label for _, label in top_k) / k\n","        precision_list.append(precision)\n","\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    recall_list = []\n","    for user, entries in user_data.items():\n","        total_positives = sum(label for _, label in entries)\n","        if total_positives == 0:\n","            recall_list.append(0.0)\n","            continue\n","\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        retrieved_positives = sum(label for _, label in top_k)\n","        recall = retrieved_positives / total_positives\n","        recall_list.append(recall)\n","\n","    return np.mean(recall_list)\n","\n","\n","from sklearn.metrics import f1_score\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    f1_list = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        y_true = [label for _, label in top_k]\n","        y_pred = [1] * len(top_k)\n","        if sum(y_true) == 0:\n","            f1_list.append(0.0)\n","        else:\n","            f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","\n","    return np.mean(f1_list)"],"metadata":{"id":"MmAoRDSorhtO","executionInfo":{"status":"ok","timestamp":1744697430695,"user_tz":-480,"elapsed":14,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Training and Evaluation"],"metadata":{"id":"bJfLurKFb1Sm"}},{"cell_type":"code","source":["def train_lstm_model(model, train_loader, val_loader, device, epochs, lr, clip):\n","    valid_loss_min = float('inf')\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n","\n","    epoch_tr_loss, epoch_vl_loss = [], []\n","    epoch_tr_acc, epoch_vl_acc = [], []\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss, train_correct, total_train = 0, 0, 0\n","\n","        for inputs, candidate, labels, _ in train_loader:\n","            inputs, candidate, labels = inputs.to(device), candidate.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(inputs, candidate)\n","            loss = criterion(output, labels.float())\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_correct += acc(output, labels)\n","            total_train += labels.size(0)\n","\n","        train_acc = train_correct / total_train\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss, val_correct, total_val = 0, 0, 0\n","        with torch.no_grad():\n","            for inputs, candidate, labels, _ in val_loader:\n","                inputs, candidate, labels = inputs.to(device), candidate.to(device), labels.to(device)\n","                output = model(inputs, candidate)\n","                loss = criterion(output, labels.float())\n","                val_loss += loss.item()\n","                val_correct += acc(output, labels)\n","                total_val += labels.size(0)\n","\n","        val_acc = val_correct / total_val\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch+1}/{epochs}\")\n","        print(f\"\\tTrain Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n","        print(f\"\\tTrain Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")"],"metadata":{"id":"I-Ryi-MISo1t","executionInfo":{"status":"ok","timestamp":1744698507870,"user_tz":-480,"elapsed":7,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["In this method, we evaluate the LSTM-based binary classification model that is designed to predict the probability that a user wil purchase each candidate product, given their past purchase sequence. During evaluation, the model scores candidate products for users, including both positive (purchased) and negative (not purchased) items. It then ranks these candidates by predicted probability and computes ranking-based metrics: Precision@K, Recall@K, F1@K, and NDCG@K."],"metadata":{"id":"ZRDemiY8T0aj"}},{"cell_type":"code","source":["import inspect\n","def evaluate_model(model, test_loader, device, k=10, threshold=0.5, set_name=\"test\"):\n","    model.eval()\n","    correct, total = 0, 0\n","    all_users, all_preds, all_labels = [], [], []\n","    user_results = defaultdict(lambda: {'preds': [], 'labels': []})\n","    criterion = nn.BCELoss()\n","    total_loss = 0\n","\n","    expects_user_id = 'user_id' in inspect.signature(model.forward).parameters\n","\n","    with torch.no_grad():\n","        for inputs, candidate, labels, user_ids in test_loader:\n","            inputs, candidate, labels = inputs.to(device), candidate.to(device), labels.to(device)\n","\n","            if expects_user_id:\n","                output = model(inputs, candidate, user_ids)\n","            else:\n","                output = model(inputs, candidate)\n","\n","            loss = criterion(output, labels.float())\n","            total_loss += loss.item()\n","\n","            preds = output.squeeze().cpu().numpy()\n","            labs = labels.squeeze().cpu().numpy()\n","            user_ids = user_ids.squeeze().cpu().numpy()\n","\n","            preds_binary = (output.squeeze() >= 0.5).float()\n","            correct += (preds_binary == labels.squeeze()).sum().item()\n","            total += labels.size(0)\n","\n","            for u, p, l in zip(user_ids, preds, labs):\n","                user_results[int(u)]['preds'].append(p)\n","                user_results[int(u)]['labels'].append(l)\n","\n","            all_users.extend(user_ids)\n","            all_preds.extend(preds)\n","            all_labels.extend(labs)\n","\n","    # Flat metrics\n","    binarized_preds = (np.array(all_preds) >= 0.5).astype(int)\n","    flat_precision = precision_score(all_labels, binarized_preds, zero_division=0)\n","    flat_recall = recall_score(all_labels, binarized_preds, zero_division=0)\n","    flat_f1 = f1_score(all_labels, binarized_preds, zero_division=0)\n","\n","    # Ranking metrics\n","    prec_at_k = mean_precision_user_at_k(all_users, all_preds, all_labels, k=k)\n","    rec_at_k = mean_recall_user_at_k(all_users, all_preds, all_labels, k=k)\n","    f1_at_k = mean_f1_user_at_k(all_users, all_preds, all_labels, k=k)\n","    ndcg_at_k_val = mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=k)\n","\n","    # Aggregate & Log\n","    avg_loss = total_loss / len(test_loader)\n","    accuracy = correct / total\n","\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","    print(f\"Flat Precision: {flat_precision:.4f} | Recall: {flat_recall:.4f} | F1: {flat_f1:.4f}\")\n","    print(f\"Precision@{k}: {prec_at_k:.4f} | Recall@{k}: {rec_at_k:.4f} | F1@{k}: {f1_at_k:.4f} | NDCG@{k}: {ndcg_at_k_val:.4f}\")\n","\n","    metrics = {\n","        'val_acc': accuracy,\n","        'loss': avg_loss,\n","        'flat_precision': flat_precision,\n","        'flat_recall': flat_recall,\n","        'flat_f1': flat_f1,\n","        f'precision@{k}': prec_at_k,\n","        f'recall@{k}': rec_at_k,\n","        f'f1@{k}': f1_at_k,\n","        f'ndcg@{k}': ndcg_at_k_val,\n","    }\n","\n","    # Build path and save\n","    filename = f\"{set_name}_evaluation_results.csv\"\n","    output_path = os.path.join(project_dir, \"Model Results\", \"LSTM\", filename)\n","    pd.DataFrame([metrics]).to_csv(output_path, index=False)\n","    print(f\"Evaluation metrics saved to {filename}\")\n","\n","    return metrics"],"metadata":{"id":"ya771HBv25xX","executionInfo":{"status":"ok","timestamp":1744704036773,"user_tz":-480,"elapsed":3,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"lC4J99_jUZO-"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_dim = 256\n","dropout = 0.5\n","num_layers = 2\n","clip = 5\n","epochs = 10\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_items = df_reviews['item_idx'].max() + 1  # assumes 1-based indexing after .map()\n","\n","# Initialize the model\n","model = PurchaseCandidateLSTM(\n","    num_items=num_items,\n","    embed_dim=embedding_dim,\n","    hidden_dim=hidden_dim,\n","    dropout=dropout,\n","    num_layers=num_layers\n",").to(device)\n","\n","# Confirm model structure\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg6z-wfnUZtj","executionInfo":{"status":"ok","timestamp":1744698681670,"user_tz":-480,"elapsed":102,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"3c80927b-92ee-444c-d13a-f9258399e73f"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["PurchaseCandidateLSTM(\n","  (embedding): Embedding(73915, 64, padding_idx=0)\n","  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (candidate_embedding): Embedding(73915, 64, padding_idx=0)\n","  (fc): Linear(in_features=320, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"]}]},{"cell_type":"code","source":["train_lstm_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    device=device,\n","    epochs=epochs,\n","    lr=0.0005,\n","    clip=clip\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yf1tMB0Ucin","executionInfo":{"status":"ok","timestamp":1744698805831,"user_tz":-480,"elapsed":122869,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"bad8e4a9-a5dd-4c7b-d80f-cf42d2d38857"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\tTrain Loss: 0.6925 | Val Loss: 0.6891\n","\tTrain Acc: 51.78% | Val Acc: 53.76%\n","Epoch 2/10\n","\tTrain Loss: 0.6853 | Val Loss: 0.6803\n","\tTrain Acc: 54.45% | Val Acc: 56.29%\n","Epoch 3/10\n","\tTrain Loss: 0.6663 | Val Loss: 0.6541\n","\tTrain Acc: 59.18% | Val Acc: 61.33%\n","Epoch 4/10\n","\tTrain Loss: 0.6220 | Val Loss: 0.6098\n","\tTrain Acc: 66.93% | Val Acc: 67.89%\n","Epoch 5/10\n","\tTrain Loss: 0.5550 | Val Loss: 0.5680\n","\tTrain Acc: 74.59% | Val Acc: 71.64%\n","Epoch 6/10\n","\tTrain Loss: 0.4803 | Val Loss: 0.5475\n","\tTrain Acc: 78.94% | Val Acc: 72.39%\n","Epoch 7/10\n","\tTrain Loss: 0.4137 | Val Loss: 0.5519\n","\tTrain Acc: 83.02% | Val Acc: 72.44%\n","Epoch 8/10\n","\tTrain Loss: 0.3624 | Val Loss: 0.5738\n","\tTrain Acc: 86.54% | Val Acc: 72.50%\n","Epoch 9/10\n","\tTrain Loss: 0.3296 | Val Loss: 0.6008\n","\tTrain Acc: 87.00% | Val Acc: 72.48%\n","Epoch 10/10\n","\tTrain Loss: 0.3107 | Val Loss: 0.6242\n","\tTrain Acc: 86.99% | Val Acc: 72.52%\n"]}]},{"cell_type":"code","source":["evaluation_results = evaluate_model(\n","    model=model,\n","    test_loader=val_loader,\n","    device=device,\n","    k=10,  # top-k\n","    set_name=\"val\"\n",")\n","\n","evaluation_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sFsoSCojmeXP","executionInfo":{"status":"ok","timestamp":1744704050769,"user_tz":-480,"elapsed":6473,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"af61ba68-1287-4f7f-e532-882231f9e405"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.6242\n","Flat Precision: 0.7729 | Recall: 0.6378 | F1: 0.6989\n","Precision@10: 0.2955 | Recall@10: 0.9654 | F1@10: 0.6830 | NDCG@10: 0.9244\n","Evaluation metrics saved to val_evaluation_results.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["{'val_acc': 0.7252134971880858,\n"," 'loss': 0.6241879086953857,\n"," 'flat_precision': 0.7729395431023602,\n"," 'flat_recall': 0.6377837950426994,\n"," 'flat_f1': 0.6988873038516406,\n"," 'precision@10': np.float32(0.29548565),\n"," 'recall@10': np.float32(0.96543306),\n"," 'f1@10': np.float64(0.6830009745830243),\n"," 'ndcg@10': np.float64(0.9243714248734181)}"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["evaluation_results = evaluate_model(\n","    model=model,\n","    test_loader=test_loader,\n","    device=device,\n","    k=10,  # top-k\n","    set_name=\"test\"\n",")\n","\n","evaluation_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYR-Rjs-U5IW","executionInfo":{"status":"ok","timestamp":1744704055349,"user_tz":-480,"elapsed":4578,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"2f2c47ea-5461-4099-db8e-c6381fba2e1c"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.6327\n","Flat Precision: 0.3482 | Recall: 0.5852 | F1: 0.4366\n","Precision@10: 0.1572 | Recall@10: 0.9317 | F1@10: 0.2656 | NDCG@10: 0.7412\n","Evaluation metrics saved to test_evaluation_results.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["{'val_acc': 0.78000896458987,\n"," 'loss': 0.6327012527125613,\n"," 'flat_precision': 0.3482240937385573,\n"," 'flat_recall': 0.5852307692307692,\n"," 'flat_f1': 0.4366391184573003,\n"," 'precision@10': np.float32(0.1572403),\n"," 'recall@10': np.float32(0.9317069),\n"," 'f1@10': np.float64(0.2655719586464602),\n"," 'ndcg@10': np.float64(0.7412405813487741)}"]},"metadata":{},"execution_count":43}]}]}