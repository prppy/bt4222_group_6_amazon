{"cells":[{"cell_type":"markdown","metadata":{"id":"sXYkhGwl2pby"},"source":["# Neural Collaborative Filtering with Custom Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24762,"status":"ok","timestamp":1744707427839,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"J19TNojDIEkx","outputId":"222802bd-8288-45e5-f6f4-4e73a19d3e83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cyvfGjxo_HkB"},"outputs":[],"source":["import os\n","import time\n","import copy\n","import torch\n","import random\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.metrics import f1_score\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Set random seed\n","random.seed(42)\n","np.random.seed(42)\n","torch.manual_seed(42)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"]},{"cell_type":"markdown","metadata":{"id":"dlowKi5Y-WfV"},"source":["# Step 1: Load dataset"]},{"cell_type":"markdown","metadata":{"id":"5cbayT1uKOyM"},"source":["Previously in step1_data_preprocessing.ipynb, We have split the df_reviews dataset into training, testing and validation samples for each user, following chronological order and using the early 70% of each user's interactions for training, followed by the next 15% for validation and the last 15% for testing.\n","\n","Now we will load the train, val and test CSVs and the filtered_and_clustered review dataset with engineered features and cluster assignments."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16188,"status":"ok","timestamp":1744708175818,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"ew_y5LJ3J7ln","outputId":"052fbe68-9891-41e3-931c-b261013b1a88"},"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n","Training Data Shape: (116086, 26)\n","Testing Data Shape: (43223, 26)\n","Validation Data Shape: (21664, 26)\n","Index(['customer_id', 'product_id', 'product_parent', 'product_title',\n","       'product_category', 'star_rating', 'helpful_votes', 'total_votes',\n","       'vine', 'verified_purchase', 'review_headline', 'review_body',\n","       'review_date', 'temporal_purchase_sequence', 'previous_purchase',\n","       'subsequent_purchase', 'time_since_last_purchase',\n","       'parent_product_average_rating', 'product_id_average_rating',\n","       'sum_helpfulvotes', 'sum_totalvotes', 'full_review', 'sentiments',\n","       'purchases_last_4_years', 'monthly_purchase_frequency', 'cluster'],\n","      dtype='object')\n","Index(['customer_id', 'product_id', 'product_parent', 'product_title',\n","       'product_category', 'star_rating', 'helpful_votes', 'total_votes',\n","       'vine', 'verified_purchase', 'review_headline', 'review_body',\n","       'review_date', 'temporal_purchase_sequence', 'previous_purchase',\n","       'subsequent_purchase', 'time_since_last_purchase',\n","       'parent_product_average_rating', 'product_id_average_rating',\n","       'sum_helpfulvotes', 'sum_totalvotes', 'full_review', 'sentiments',\n","       'purchases_last_4_years', 'monthly_purchase_frequency', 'cluster'],\n","      dtype='object')\n","sentiments\n","positive    287894\n","negative     49135\n","Name: count, dtype: int64\n"]}],"source":["train_data = pd.read_csv(os.path.join(data_dir,\"train_data.csv\"))\n","test_data = pd.read_csv(os.path.join(data_dir,\"test_data.csv\"))\n","val_data = pd.read_csv(os.path.join(data_dir,\"val_data.csv\"))\n","\n","df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))\n","print(df_reviews.head())\n","\n","print(f\"Training Data Shape: {train_data.shape}\")\n","print(f\"Testing Data Shape: {test_data.shape}\")\n","print(f\"Validation Data Shape: {val_data.shape}\")\n","\n","print(train_data.columns)\n","print(test_data.columns)\n","\n","print(df_reviews['sentiments'].value_counts())\n"]},{"cell_type":"markdown","metadata":{"id":"608Mm0TBSq7z"},"source":["# Filter out Customers and Products in test and val set that do not appear in training set\n","\n","Prepares cluster-specific training, validation, and test data by:\n","- Extracting the subset of each dataframe corresponding to the given cluster ID.\n","- Mapping customer_id and product_id to 0-based index values within the cluster.\n","- Filtering val/test sets to only include users/items present in the cluster's training set.\n","\n","Prevent Cold Start problems during validation and testing. If a customer or product appears only in the validation or test set appears only in the validataion or test set, the model has never seen it before and cannot generate a valid prediction. Thus we will remove any rows in the validation or test sets which does not belong to any user in training set or the product is absent in the training set.\n","\n","We do not remove any rows from the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q572_moTSr34"},"outputs":[],"source":["def prepare_cluster_data(cluster_id, train_df, val_df=None, test_df=None):\n","    train_cluster = train_df[train_df['cluster'] == cluster_id].reset_index(drop=True)\n","    user2idx = {uid: idx for idx, uid in enumerate(train_cluster['customer_id'].unique())}\n","    item2idx = {pid: idx for idx, pid in enumerate(train_cluster['product_id'].unique())}\n","\n","    train_cluster['user_idx'] = train_cluster['customer_id'].map(user2idx)\n","    train_cluster['item_idx'] = train_cluster['product_id'].map(item2idx)\n","\n","    val_cluster, test_cluster = None, None\n","\n","    if val_df is not None:\n","        val_cluster = val_df[val_df['cluster'] == cluster_id].reset_index(drop=True)\n","        val_cluster = val_cluster[\n","            val_cluster['customer_id'].isin(user2idx) &\n","            val_cluster['product_id'].isin(item2idx)\n","        ].reset_index(drop=True)\n","        val_cluster['user_idx'] = val_cluster['customer_id'].map(user2idx)\n","        val_cluster['item_idx'] = val_cluster['product_id'].map(item2idx)\n","\n","    if test_df is not None:\n","        test_cluster = test_df[test_df['cluster'] == cluster_id].reset_index(drop=True)\n","        test_cluster = test_cluster[\n","            test_cluster['customer_id'].isin(user2idx) &\n","            test_cluster['product_id'].isin(item2idx)\n","        ].reset_index(drop=True)\n","        test_cluster['user_idx'] = test_cluster['customer_id'].map(user2idx)\n","        test_cluster['item_idx'] = test_cluster['product_id'].map(item2idx)\n","\n","    return train_cluster, val_cluster, test_cluster, user2idx, item2idx"]},{"cell_type":"markdown","metadata":{"id":"NWUDdw36IuUt"},"source":["# Create Data Loader\n","Defines ReviewsDataset, a PyTorch Dataset class used for model training and evaluation. Each item returns user_idx, product_idx, and star_rating"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf5-Tjc0ItiQ"},"outputs":[],"source":["class ReviewsDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'customer_id': torch.tensor(row['user_idx'], dtype=torch.long),\n","            'product_id': torch.tensor(row['item_idx'], dtype=torch.long),\n","            'rating': torch.tensor(row['star_rating'], dtype=torch.float)\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"WpW5fl31iBx6"},"source":["# Building Customer Embeddings\n","\n","The original **df_reviews** will be used to build the custom customer and product embeddings. These custom embeddings are meant to reflect historical behaviour or characteristics of customers/products\n","\n","The customer embeddings will constructed by aggregating features within df_reviews by customer_id to find a customer's:\n"," - Purchase Frequency (Indicate how active a customer is)\n"," - Time Since Last Purchase (Indicate how active a customer is)\n"," - Average Star Rating (Overall Customer satisfaction across all of his purchases)\n"," - Total Vine Reviews (Measure of Credibility of his Reviews)\n"," - Total Helpful Votes (Measure the Credibility of his Reviews)\n"," - Total Votes (Measure the Credibility of his Reviews)\n"," - Average Sentiment (Overall Customer satisfaction across all of his purchases)\n","\n","These embeddings are more informative than a randomly intialized embedding in typical recommnedation systems. The choice of specific features injects domain knowledge into the model.\n","\n","Within the build_customer_embedding method, features are scaled and reduced via PCA to match the specified embedding_dim. They will also be cached (saved as .npy files) for reuse E.g. cust_emb_cluster_0_6.npy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uxar_lIqh6Nf"},"outputs":[],"source":["def build_customer_embeddings(df, embedding_dim, cache_path):\n","    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","    if os.path.exists(cache_path):\n","        return np.load(cache_path)\n","\n","    cust_features = df.groupby('customer_id').agg({\n","        'monthly_purchase_frequency': 'mean',\n","        'time_since_last_purchase': 'mean',\n","        'star_rating': 'mean',\n","        'vine': lambda x: (x == \"Y\").sum(),\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean()\n","    }).reset_index()\n","\n","    X = cust_features.drop(columns=['customer_id'])\n","    X_scaled = StandardScaler().fit_transform(X)\n","\n","    # Only apply PCA if embedding_dim < num_features\n","    num_features = X_scaled.shape[1]\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        embeddings = pca.fit_transform(X_scaled)\n","    elif embedding_dim == num_features:\n","        embeddings = X_scaled\n","    else:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features are available.\")\n","\n","    np.save(cache_path, embeddings)\n","    return embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"1JV0DADflaMj"},"source":["## Building Product Embeddings\n","\n","The product embeddings will constructed by aggregating features within df_reviews by product_id to find a product's:\n","- Mean Star Rating that it received (Customer satisfaction)\n","- Total Helpful Votes given to all its reviews (Quality of customer feedback)\n","- Total Votes given to all its reviews (Review Engagement by customers)\n","- Average sentiment (1 is Positive and 0 is Negative)\n","- Total Number of Vine Reviews (Number of Credible Reviews)\n","- Total Product Sales (Demand for Product)\n","\n","Within the build_product_embedding method, features are scaled and reduced via PCA to match the specified embedding_dim. They will also be cached (saved as .npy files) for reuse E.g. prod_emb_cluster_0_6.npy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbSRPfctldXM"},"outputs":[],"source":["def build_product_embeddings(df, embedding_dim, cache_path):\n","    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","    if os.path.exists(cache_path):\n","        return np.load(cache_path)\n","\n","    prod_features = df.groupby('product_id').agg({\n","        'star_rating': 'mean',\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean(),\n","        'vine': lambda x: (x == 'Y').sum(),\n","        'product_id': 'count'\n","    }).rename(columns={'product_id': 'sales_volume'}).reset_index()\n","\n","    X = prod_features.drop(columns=['product_id'])\n","    X_scaled = StandardScaler().fit_transform(X)\n","\n","    num_features = X_scaled.shape[1]\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        embeddings = pca.fit_transform(X_scaled)\n","    elif embedding_dim == num_features:\n","        embeddings = X_scaled\n","    else:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features are available.\")\n","\n","    np.save(cache_path, embeddings)\n","    return embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"LIQJaTpPeCWj"},"source":["# Define the NCF model with GMF and MLP\n","\n","NCF class implements a Neural Collaborative Filtering model combining:\n","\n","- **GMF (Generalized Matrix Factorization)**: Element-wise product of user and item embeddings\n","\n","- **MLP (Multi-Layer Perceptron)**: Concatenated embeddings passed through FC layers\n","\n","- **Final prediction**: Merges GMF and MLP outputs to produce a predicted rating (1 to 5 scale)"]},{"cell_type":"markdown","metadata":{"id":"KL4I1LYheKZh"},"source":["## Aggregate the outputs of GMF and MLP by **concatenation**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIFqdHUTeIMY"},"outputs":[],"source":["class NCF(nn.Module):\n","    def __init__(self, cust_emb_gmf, prod_emb_gmf, cust_emb_mlp, prod_emb_mlp, embedding_dim, dropout_rate=0.3):\n","        super(NCF, self).__init__()\n","        # GMF Components\n","        self.customer_embeddings_gmf = nn.Embedding.from_pretrained(cust_emb_gmf, freeze=False)\n","        self.product_embeddings_gmf = nn.Embedding.from_pretrained(prod_emb_gmf, freeze=False)\n","        # MLP Components\n","        self.customer_embeddings_mlp = nn.Embedding.from_pretrained(cust_emb_mlp, freeze=False)\n","        self.product_embeddings_mlp = nn.Embedding.from_pretrained(prod_emb_mlp, freeze=False)\n","\n","        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n","        self.bn1_mlp = nn.BatchNorm1d(128)\n","        self.dropout1_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc2_mlp = nn.Linear(128, 64)\n","        self.bn2_mlp = nn.BatchNorm1d(64)\n","        self.dropout2_mlp = nn.Dropout(dropout_rate)\n","        # Final layers\n","        self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n","        self.bn1_combined = nn.BatchNorm1d(128)\n","        self.dropout1_combined = nn.Dropout(dropout_rate)\n","\n","        self.fc2_combined = nn.Linear(128, 1)\n","\n","    def forward(self, customer_id, product_id):\n","        #GMF\n","        cust_gmf = self.customer_embeddings_gmf(customer_id)\n","        prod_gmf = self.product_embeddings_gmf(product_id)\n","        gmf_output = cust_gmf * prod_gmf\n","\n","        #MLP\n","        cust_mlp = self.customer_embeddings_mlp(customer_id)\n","        prod_mlp = self.product_embeddings_mlp(product_id)\n","        mlp_input = torch.cat([cust_mlp, prod_mlp], dim=-1)\n","\n","        mlp_output = F.relu(self.bn1_mlp(self.fc1_mlp(mlp_input)))\n","        mlp_output = self.dropout1_mlp(mlp_output)\n","\n","        mlp_output = F.relu(self.bn2_mlp(self.fc2_mlp(mlp_output)))\n","        mlp_output = self.dropout2_mlp(mlp_output)\n","\n","        # Combine GMF and MLP\n","        combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n","        combined_output = F.relu(self.bn1_combined(self.fc1_combined(combined_input)))\n","        combined_output = self.dropout1_combined(combined_output)\n","\n","        # Final layer & output scaling (1–5 range)\n","        output = self.fc2_combined(combined_output)\n","        return output.squeeze() * 4 + 1\n"]},{"cell_type":"markdown","metadata":{"id":"4o7WFC67-ogu"},"source":["# Evaluation Functions\n","\n","- **ndcg_at_k**: Computes the Normalized Discounted Cumulative Gain (NDCG) at rank k for a single list of relevance. If the list contains fewer than k items, it will use actual_k = min(k, len(relevances)) to ensure fair computation.\n","\n","- **mean_ndcg_user_at_k**: Computes the mean NDCG@k across all users by grouping predicted scores and relevance labels per user, sorting by prediction, and applying ndcg_at_k. For each user, their items are sorted by predicted scores, and NDCG is computed using `ndcg_at_k` with actual_k = min(k, len(user_items)).\n","\n","- **mean_precision_user_at_k**: Computes the mean Precision@k across all users.\n","Precision@k is the proportion of relevant items (e.g., rating ≥ threshold) among the top-k predicted items for each user. For each user, top-k items are selected based on predicted scores. If the user has fewer than k items, actual_k = min(k, len(user_items)) is used.  \n","  Precision is calculated as:  \n","  `precision = (# of relevant items among top-k) / actual_k`  \n","  where an item is considered relevant if `rating ≥ threshold`.\n","\n","- **mean_recall_user_at_k**: Computes the mean Recall@k across all users.\n","Recall@k is the proportion of a user's relevant items (rating ≥ threshold) that are retrieved in the top-k predicted list. For each user, top-k items are selected based on predicted scores, and recall is calculated as:  \n","  `recall = (# of relevant items among top-k) / total number of relevant items for the user`  \n","  actual_k = min(k, len(user_items)) is used to handle users with fewer than k items.\n","\n","- **mean_f1_user_at_k**:  \n","  Computes the mean F1@k across all users, where F1 combines precision and recall.  \n","  For each user, top-k items are selected (using actual_k = min(k, len(user_items))), and F1 is calculated based on binarized relevance labels (`rating ≥ threshold`).  \n","  The predicted labels are assumed to be all 1s (e.g top-k are predicted as relevant)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6j0Vq7P-bTu"},"outputs":[],"source":["def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)\n","    actual_k = min(k, len(relevances))\n","    if actual_k == 0:\n","        return 0.0\n","    relevances = relevances[:actual_k]\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for u, pred, rel in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, rel))\n","    ndcg_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [rel for _, rel in entries_sorted]\n","        ndcg_list.append(ndcg_at_k(relevances, k))\n","    return np.mean(ndcg_list) if ndcg_list else 0.0\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    precision_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","        rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        precision_list.append(np.sum(rels) / actual_k if actual_k > 0 else 0)\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    recall_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","\n","        all_rels = [1 if r >= threshold else 0 for _, r in entries]\n","        top_k_rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        total_relevant = np.sum(all_rels)\n","\n","        if total_relevant == 0:\n","            recall = 0.0\n","        else:\n","            recall = np.sum(top_k_rels) / total_relevant\n","        recall_list.append(recall)\n","    return np.mean(recall_list) if recall_list else 0.0\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, p, l in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((p, l))\n","\n","    f1_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        y_true = [int(l >= threshold) for _, l in entries_sorted[:actual_k]]\n","        y_pred = [1] * actual_k\n","        f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","    return np.mean(f1_list) if f1_list else 0.0"]},{"cell_type":"markdown","metadata":{"id":"_-AdHtAYrbEI"},"source":["# Grid Search\n","\n","The grid search algorithm here will perform an exhaustive search to identify the best combination of hyperparamters (embedding_dim, learning_rate, batch_size, dropout_rate, num_epoches) for training the NCF model.\n","\n","For each configuration:\n","\n","1. Custom embeddings for users and products are generated using PCA on the training data based on the current embedding_dim.\n","\n","2. A new NCF model is instantiated with the configuration parameters.\n","\n","3. The model is trained on the training set and evaluated on the validation set.\n","\n","4. The best model state (with lowest validation loss) is stored using early stopping.\n","\n","5. The configuration and model weights are saved if it performs better than all previous configurations.\n","\n","It will then report the best-performing configuratuon which we will use to train the final model on the combined training and validation data before evaluating it on our test data.\n","\n","\n","**Embeddings**\n","\n","During grid search, both customer_embedding and prod_embedding is build using the train_data. The embedding_dim is changing in each iteration of grid search and PCA must be redone with each new embedding dimension.\n","\n","Note: The maximum embedding_dim for Grid Search is limited by the number of features that I used to form the customer_embedding and product_embedding respectively.\n"]},{"cell_type":"markdown","source":["**run_grid_search()**\n","\n","- Loops through all hyperparameter combinations (embedding_dim, learning_rate, batch_size, dropout_rate, num_epochs)\n","\n","- Trains cluster model using train_data and evaluates using only val_data\n","\n","- Logs each config + cluster result to grid_search_log.csv\n","\n","- Returns best config (lowest avg MSE)\n"],"metadata":{"id":"Mbynh7iqudoV"}},{"cell_type":"code","source":["def train_and_evaluate_cluster(\n","    cluster_id, train_data, val_data, df_reviews, config, cache_dir, model_dir\n","):\n","    import torch.optim as optim\n","    from torch.utils.data import DataLoader\n","\n","    train_cluster, val_cluster, _, user2idx, item2idx = prepare_cluster_data(\n","        cluster_id, train_data, val_data, test_df=None\n","    )\n","    cust_cache_path = os.path.join(cache_dir, f\"cust_emb_cluster_{cluster_id}_{config['embedding_dim']}_train.npy\")\n","    prod_cache_path = os.path.join(cache_dir, f\"prod_emb_cluster_{cluster_id}_{config['embedding_dim']}_train.npy\")\n","\n","    cust_emb = torch.tensor(build_customer_embeddings(train_data,config['embedding_dim'], cust_cache_path), dtype=torch.float32)\n","    prod_emb = torch.tensor(build_product_embeddings(train_data,config['embedding_dim'], prod_cache_path), dtype=torch.float32)\n","\n","    model = NCF(cust_emb, prod_emb, cust_emb.clone(), prod_emb.clone(),\n","                embedding_dim=config['embedding_dim'],\n","                dropout_rate=config['dropout_rate']).to(device)\n","\n","    criterion = nn.MSELoss()\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","\n","    train_loader = DataLoader(ReviewsDataset(train_cluster), batch_size=config['batch_size'], shuffle=True, num_workers=2)\n","    val_loader = DataLoader(ReviewsDataset(val_cluster), batch_size=config['batch_size'], num_workers=2)\n","\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","    patience_counter = 0\n","    best_val_metrics = {}\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        for batch in train_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(user, item)\n","            loss = criterion(output, label)\n","            loss.backward()\n","            optimizer.step()\n","\n","        # Validation\n","        model.eval()\n","        val_preds, val_labels, val_users = [], [], []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                label = batch['rating'].to(device)\n","                output = model(user, item).squeeze()\n","\n","                val_preds.extend(output.cpu().numpy())\n","                val_labels.extend(label.cpu().numpy())\n","                val_users.extend(user.cpu().numpy())\n","\n","        mse_val = np.mean((np.array(val_preds) - np.array(val_labels))**2)\n","        rmse_val = np.sqrt(mse_val)\n","        ndcg_val = mean_ndcg_user_at_k(val_users, val_preds, val_labels, k=10)\n","        precision_val = mean_precision_user_at_k(val_users, val_preds, val_labels, k=10)\n","        recall_val = mean_recall_user_at_k(val_users, val_preds, val_labels, k=10)\n","        f1_val = mean_f1_user_at_k(val_users, val_preds, val_labels, k=10)\n","\n","        if mse_val < best_val_loss:\n","            best_val_loss = mse_val\n","            best_model_state = copy.deepcopy(model.state_dict())\n","            patience_counter = 0\n","            best_val_metrics = {\n","                'cluster_id': cluster_id,\n","                'mse': mse_val,\n","                'rmse': rmse_val,\n","                'ndcg@10': ndcg_val,\n","                'precision@10': precision_val,\n","                'recall@10': recall_val,\n","                'f1@10': f1_val,\n","                **config\n","            }\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= config.get('patience', 5):\n","                break\n","\n","    weight_path = os.path.join(model_dir, f\"model_cluster_{cluster_id}.pt\")\n","    torch.save(best_model_state, weight_path)\n","\n","    return best_val_metrics\n","\n","#Run grid searhc over all clusters\n","def run_grid_search(train_data, val_data, df_reviews, param_grid, cache_dir, model_dir, log_path):\n","    import itertools\n","    import csv\n","    import time\n","\n","    os.makedirs(os.path.dirname(log_path), exist_ok=True)\n","    param_combos = list(itertools.product(*param_grid.values()))\n","    param_names = list(param_grid.keys())\n","\n","    best_config = None\n","    best_cluster_val_results = None\n","    best_avg_mse = float('inf')\n","\n","    log_file = open(log_path, mode='w', newline='')\n","    logger = csv.DictWriter(log_file, fieldnames=[\"cluster_id\"] + param_names + [\"mse\", \"rmse\", \"ndcg@10\", \"precision@10\", \"recall@10\", \"f1@10\", \"train_time_sec\"])\n","    logger.writeheader()\n","\n","    for values in param_combos:\n","        config = dict(zip(param_names, values))\n","        all_metrics = []\n","        total_time = 0\n","        print(f\"Running config: {config}\")\n","\n","        for cluster_id in sorted(train_data['cluster'].unique()):\n","            start_time = time.time()\n","            metrics = train_and_evaluate_cluster(cluster_id, train_data, val_data, df_reviews, config, cache_dir, model_dir)\n","            duration = time.time() - start_time\n","            metrics['train_time_sec'] = round(duration, 2)\n","            logger.writerow(metrics)\n","            all_metrics.append(metrics)\n","            total_time += duration\n","\n","        avg_mse = np.mean([m['mse'] for m in all_metrics])\n","        if avg_mse < best_avg_mse:\n","            best_avg_mse = avg_mse\n","            best_config = config\n","            best_cluster_val_results = all_metrics\n","\n","    log_file.close()\n","    return best_config, best_cluster_val_results"],"metadata":{"id":"0orSmCQBt2lW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ywwIJV8tEzp"},"source":["### Final Model Training\n","\n","We will retrain the NCF model using the optimal hyperparameters identified through Grid Search, this time on the combined training and validation data. Lastly, the model is then evaluated on the test data.\n","\n","**Embeddings**\n","\n","The final model will be intialized using customer and product embeddings that are forming using the (train_data + val_data) as we want our final model to have the most informed embeddings for evaluation on the test set.\n","\n","\n","**final_training_all_clusters()**\n","- Trains NCF on final_train = train_data + val_data for each cluster\n","- Use best config from grid search\n","- Saves final test results to \"test_results_by_cluster.csv\"\n","- Caches final embeddings and save final model weights\n"]},{"cell_type":"code","source":["def final_training_all_clusters(final_train, test_data, df_reviews, config, cache_dir, model_dir, result_dir):\n","    os.makedirs(result_dir, exist_ok=True)\n","    val_records = []\n","    test_records = []\n","\n","    for cluster_id in sorted(final_train['cluster'].unique()):\n","        print(f\"Retraining final model for cluster {cluster_id}...\")\n","\n","        train_cluster, _, test_cluster, user2idx, item2idx = prepare_cluster_data(cluster_id, final_train, val_df=None, test_df=test_data)\n","\n","        cust_cache_path = os.path.join(cache_dir, f\"cust_emb_cluster_{cluster_id}_final.npy\")\n","        prod_cache_path = os.path.join(cache_dir, f\"prod_emb_cluster_{cluster_id}_final.npy\")\n","\n","        cust_emb = torch.tensor(\n","            build_customer_embeddings(final_train,\n","                                       config['embedding_dim'], cust_cache_path), dtype=torch.float32\n","        )\n","        prod_emb = torch.tensor(\n","            build_product_embeddings(final_train,\n","                                      config['embedding_dim'], prod_cache_path), dtype=torch.float32\n","        )\n","\n","\n","        model = NCF(cust_emb, prod_emb, cust_emb.clone(), prod_emb.clone(),\n","                    embedding_dim = config['embedding_dim'], dropout_rate=config['dropout_rate']).to(device)\n","\n","        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","        criterion = nn.MSELoss()\n","\n","        train_loader = DataLoader(ReviewsDataset(train_cluster), batch_size=config['batch_size'], shuffle=True)\n","        for epoch in range(config['num_epochs']):\n","            model.train()\n","            for batch in train_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                rating = batch['rating'].to(device)\n","                optimizer.zero_grad()\n","                preds = model(user, item)\n","                loss = criterion(preds, rating)\n","                loss.backward()\n","                optimizer.step()\n","\n","        torch.save(model.state_dict(), os.path.join(model_dir, f\"model_cluster_{cluster_id}.pt\"))\n","\n","        test_loader = DataLoader(ReviewsDataset(test_cluster), batch_size=256)\n","        model.eval()\n","        preds, labels, users = [], [], []\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                rating = batch['rating'].to(device)\n","                output = model(user, item).squeeze()\n","                preds.extend(output.cpu().numpy())\n","                labels.extend(rating.cpu().numpy())\n","                users.extend(user.cpu().numpy())\n","\n","        mse = np.mean((np.array(preds) - np.array(labels)) ** 2)\n","        rmse = np.sqrt(mse)\n","        ndcg = mean_ndcg_user_at_k(users, preds, labels, k=10)\n","        precision = mean_precision_user_at_k(users, preds, labels, k=10)\n","        recall = mean_recall_user_at_k(users, preds, labels, k=10)\n","        f1 = mean_f1_user_at_k(users, preds, labels, k=10)\n","\n","        record = {\n","            'cluster_id': cluster_id, 'mse': mse, 'rmse': rmse,\n","            'ndcg@10': ndcg, 'precision@10': precision, 'recall@10': recall, 'f1@10': f1,\n","            **config\n","        }\n","        test_records.append(record)\n","\n","    pd.DataFrame(test_records).to_csv(os.path.join(result_dir, 'test_results_by_cluster.csv'), index=False)\n","    print(\"Finished retraining and evaluation on final dataset.\")\n"],"metadata":{"id":"PIXnpvx5ucwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LRiI2H_-CJE0"},"source":["# Evaluation\n","\n","The model is evaluated on the test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPYOOQjeCK1g"},"outputs":[],"source":["def run_full_clustered_ncf_pipeline(train_data, val_data, test_data, df_reviews, param_grid,\n","                                    cache_dir, model_dir, result_dir, log_path):\n","    print(\"\\n======================= GRID SEARCH =======================\")\n","    best_config, val_results = run_grid_search(\n","        train_data, val_data, df_reviews,\n","        param_grid, cache_dir, model_dir, log_path\n","    )\n","\n","    # Save validation results\n","    val_df = pd.DataFrame(val_results)\n","    val_path = os.path.join(result_dir, 'val_results_by_cluster.csv')\n","    os.makedirs(result_dir, exist_ok=True)\n","    val_df.to_csv(val_path, index=False)\n","    print(f\"Saved best validation metrics to {val_path}\")\n","\n","    print(\"\\n================= FINAL MODEL TRAINING =================\")\n","    final_train = pd.concat([train_data, val_data]).reset_index(drop=True)\n","    final_training_all_clusters(final_train, test_data, df_reviews,\n","                                best_config, cache_dir, model_dir, result_dir)\n","\n","    print(\"\\nPipeline complete.\")"]},{"cell_type":"code","source":["param_grid = {                           # Total Combinations:\n","        'embedding_dim': [5,6],          # keep both for low vs high capacity\n","        'learning_rate': [0.001],           # pick one reliable value\n","        'batch_size': [128, 512],           # small vs large batch\n","        'dropout_rate': [0.0, 0.3],         # low vs regular dropout\n","        'num_epochs': [20, 40]              # moderate vs longer training\n","    }\n","\n","# param_grid = {\n","#     'embedding_dim': [6],\n","#     'learning_rate': [0.001],\n","#     'batch_size': [128],\n","#     'dropout_rate': [0.0],\n","#     'num_epochs': [20]\n","# }\n","\n","run_full_clustered_ncf_pipeline(\n","    train_data=train_data,\n","    val_data=val_data,\n","    test_data=test_data,\n","    df_reviews=df_reviews,\n","    param_grid=param_grid,\n","    cache_dir= project_dir + \"/\" + \"Model Results/NCF Custom Embedding/Clustered Model/cache\",\n","    model_dir= project_dir  + \"/\" + \"Model Results/NCF Custom Embedding/Clustered Model/weights\",\n","    result_dir= project_dir + \"/\" + \"Model Results/NCF Custom Embedding/Clustered Model\",\n","    log_path= project_dir  + \"/\" + \"Model Results/NCF Custom Embedding/Clustered Model/grid_search_log.csv\"\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yklSzBTgyDwY","executionInfo":{"status":"ok","timestamp":1744615941521,"user_tz":-480,"elapsed":10386694,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"c6b45a75-9368-4d31-a8a1-73db1747d5dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================= GRID SEARCH =======================\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Saved best validation metrics to /content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results/NCF Custom Embedding/Clustered Model/val_results_by_cluster.csv\n","\n","================= FINAL MODEL TRAINING =================\n","Retraining final model for cluster 0...\n","Retraining final model for cluster 1...\n","Retraining final model for cluster 2...\n","Retraining final model for cluster 3...\n","Finished retraining and evaluation on final dataset.\n","\n","Pipeline complete.\n"]}]},{"cell_type":"markdown","source":["# Check Stored Validation and Testing Results"],"metadata":{"id":"HfURaffM86GE"}},{"cell_type":"code","source":["# step5_1_2 - NCF Model: Custom Embedding By Cluster\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","\n","cust_emb_by_cluster_val_results = pd.read_csv(os.path.join(results_dir,\"NCF Custom Embedding/Clustered Model/val_results_by_cluster.csv\"))\n","print(cust_emb_by_cluster_val_results.shape)\n","print(\"==========step5_1_2 - NCF Model: Custom Embedding By Cluster Validation Results==========\")\n","display(cust_emb_by_cluster_val_results.head())\n","\n","\n","cust_emb_by_cluster_test_results = pd.read_csv(os.path.join(results_dir,\"NCF Custom Embedding/Clustered Model/test_results_by_cluster.csv\"))\n","print(cust_emb_by_cluster_test_results.shape)\n","print(\"==========step5_1_2 - NCF Model: Custom Embedding By Cluster Test Results==========\")\n","display(cust_emb_by_cluster_test_results.head())\n","\n","print(\"Average Validation Metrics:\")\n","print(cust_emb_by_cluster_val_results.mean(numeric_only=True))\n","\n","# Compute average metrics across clusters for test results\n","print(\"\\nAverage Test Metrics:\")\n","print(cust_emb_by_cluster_test_results.mean(numeric_only=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":960},"id":"Kjcm7aT388sl","executionInfo":{"status":"ok","timestamp":1744707445274,"user_tz":-480,"elapsed":3187,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"01ba4ea7-3bcf-4efc-bbf2-45ed843e0d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, 13)\n","==========step5_1_2 - NCF Model: Custom Embedding By Cluster Validation Results==========\n"]},{"output_type":"display_data","data":{"text/plain":["   cluster_id       mse      rmse   ndcg@10  precision@10  recall@10  \\\n","0           0  0.817287  0.904039  0.996735      0.888622   0.896832   \n","1           1  2.007905  1.417006  1.000000      0.642157   0.642157   \n","2           2  0.695792  0.834141  0.974441      0.906701   0.953757   \n","3           3  0.550171  0.741735  1.000000      0.933949   0.933949   \n","\n","      f1@10  embedding_dim  learning_rate  batch_size  dropout_rate  \\\n","0  0.891358              5          0.001         512           0.0   \n","1  0.642157              5          0.001         512           0.0   \n","2  0.922668              5          0.001         512           0.0   \n","3  0.933949              5          0.001         512           0.0   \n","\n","   num_epochs  train_time_sec  \n","0          40          195.19  \n","1          40           60.92  \n","2          40           57.76  \n","3          40          138.14  "],"text/html":["\n","  <div id=\"df-97108df4-f477-4c7c-8a1b-771412caaeae\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_id</th>\n","      <th>mse</th>\n","      <th>rmse</th>\n","      <th>ndcg@10</th>\n","      <th>precision@10</th>\n","      <th>recall@10</th>\n","      <th>f1@10</th>\n","      <th>embedding_dim</th>\n","      <th>learning_rate</th>\n","      <th>batch_size</th>\n","      <th>dropout_rate</th>\n","      <th>num_epochs</th>\n","      <th>train_time_sec</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.817287</td>\n","      <td>0.904039</td>\n","      <td>0.996735</td>\n","      <td>0.888622</td>\n","      <td>0.896832</td>\n","      <td>0.891358</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","      <td>195.19</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2.007905</td>\n","      <td>1.417006</td>\n","      <td>1.000000</td>\n","      <td>0.642157</td>\n","      <td>0.642157</td>\n","      <td>0.642157</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","      <td>60.92</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.695792</td>\n","      <td>0.834141</td>\n","      <td>0.974441</td>\n","      <td>0.906701</td>\n","      <td>0.953757</td>\n","      <td>0.922668</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","      <td>57.76</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.550171</td>\n","      <td>0.741735</td>\n","      <td>1.000000</td>\n","      <td>0.933949</td>\n","      <td>0.933949</td>\n","      <td>0.933949</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","      <td>138.14</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97108df4-f477-4c7c-8a1b-771412caaeae')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-97108df4-f477-4c7c-8a1b-771412caaeae button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-97108df4-f477-4c7c-8a1b-771412caaeae');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-8029342a-4332-4085-81cc-965a4fb15d04\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8029342a-4332-4085-81cc-965a4fb15d04')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-8029342a-4332-4085-81cc-965a4fb15d04 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(cust_emb_by_cluster_test_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"cluster_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.669048934854985,\n        \"min\": 0.550171,\n        \"max\": 2.007905,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2.007905,\n          0.550171,\n          0.8172871\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.30257545226913707,\n        \"min\": 0.74173516,\n        \"max\": 1.4170057,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.4170057,\n          0.74173516,\n          0.9040393\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ndcg@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.012331484451150232,\n        \"min\": 0.9744414860006144,\n        \"max\": 1.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.9967346705286488,\n          1.0,\n          0.9744414860006144\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13509085991821088,\n        \"min\": 0.6421568627450981,\n        \"max\": 0.9339485186983972,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6421568627450981,\n          0.9339485186983972,\n          0.8886216619671587\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14494455185232982,\n        \"min\": 0.6421568627450981,\n        \"max\": 0.953757225433526,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6421568627450981,\n          0.9339485186983972,\n          0.8968319787692818\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13809773249616247,\n        \"min\": 0.6421568627450981,\n        \"max\": 0.9339485186983972,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6421568627450981,\n          0.9339485186983972,\n          0.8913584342345331\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 512,\n        \"max\": 512,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 40,\n        \"max\": 40,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_time_sec\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66.20926338763583,\n        \"min\": 57.76,\n        \"max\": 195.19,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.92\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(4, 12)\n","==========step5_1_2 - NCF Model: Custom Embedding By Cluster Test Results==========\n"]},{"output_type":"display_data","data":{"text/plain":["   cluster_id       mse      rmse   ndcg@10  precision@10  recall@10  \\\n","0           0  1.004079  1.002037  0.972411      0.885086   0.954904   \n","1           1  3.231911  1.797752  0.973492      0.617072   0.700048   \n","2           2  1.263324  1.123977  0.970354      0.896678   0.962527   \n","3           3  1.100766  1.049174  0.986349      0.933419   0.964502   \n","\n","      f1@10  embedding_dim  learning_rate  batch_size  dropout_rate  \\\n","0  0.908728              5          0.001         512           0.0   \n","1  0.644731              5          0.001         512           0.0   \n","2  0.920061              5          0.001         512           0.0   \n","3  0.943780              5          0.001         512           0.0   \n","\n","   num_epochs  \n","0          40  \n","1          40  \n","2          40  \n","3          40  "],"text/html":["\n","  <div id=\"df-b12b5f0f-5ba0-492c-8440-ebdc13531677\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_id</th>\n","      <th>mse</th>\n","      <th>rmse</th>\n","      <th>ndcg@10</th>\n","      <th>precision@10</th>\n","      <th>recall@10</th>\n","      <th>f1@10</th>\n","      <th>embedding_dim</th>\n","      <th>learning_rate</th>\n","      <th>batch_size</th>\n","      <th>dropout_rate</th>\n","      <th>num_epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>1.004079</td>\n","      <td>1.002037</td>\n","      <td>0.972411</td>\n","      <td>0.885086</td>\n","      <td>0.954904</td>\n","      <td>0.908728</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>3.231911</td>\n","      <td>1.797752</td>\n","      <td>0.973492</td>\n","      <td>0.617072</td>\n","      <td>0.700048</td>\n","      <td>0.644731</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1.263324</td>\n","      <td>1.123977</td>\n","      <td>0.970354</td>\n","      <td>0.896678</td>\n","      <td>0.962527</td>\n","      <td>0.920061</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1.100766</td>\n","      <td>1.049174</td>\n","      <td>0.986349</td>\n","      <td>0.933419</td>\n","      <td>0.964502</td>\n","      <td>0.943780</td>\n","      <td>5</td>\n","      <td>0.001</td>\n","      <td>512</td>\n","      <td>0.0</td>\n","      <td>40</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b12b5f0f-5ba0-492c-8440-ebdc13531677')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b12b5f0f-5ba0-492c-8440-ebdc13531677 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b12b5f0f-5ba0-492c-8440-ebdc13531677');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-765519e8-985d-49ae-a171-42c131886a75\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-765519e8-985d-49ae-a171-42c131886a75')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-765519e8-985d-49ae-a171-42c131886a75 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"print(cust_emb_by_cluster_test_results\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"cluster_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 3,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1,\n          3,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0600051988934818,\n        \"min\": 1.0040786,\n        \"max\": 3.2319112,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3.2319112,\n          1.1007664,\n          1.0040786\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.37307154893452077,\n        \"min\": 1.0020373,\n        \"max\": 1.7977517,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.7977517,\n          1.0491742,\n          1.0020373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ndcg@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007249336742442358,\n        \"min\": 0.9703543098095708,\n        \"max\": 0.986348946689973,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.973491739219516,\n          0.986348946689973,\n          0.9724113621293718\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14546103157646034,\n        \"min\": 0.6170720076299475,\n        \"max\": 0.933419164980688,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6170720076299475,\n          0.933419164980688,\n          0.8850855745721271\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.13036400243572288,\n        \"min\": 0.7000476871721507,\n        \"max\": 0.9645024829869412,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.7000476871721507,\n          0.9645024829869412,\n          0.9549035588155392\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.14049076881709643,\n        \"min\": 0.6447305674773486,\n        \"max\": 0.9437802709827724,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.6447305674773486,\n          0.9437802709827724,\n          0.9087282182636706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 5,\n        \"max\": 5,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 512,\n        \"max\": 512,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          512\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 40,\n        \"max\": 40,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          40\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Average Validation Metrics:\n","cluster_id          1.500000\n","mse                 1.017789\n","rmse                0.974230\n","ndcg@10             0.992794\n","precision@10        0.842857\n","recall@10           0.856674\n","f1@10               0.847533\n","embedding_dim       5.000000\n","learning_rate       0.001000\n","batch_size        512.000000\n","dropout_rate        0.000000\n","num_epochs         40.000000\n","train_time_sec    113.002500\n","dtype: float64\n","\n","Average Test Metrics:\n","cluster_id         1.500000\n","mse                1.650020\n","rmse               1.243235\n","ndcg@10            0.975652\n","precision@10       0.833064\n","recall@10          0.895495\n","f1@10              0.854325\n","embedding_dim      5.000000\n","learning_rate      0.001000\n","batch_size       512.000000\n","dropout_rate       0.000000\n","num_epochs        40.000000\n","dtype: float64\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGD1YwobRsXFlkibn63eaV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}