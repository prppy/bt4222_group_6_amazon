{"cells":[{"cell_type":"markdown","metadata":{"id":"gSU_nwUFGay0"},"source":["# 5.3 NCF Negative Sampling"]},{"cell_type":"markdown","metadata":{"id":"fIghYSExQzHw"},"source":["## 5.3.1 Load Dataset\n","---\n","The following code will load in the full dataset from `filtered_reviews_With_features_and_clusters.csv` as well as the training, testing and validation datasets from `train_data.csv`, `test_data.csv` and `val_data.csv`."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5261,"status":"ok","timestamp":1744625103423,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"WiiZuKVbjtps","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9457df5b-6b39-408a-a03e-7d9fa174b495"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","import os\n","import pandas as pd\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.metrics import f1_score\n","\n","\n","# Mount Google Drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Define project paths\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gj9qoNpF9VBS"},"outputs":[],"source":["train_data = pd.read_csv(os.path.join(data_dir,\"train_data.csv\"))\n","test_data = pd.read_csv(os.path.join(data_dir,\"test_data.csv\"))\n","val_data = pd.read_csv(os.path.join(data_dir,\"val_data.csv\"))\n","\n","df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))"]},{"cell_type":"markdown","metadata":{"id":"JPRyXkF1RQ3s"},"source":["## 5.3.2 Data Preparation\n","---\n","The following code converts raw user and item IDs into numerical indices so that they can be used as input to a neural network and generates artificial negative samples (items the user hasn't interacted with) to help the model learn what not to recommend.\n","\n","NCF models are often used for recommendation systems, and they need to know not just what a user liked (positive samples), but also what they likely weren’t interested in (negative samples). Since real-world data doesn’t always have explicit dislikes, this technique (negative sampling) simulates those negatives to help the model learn more effectively."]},{"cell_type":"code","source":["# Create ID to index mappings\n","user2idx = {user_id: idx for idx, user_id in enumerate(train_data['customer_id'].unique())}\n","item2idx = {item_id: idx for idx, item_id in enumerate(train_data['product_id'].unique())}\n","\n","# Map to new columns\n","train_data['user_idx'] = train_data['customer_id'].map(user2idx)\n","train_data['item_idx'] = train_data['product_id'].map(item2idx)\n","val_data['user_idx'] = val_data['customer_id'].map(user2idx)\n","val_data['item_idx'] = val_data['product_id'].map(item2idx)\n","test_data['user_idx'] = test_data['customer_id'].map(user2idx)\n","test_data['item_idx'] = test_data['product_id'].map(item2idx)"],"metadata":{"id":"l9EWNyoUWQWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JJiYaiCyH18i"},"outputs":[],"source":["def generate_negative_samples(df, num_neg=1.0):\n","    all_users = df['customer_id'].unique()\n","    all_items = df['product_id'].unique()\n","    user_item_set = set(zip(df['customer_id'], df['product_id']))\n","\n","    negatives = []\n","    for user in tqdm(all_users, desc='Generating negatives'):\n","        user_known_items = df[df['customer_id'] == user]['product_id'].tolist()\n","        unknown_items = list(set(all_items) - set(user_known_items))\n","        n_samples = int(len(user_known_items) * num_neg)\n","        sampled = np.random.choice(unknown_items, size=n_samples, replace=False)\n","        for item in sampled:\n","            negatives.append((user, item, 0.0))\n","\n","    return pd.DataFrame(negatives, columns=['customer_id', 'product_id', 'rating'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415131,"status":"ok","timestamp":1744625527393,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"fVvprjfeH1It","outputId":"20739998-370b-4618-cd13-affd3c4df87c"},"outputs":[{"output_type":"stream","name":"stderr","text":["Generating negatives: 100%|██████████| 18484/18484 [03:26<00:00, 89.36it/s]\n","Generating negatives: 100%|██████████| 18484/18484 [02:15<00:00, 136.76it/s]\n","Generating negatives: 100%|██████████| 18484/18484 [01:12<00:00, 254.64it/s]\n"]}],"source":["# Training data\n","positive_df = train_data[['customer_id', 'product_id']].copy()\n","positive_df['rating'] = 1.0\n","negative_df = generate_negative_samples(positive_df, num_neg=1.0)\n","train_data_with_neg = pd.concat([positive_df, negative_df], ignore_index=True)\n","\n","# Testing data\n","positive_test_df = test_data[['customer_id', 'product_id']].copy()\n","positive_test_df['rating'] = 1.0\n","negative_test_df = generate_negative_samples(positive_test_df, num_neg=1.0)\n","test_data_with_neg = pd.concat([positive_test_df, negative_test_df], ignore_index=True)\n","\n","# Validation data\n","positive_val_df = val_data[['customer_id', 'product_id']].copy()\n","positive_val_df['rating'] = 1.0\n","negative_val_df = generate_negative_samples(positive_val_df, num_neg=1.0)\n","val_data_with_neg = pd.concat([positive_val_df, negative_val_df], ignore_index=True)"]},{"cell_type":"code","source":["print(f\"Original training set size: {len(train_data)}\")\n","print(f\"After negative sampling:    {len(train_data_with_neg)}\")\n","\n","print(f\"Original testing set size: {len(test_data)}\")\n","print(f\"After negative sampling:   {len(test_data_with_neg)}\")\n","\n","print(f\"Original validation set size: {len(val_data)}\")\n","print(f\"After negative sampling:      {len(val_data_with_neg)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kep3f0N6f1oc","executionInfo":{"status":"ok","timestamp":1744625527418,"user_tz":-480,"elapsed":15,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"d2554c0e-33a3-41ec-999a-f0caf51fb53b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original training set size: 116086\n","After negative sampling:    232172\n","Original testing set size: 43223\n","After negative sampling:    86446\n","Original validation set size: 21664\n","After negative sampling:    43328\n"]}]},{"cell_type":"code","source":["train_data_with_neg['user_idx'] = train_data_with_neg['customer_id'].map(user2idx)\n","train_data_with_neg['item_idx'] = train_data_with_neg['product_id'].map(item2idx)\n","\n","val_data_with_neg['user_idx'] = val_data_with_neg['customer_id'].map(user2idx)\n","val_data_with_neg['item_idx'] = val_data_with_neg['product_id'].map(item2idx)\n","\n","test_data_with_neg['user_idx'] = test_data_with_neg['customer_id'].map(user2idx)\n","test_data_with_neg['item_idx'] = test_data_with_neg['product_id'].map(item2idx)\n","\n","train_data_with_neg.dropna(subset=['user_idx', 'item_idx'], inplace=True)\n","val_data_with_neg.dropna(subset=['user_idx', 'item_idx'], inplace=True)\n","test_data_with_neg.dropna(subset=['user_idx', 'item_idx'], inplace=True)\n","\n","train_data_with_neg['user_idx'] = train_data_with_neg['user_idx'].astype(int)\n","train_data_with_neg['item_idx'] = train_data_with_neg['item_idx'].astype(int)\n","val_data_with_neg['user_idx'] = val_data_with_neg['user_idx'].astype(int)\n","val_data_with_neg['item_idx'] = val_data_with_neg['item_idx'].astype(int)\n","test_data_with_neg['user_idx'] = test_data_with_neg['user_idx'].astype(int)\n","test_data_with_neg['item_idx'] = test_data_with_neg['item_idx'].astype(int)\n"],"metadata":{"id":"hZdLMNDjoDbk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 5.3.3 Data Loader with Negative Sampling\n","---\n","We wrap our training, validation, and test data into a custom AmazonDataset class to prepare them for model training. Each sample includes user index, item index, and rating. Train loader uses shuffled data with negative samples, whereas the Validation/Test loaders use unshuffled data for evaluation.\n"],"metadata":{"id":"KGXFxaP8ik8A"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MHWXElpH4pM"},"outputs":[],"source":["class AmazonDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'customer_id': torch.tensor(row['user_idx'], dtype=torch.long),\n","            'product_id': torch.tensor(row['item_idx'], dtype=torch.long),\n","            'rating': torch.tensor(row['rating'], dtype=torch.float)\n","        }\n","\n","batch_size = 512\n","\n","train_dataset_with_neg = AmazonDataset(train_data_with_neg)\n","test_dataset_with_neg = AmazonDataset(test_data_with_neg)\n","val_dataset_with_neg = AmazonDataset(val_data_with_neg)\n","\n","train_loader_with_neg = DataLoader(train_dataset_with_neg, batch_size=batch_size, shuffle=True)\n","test_loader_with_neg = DataLoader(test_dataset_with_neg, batch_size=batch_size, shuffle=False)\n","val_loader_with_neg = DataLoader(val_dataset_with_neg, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","source":["## 5.3.4 Create Model\n","---\n","We implement the NCF architecture that combines:\n","\n","*   GMF (Generalized Matrix Factorization): Element-wise product of user and item embeddings\n","*   MLP (Multi-Layer Perceptron): Deep interaction modeling using concatenated embeddings\n","\n","These are merged and passed through fully connected layers to predict interaction probabilities."],"metadata":{"id":"1JL8clsBiymS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"F2GcracNH7tg"},"outputs":[],"source":["class NCF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_dim):\n","        super(NCF, self).__init__()\n","        self.user_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n","        self.item_embeddings_gmf = nn.Embedding(num_items, embedding_dim)\n","\n","        self.user_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n","        self.item_embeddings_mlp = nn.Embedding(num_items, embedding_dim)\n","\n","        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n","        self.fc2_mlp = nn.Linear(128, 64)\n","\n","        self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n","        self.fc2_combined = nn.Linear(128, 1)\n","\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, user_id, item_id):\n","        user_emb_gmf = self.user_embeddings_gmf(user_id)\n","        item_emb_gmf = self.item_embeddings_gmf(item_id)\n","        gmf_output = user_emb_gmf * item_emb_gmf\n","\n","        user_emb_mlp = self.user_embeddings_mlp(user_id)\n","        item_emb_mlp = self.item_embeddings_mlp(item_id)\n","        mlp_input = torch.cat([user_emb_mlp, item_emb_mlp], dim=-1)\n","        mlp_output = self.dropout(torch.relu(self.fc1_mlp(mlp_input)))\n","        mlp_output = self.dropout(torch.relu(self.fc2_mlp(mlp_output)))\n","\n","        combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n","        combined_output = self.dropout(torch.relu(self.fc1_combined(combined_input)))\n","        prediction = torch.sigmoid(self.fc2_combined(combined_output))\n","\n","        return prediction.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dFtG5YDH__I"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","num_users = len(user2idx)\n","num_items = len(item2idx)\n","embedding_dim = 32  # you can tune this\n","\n","model = NCF(num_users, num_items, embedding_dim).to(device)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","source":["## 5.3.5 Training the NCF Model\n","---\n","We define a train_model function to train the NCF model over several epochs using the training DataLoader. For each batch, it computes predictions, calculates loss, and updates weights via backpropagation."],"metadata":{"id":"rlQmvFQWlo02"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXt-QQgnRLZM"},"outputs":[],"source":["def train_model(model, train_loader, criterion, optimizer, device, num_epochs=10):\n","    model = model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0\n","\n","        for batch in train_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","\n","            optimizer.zero_grad()\n","            prediction = model(user, item)\n","            loss = criterion(prediction, label)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")"]},{"cell_type":"code","source":["train_model(model, train_loader_with_neg, criterion, optimizer, device, num_epochs=10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9pKP2jdmQbn","executionInfo":{"status":"ok","timestamp":1744625731651,"user_tz":-480,"elapsed":204135,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"ef445ad2-82f9-46ff-9388-58c01534c88d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10 | Loss: 0.2480\n","Epoch 2/10 | Loss: 0.2303\n","Epoch 3/10 | Loss: 0.2130\n","Epoch 4/10 | Loss: 0.1974\n","Epoch 5/10 | Loss: 0.1808\n","Epoch 6/10 | Loss: 0.1624\n","Epoch 7/10 | Loss: 0.1427\n","Epoch 8/10 | Loss: 0.1227\n","Epoch 9/10 | Loss: 0.1033\n","Epoch 10/10 | Loss: 0.0856\n"]}]},{"cell_type":"markdown","source":["## 5.3.6 Evaluation Functions\n","---\n","The evaluation functions and metrics are the same as what was explained in `step5_1_1 - NCF Model: Custom Embedding Full Dataset.ipynb`.\n","\n","- **ndcg_at_k**: Computes the Normalized Discounted Cumulative Gain (NDCG) at rank k for a single list of relevance. If the list contains fewer than k items, it will use actual_k = min(k, len(relevances)) to ensure fair computation.\n","\n","- **mean_ndcg_user_at_k**: Computes the mean NDCG@k across all users by grouping predicted scores and relevance labels per user, sorting by prediction, and applying ndcg_at_k. For each user, their items are sorted by predicted scores, and NDCG is computed using `ndcg_at_k` with actual_k = min(k, len(user_items)).\n","\n","- **mean_precision_user_at_k**: Computes the mean Precision@k across all users.\n","Precision@k is the proportion of relevant items (e.g., rating ≥ threshold) among the top-k predicted items for each user. For each user, top-k items are selected based on predicted scores. If the user has fewer than k items, actual_k = min(k, len(user_items)) is used.  \n","  Precision is calculated as:  \n","  `precision = (# of relevant items among top-k) / actual_k`  \n","  where an item is considered relevant if `rating ≥ threshold`.\n","\n","- **mean_recall_user_at_k**: Computes the mean Recall@k across all users.\n","Recall@k is the proportion of a user's relevant items (rating ≥ threshold) that are retrieved in the top-k predicted list. For each user, top-k items are selected based on predicted scores, and recall is calculated as:  \n","  `recall = (# of relevant items among top-k) / total number of relevant items for the user`  \n","  actual_k = min(k, len(user_items)) is used to handle users with fewer than k items.\n","\n","- **mean_f1_user_at_k**:  \n","  Computes the mean F1@k across all users, where F1 combines precision and recall.  \n","  For each user, top-k items are selected (using actual_k = min(k, len(user_items))), and F1 is calculated based on binarized relevance labels (`rating ≥ threshold`).  \n","  The predicted labels are assumed to be all 1s (e.g top-k are predicted as relevant)."],"metadata":{"id":"2iDP0opRmY_n"}},{"cell_type":"code","source":["def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)\n","    actual_k = min(k, len(relevances))\n","    if actual_k == 0:\n","        return 0.0\n","    relevances = relevances[:actual_k]\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for u, pred, rel in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, rel))\n","    ndcg_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [rel for _, rel in entries_sorted]\n","        ndcg_list.append(ndcg_at_k(relevances, k))\n","    return np.mean(ndcg_list) if ndcg_list else 0.0\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    precision_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","        rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        precision_list.append(np.sum(rels) / actual_k if actual_k > 0 else 0)\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    recall_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","\n","        all_rels = [1 if r >= threshold else 0 for _, r in entries]\n","        top_k_rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        total_relevant = np.sum(all_rels)\n","\n","        if total_relevant == 0:\n","            recall = 0.0\n","        else:\n","            recall = np.sum(top_k_rels) / total_relevant\n","        recall_list.append(recall)\n","    return np.mean(recall_list) if recall_list else 0.0\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, p, l in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((p, l))\n","\n","    f1_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        y_true = [int(l >= threshold) for _, l in entries_sorted[:actual_k]]\n","        y_pred = [1] * actual_k\n","        f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","    return np.mean(f1_list) if f1_list else 0.0"],"metadata":{"id":"SrmynZSipqY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nrqyOtP6YYmP"},"outputs":[],"source":["def evaluate_model(model, test_data, batch_size=512):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    criterion = nn.BCELoss()  # Use BCELoss for implicit feedback\n","\n","    test_loader = DataLoader(AmazonDataset(test_data), batch_size=batch_size)\n","    preds, labels, users = [], [], []\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","            output = model(user, item)  # Already has sigmoid inside\n","\n","            total_loss += criterion(output, label).item()\n","\n","            preds.extend(output.cpu().numpy())\n","            labels.extend(label.cpu().numpy())\n","            users.extend(user.cpu().numpy())\n","\n","    preds = np.array(preds)\n","    labels = np.array(labels)\n","    users = np.array(users)\n","\n","    rmse = np.sqrt(np.mean((preds - labels) ** 2))\n","    ndcg = mean_ndcg_user_at_k(users, preds, labels, k=10)\n","    precision = mean_precision_user_at_k(users, preds, labels, k=10, threshold=0.5)\n","    recall = mean_recall_user_at_k(users, preds, labels, k=10, threshold=0.5)\n","    f1_val = mean_f1_user_at_k(users, preds, labels, k=10, threshold=0.5)\n","\n","    return {\n","        'mse': total_loss / len(test_loader),\n","        'rmse': rmse,\n","        'ndcg@10': ndcg,\n","        'precision@10': precision,\n","        'recall@10': recall,\n","        'f1@10': f1_val\n","    }"]},{"cell_type":"code","source":["def get_predictions_dataframe(model, data_df, batch_size=512):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","\n","    loader = DataLoader(AmazonDataset(data_df), batch_size=batch_size)\n","    results = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            user_idx = batch['customer_id'].to(device)\n","            item_idx = batch['product_id'].to(device)\n","            rating = batch['rating'].to(device)\n","\n","            preds = model(user_idx, item_idx).cpu().numpy()\n","            actuals = rating.cpu().numpy()\n","            user_idx_np = user_idx.cpu().numpy()\n","            item_idx_np = item_idx.cpu().numpy()\n","\n","            for u, i, p, r in zip(user_idx_np, item_idx_np, preds, actuals):\n","                results.append([u, i, p, r])\n","\n","    df = pd.DataFrame(results, columns=['user_idx', 'item_idx', 'predicted_score', 'actual_label'])\n","\n","    # Map back to original customer_id and product_id\n","    inv_user_map = {v: k for k, v in user2idx.items()}\n","    inv_item_map = {v: k for k, v in item2idx.items()}\n","\n","    df['customer_id'] = df['user_idx'].map(inv_user_map)\n","    df['product_id'] = df['item_idx'].map(inv_item_map)\n","    return df[['customer_id', 'product_id', 'predicted_score', 'actual_label']]"],"metadata":{"id":"5FnHpxaOrO5Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get prediction DataFrames\n","val_preds_df = get_predictions_dataframe(model, val_data_with_neg)\n","test_preds_df = get_predictions_dataframe(model, test_data_with_neg)\n","\n","# Save to CSV\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","\n","val_preds_df.to_csv(os.path.join(results_dir, \"NCF Negative Sampling/full_model_val_predictions.csv\"), index=False)\n","test_preds_df.to_csv(os.path.join(results_dir, \"NCF Negative Sampling/full_model_test_predictions.csv\"), index=False)\n","\n","print(\"✅ Predictions saved to:\")\n","print(f\"- Validation: {results_dir}/full_model_val_predictions.csv\")\n","print(f\"- Testing:    {results_dir}/full_model_test_predictions.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QRbs3iV7rQt8","executionInfo":{"status":"ok","timestamp":1744626608345,"user_tz":-480,"elapsed":7687,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"2c1675e1-0c9d-4d68-e6da-449f9923c21c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Predictions saved to:\n","- Validation: /content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results/val_predictions.csv\n","- Testing:    /content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results/test_predictions.csv\n"]}]},{"cell_type":"code","source":["# Get Evaluation DataFrames\n","val_eval_df = pd.DataFrame([evaluate_model(model, val_data_with_neg)])\n","test_eval_df = pd.DataFrame([evaluate_model(model, test_data_with_neg)])\n","\n","# Save to CSV\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","\n","val_eval_df.to_csv(os.path.join(results_dir, \"NCF Negative Sampling/full_model_validation_results.csv\"), index=False)\n","test_eval_df.to_csv(os.path.join(results_dir, \"NCF Negative Sampling/full_model_testing_results.csv\"), index=False)\n","\n","print(\"✅ Evaluation results saved to:\")\n","print(f\"- Validation: {results_dir}/NCF Negative Sampling/full_model_validation_results.csv\")\n","print(f\"- Testing:    {results_dir}/NCF Negative Sampling/full_model_testing_results.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEVnR3cNzXMV","executionInfo":{"status":"ok","timestamp":1744626820306,"user_tz":-480,"elapsed":72568,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"b552bb9c-0a06-4919-c1e2-8df66e62dbb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Evaluation results saved to:\n","- Validation: /content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results/NCF Negative Sampling/full_model_validation_results.csv\n","- Testing:    /content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results/NCF Negative Sampling/full_model_testing_results.csv\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}