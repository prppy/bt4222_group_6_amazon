{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNStpH2Q3dqc0CHBZaTi8z2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1LeXbWR2qDe","executionInfo":{"status":"ok","timestamp":1744727358586,"user_tz":-480,"elapsed":4422,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"d22d11a6-7631-4ac9-f795-2712fe883e37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","# drive.mount('/content/drive')\n","drive.mount(\"/content/drive\", force_remount=True)"]},{"cell_type":"code","source":["import pandas as pd\n","import itertools\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import copy\n","import os\n","from collections import defaultdict\n","import random\n","import itertools\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.metrics import ndcg_score as sk_ndcg\n","\n","# Build Custom Customer and Product Embedding\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"],"metadata":{"id":"uaClfpS825_G","executionInfo":{"status":"ok","timestamp":1744727358590,"user_tz":-480,"elapsed":2,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["# Loading and splitting the data"],"metadata":{"id":"GLjUWRG6OSiP"}},{"cell_type":"code","source":["df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))"],"metadata":{"id":"-OoDpp3D258O","executionInfo":{"status":"ok","timestamp":1744727364145,"user_tz":-480,"elapsed":5554,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["print(df_reviews.head())"],"metadata":{"id":"TPwRNsKtO9-_","executionInfo":{"status":"ok","timestamp":1744727364151,"user_tz":-480,"elapsed":4,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"2e385d4e-e788-43d8-f10f-4f928f70ebc5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n"]}]},{"cell_type":"markdown","source":["# Mapping IDs in df_reviews\n","\n","ID mapping for users and products in the df_reviews DataFrame, creating new columns for numerical indices corresponding to each unique user and product."],"metadata":{"id":"gsVwxStn4Ykq"}},{"cell_type":"code","source":["user2idx = {user_id: idx + 1 for idx, user_id in enumerate(df_reviews['customer_id'].unique())}\n","item2idx = {item_id: idx + 1 for idx, item_id in enumerate(df_reviews['product_id'].unique())}\n","\n","df_reviews['user_idx'] = df_reviews['customer_id'].map(user2idx)\n","df_reviews['item_idx'] = df_reviews['product_id'].map(item2idx)"],"metadata":{"id":"A3F4zb3R4cSr","executionInfo":{"status":"ok","timestamp":1744727364153,"user_tz":-480,"elapsed":1,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["# Splitting the data for LSTM\n","\n","- **Padding**: Sequences are padded to ensure a fixed length.\n","- **Data Splitting**: Users are split into training, validation, and test sets (64%, 16%, 20%).\n","\n","#### Formation of Training, Validation, and Testing Data\n","1. **Training Data**: From the data, we select the 80% of the interactions to generate sequences. These sequences, along with positive samples (items the user interacted with), and negative samples (randomly selected items the user did not interact with), are created and added to the training data. These sequences allow the model to learn patterns in user behavior.\n","\n","2. **Validation Data**: 16% of the interactions are used for validation. Similar to training data, we generate sequences, positive and negative samples, to evaluate how well the model generalizes to unseen data during training.\n","\n","3. **Testing Data**: 24% of the data is used for testing. For each user in the testing data, the first 80% of interactions are used as model input and last 20% as positive labels. 10 negative items are also sampled. The model does not see this data during training. Positive samples are taken from the user's reviews that were held out for testing, and multiple negative samples are randomly chosen from items the user hasn't interacted with.\n","\n","This approach ensures that the model is trained on sequences of past interactions, with positive samples (items the user interacted with) and negative samples (items the user did not interact with), preventing the model from seeing any cold-start users or products in the validation and test sets. Also ensures that the model is trained, validated, and tested on distinct sets of data, avoiding overfitting and ensuring generalizability."],"metadata":{"id":"DhyR1BeXmQPX"}},{"cell_type":"code","source":["max_len = 10\n","min_seq_length = 5\n","max_interactions = 20\n","\n","df_sorted = df_reviews.sort_values(by=['user_idx', 'review_date'])\n","grouped = df_sorted.groupby('user_idx')\n","\n","all_items = np.array(df_sorted['item_idx'].unique())\n","filtered_users = [uid for uid, grp in grouped if len(grp) >= min_seq_length]\n","\n","# 64% train, 16% val, 20% test\n","train_val_users, test_users = train_test_split(filtered_users, test_size=0.2, random_state=42)\n","train_users, val_users = train_test_split(train_val_users, test_size=0.2, random_state=42)\n","\n","def pad_left(seq, max_len, pad_value=0):\n","    return [pad_value] * (max_len - len(seq)) + seq\n","\n","train_data, val_data, test_data = [], [], []\n","\n","for uid in filtered_users:\n","    group = grouped.get_group(uid).sort_values('review_date').iloc[:max_interactions]\n","    items = group['item_idx'].tolist()\n","    user_item_set = set(items)\n","    neg_pool = np.setdiff1d(all_items, list(user_item_set), assume_unique=True)\n","\n","    split_point = int(0.8 * len(items))\n","\n","    for i in range(1, split_point):\n","        seq = items[max(0, i - max_len):i]\n","        if len(seq) < 4:\n","            continue  # Skip short sequences\n","\n","        seq_padded = pad_left(seq, max_len)\n","        pos_item = items[i]\n","        neg_item = np.random.choice(neg_pool)\n","\n","        if uid in train_users:\n","            train_data.extend([\n","                (uid, seq_padded, pos_item, 1),\n","                (uid, seq_padded, neg_item, 0)\n","            ])\n","        elif uid in val_users:\n","            val_data.extend([\n","                (uid, seq_padded, pos_item, 1),\n","                (uid, seq_padded, neg_item, 0)\n","            ])\n","\n","    if uid in test_users:\n","        seq = items[max(0, split_point - max_len):split_point]\n","        if len(seq) < 4:\n","            continue\n","\n","        seq_padded = pad_left(seq, max_len)\n","        # test positives\n","        pos_items = items[split_point:]\n","\n","        for pos_item in pos_items:\n","            test_data.append((uid, seq_padded, pos_item, 1))\n","\n","        # sample multiple negatives (e.g., 50)\n","        neg_items = np.random.choice(neg_pool, size=10, replace=False)\n","        for neg_item in neg_items:\n","            test_data.append((uid, seq_padded, neg_item, 0))\n","\n","print(f\"Train Samples: {len(train_data)}\")\n","print(f\"Val Samples: {len(val_data)}\")\n","print(f\"Test Samples: {len(test_data)}\")"],"metadata":{"id":"vcW3rPL7mPHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744727449427,"user_tz":-480,"elapsed":85273,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"b1b988b5-ef82-4955-da06-ef88288664cd"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Samples: 77002\n","Val Samples: 19204\n","Test Samples: 111834\n"]}]},{"cell_type":"code","source":["import random\n","test_user_ids = list(set([row[0] for row in test_data]))\n","# Sample 20% of them\n","sampled_users = set(random.sample(test_user_ids, int(len(test_user_ids) * 0.2)))\n","reduced_test_data = [row for row in test_data if row[0] in sampled_users]\n","test_data = reduced_test_data\n","\n","print(f\"Reduced to {len(test_data)} test samples from {len(sampled_users)} users.\")"],"metadata":{"id":"dGTpg9REykrK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744727449458,"user_tz":-480,"elapsed":40,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"f948caa2-8547-4f55-f83d-e21a3c2df569"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Reduced to 22361 test samples from 1906 users.\n"]}]},{"cell_type":"markdown","source":["# Building Customer Embeddings\n","\n","The customer embeddings will constructed by aggregating features within df_reviews by customer_id to find a customer's:\n","\n","Purchase Frequency (Indicate how active a customer is)\n","Time Since Last Purchase (Indicate how active a customer is)\n","Average Star Rating (Overall Customer satisfaction across all of his purchases)\n","Total Vine Reviews (Measure of Credibility of his Reviews)\n","Total Helpful Votes (Measure the Credibility of his Reviews)\n","Total Votes (Measure the Credibility of his Reviews)\n","Average Sentiment (Overall Customer satisfaction across all of his purchases\n","These embeddings are more informative than a randomly intialized embedding in typical recommnedation systems. The choice of specific features injects domain knowledge into the model."],"metadata":{"id":"tppDalKZC3qy"}},{"cell_type":"code","source":["def build_customer_embeddings(df_reviews, embedding_dim):\n","    agg = df_reviews.groupby('customer_id').agg({\n","        'monthly_purchase_frequency': 'mean',\n","        'time_since_last_purchase': 'mean',\n","        'star_rating': 'mean',\n","        'vine': lambda x: (x == \"Y\").sum(),\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean()\n","    }).fillna(0).reset_index()\n","\n","    cust_ids = agg['customer_id']\n","    X = StandardScaler().fit_transform(agg.drop(columns='customer_id'))\n","    num_features = X.shape[1]\n","\n","    if embedding_dim > num_features:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features available.\")\n","\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        X = pca.fit_transform(X)\n","        if (pca.explained_variance_ratio_ > 1e-6).sum() < embedding_dim:\n","            raise ValueError(f\"PCA found fewer than {embedding_dim} meaningful components.\")\n","\n","    df = pd.DataFrame(X, index=cust_ids)\n","    df.index.name = 'customer_id'\n","    return df"],"metadata":{"id":"zQb0wEbQC3Kl","executionInfo":{"status":"ok","timestamp":1744727449467,"user_tz":-480,"elapsed":5,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":["# Building Product Embeddings\n","\n","The product embeddings will constructed by aggregating features within df_reviews by product_id to find a product's:\n","Mean Star Rating that it received (Customer satisfaction)\n","Total Helpful Votes given to all its reviews (Quality of customer feedback)\n","Total Votes given to all its reviews (Review Engagement by customers)\n","Average sentiment (1 is Positive and 0 is Negative)\n","Total Number of Vine Reviews (Number of Credible Reviews)\n","Total Product Sales (Demand for Product)"],"metadata":{"id":"TCu32EjtC4S2"}},{"cell_type":"code","source":["def build_product_embeddings(df_reviews, embedding_dim):\n","    agg = df_reviews.groupby('product_id').agg({\n","        'star_rating': 'mean',\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean(),\n","        'vine': lambda x: (x == 'Y').sum(),\n","        'product_id': 'count'  # will be renamed\n","    }).rename(columns={'product_id': 'sales_volume'}).fillna(0).reset_index()\n","\n","    prod_ids = agg['product_id']\n","    X = StandardScaler().fit_transform(agg.drop(columns='product_id'))\n","    num_features = X.shape[1]\n","\n","    if embedding_dim > num_features:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features available.\")\n","\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        X = pca.fit_transform(X)\n","        if (pca.explained_variance_ratio_ > 1e-6).sum() < embedding_dim:\n","            raise ValueError(f\"PCA found fewer than {embedding_dim} meaningful components.\")\n","\n","    df = pd.DataFrame(X, index=prod_ids)\n","    df.index.name = 'product_id'\n","    return df\n"],"metadata":{"id":"R6oUJe7xC3EZ","executionInfo":{"status":"ok","timestamp":1744727449510,"user_tz":-480,"elapsed":39,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":["We form two dense matrices: user_embedding_matrix and product_embedding_matrix. which will be concatenated with the LSTM's internal representations (sequence and candidate embeddings) during training to provide the model with more informative input, improving its ability to personalize recommendations based on both temporal behavior and customer/product features."],"metadata":{"id":"pHjMWBRqZWBA"}},{"cell_type":"code","source":["# Build customer & product embeddings from your reviews\n","user_embed_df = build_customer_embeddings(df_reviews, embedding_dim=7)\n","product_embed_df = build_product_embeddings(df_reviews, embedding_dim=6)\n","\n","# Convert to embedding matrices aligned with user2idx/item2idx\n","user_embedding_matrix = np.zeros((df_reviews['user_idx'].max() + 1, 7))\n","for cid, row in user_embed_df.iterrows():\n","    if cid in user2idx:\n","        user_embedding_matrix[user2idx[cid]] = row.values\n","\n","product_embedding_matrix = np.zeros((df_reviews['item_idx'].max() + 1, 6))\n","for pid, row in product_embed_df.iterrows():\n","    if pid in item2idx:\n","        product_embedding_matrix[item2idx[pid]] = row.values"],"metadata":{"id":"FRQLSmW5DcTb","executionInfo":{"status":"ok","timestamp":1744727473770,"user_tz":-480,"elapsed":24257,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":["# Creating DataLoader"],"metadata":{"id":"9mXJ-T0bHymc"}},{"cell_type":"code","source":["class PurchaseSequenceDataset(Dataset):\n","    def __init__(self, data, user_embedding_matrix, product_embedding_matrix):\n","        self.data = data\n","        self.user_embedding_matrix = user_embedding_matrix\n","        self.product_embedding_matrix = product_embedding_matrix\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        user_id, seq, candidate_id, label = self.data[idx][:4]\n","\n","        customer_vec = torch.FloatTensor(self.user_embedding_matrix[user_id])\n","        product_vec = torch.FloatTensor(self.product_embedding_matrix[candidate_id])\n","\n","        return (\n","            torch.LongTensor(seq),             # inputs\n","            torch.LongTensor([candidate_id]),  # candidate\n","            torch.FloatTensor([label]),        # label\n","            torch.LongTensor([user_id]),       # user_id for ranking\n","            torch.FloatTensor(customer_vec),   # external user vec\n","            torch.FloatTensor(product_vec)     # external product vec\n","        )\n","\n","batch_size = 64\n","\n","train_dataset = PurchaseSequenceDataset(train_data, user_embedding_matrix, product_embedding_matrix)\n","val_dataset   = PurchaseSequenceDataset(val_data, user_embedding_matrix, product_embedding_matrix)\n","test_dataset  = PurchaseSequenceDataset(test_data, user_embedding_matrix, product_embedding_matrix)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","print(f\"Training Data Size: {len(train_loader.dataset)}\")\n","print(f\"Validation Data Size: {len(val_loader.dataset)}\")\n","print(f\"Testing Data Size: {len(test_loader.dataset)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W5mmqBnkHyJj","executionInfo":{"status":"ok","timestamp":1744727473848,"user_tz":-480,"elapsed":70,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"21d82641-8624-4bc5-9206-b62a57ee5038"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Training Data Size: 77002\n","Validation Data Size: 19204\n","Testing Data Size: 22361\n"]}]},{"cell_type":"markdown","source":["# Defining the LSTM model with Custom Embeddings\n","\n","- **Model**: Predict whether a user will purchase a candidate product based on past interactions (purchase history) and custom user/product features  \n","- **Inputs**: User's past purchases; Candidate product; Customer embedding; Product embedding  \n","- **Architecture**:  \n","  - **Embedding Layers**: Convert user sequence and candidate item indices into dense vectors  \n","  - **LSTM**: Processes the user's purchase sequence to capture sequential dependencies  \n","  - **Hidden State**: The output from the LSTM is combined with the candidate's embedding and customer/product embeddings  \n","  - **Fully Connected Layer**: Computes a single logit representing the purchase probability  \n","- **Activation**: **Sigmoid**: Converts the logit into a probability between 0 and 1  \n","- **Objective**: Binary classification (purchase or not purchase)  \n"],"metadata":{"id":"qA3xSivE2Udn"}},{"cell_type":"code","source":["class PurchaseCandidateLSTM(nn.Module):\n","    def __init__(self, num_items, embed_dim, hidden_dim, dropout, num_layers,\n","                 external_user_dim, external_product_dim):\n","        super(PurchaseCandidateLSTM, self).__init__()\n","        self.num_layers = num_layers\n","        self.hidden_dim = hidden_dim\n","\n","        self.embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embed_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(\n","            input_size=embed_dim,\n","            hidden_size=hidden_dim,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout\n","        )\n","\n","        self.dropout = nn.Dropout(dropout)\n","        self.candidate_embedding = nn.Embedding(num_embeddings=num_items, embedding_dim=embed_dim, padding_idx=0)\n","\n","        total_input_dim = hidden_dim + embed_dim + external_user_dim + external_product_dim\n","        self.fc = nn.Linear(total_input_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, seq, candidate, customer_vec, product_vec):\n","        seq_embedded = self.embedding(seq)                      # (batch_size, seq_len, embed_dim)\n","        lstm_out, _ = self.lstm(seq_embedded)                   # (batch_size, seq_len, hidden_dim)\n","        seq_repr = self.dropout(lstm_out[:, -1, :])             # (batch_size, hidden_dim)\n","\n","        candidate_emb = self.candidate_embedding(candidate).squeeze(1)  # (batch_size, embed_dim)\n","\n","        combined = torch.cat([seq_repr, candidate_emb, customer_vec, product_vec], dim=1)\n","        logits = self.fc(combined)\n","        return self.sigmoid(logits)\n","\n","    def init_hidden(self, batch_size, device):\n","        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n","        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n","        return (h0, c0)\n"],"metadata":{"id":"jxAqPqF82Ysv","executionInfo":{"status":"ok","timestamp":1744727473858,"user_tz":-480,"elapsed":5,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Metrics Functions\n","\n","- ndcg_at_k: Computes the NDCG for a single list of relevance labels at rank k.\n","- mean_ndcg_user_at_k: Computes the mean NDCG@k across all users.\n","- mean_precision_user_at_k: Computes the mean Precision@k across users, where Precision@k is the proportion of relevant items in the top k predictions.\n","- mean_recall_user_at_k: Computes the mean Recall@k across users, where Recall@k is the proportion of relevant items retrieved in the top k predictions.\n","- mean_f1_user_at_k: Computes the mean F1 score at k across users, comparing the predicted top k items with the true labels."],"metadata":{"id":"aQkRlbQjRZ4e"}},{"cell_type":"code","source":["def acc(outputs, labels):\n","    predicted = (outputs.squeeze() >= 0.5).float()\n","    return (predicted == labels.squeeze()).sum().item()\n","\n","def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)[:k]\n","    if relevances.size == 0:\n","        return 0.0\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, relevances.size + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, ideal_relevances.size + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    ndcg_scores = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [label for _, label in ranked]\n","        dcg = ndcg_at_k(relevances, k)\n","        ndcg_scores.append(dcg)\n","\n","    return np.mean(ndcg_scores)\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    precision_list = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        precision = sum(label for _, label in top_k) / k\n","        precision_list.append(precision)\n","\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    recall_list = []\n","    for user, entries in user_data.items():\n","        total_positives = sum(label for _, label in entries)\n","        if total_positives == 0:\n","            recall_list.append(0.0)\n","            continue\n","\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        retrieved_positives = sum(label for _, label in top_k)\n","        recall = retrieved_positives / total_positives\n","        recall_list.append(recall)\n","\n","    return np.mean(recall_list)\n","\n","\n","from sklearn.metrics import f1_score\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for user, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[user].append((pred, label))\n","\n","    f1_list = []\n","    for user, entries in user_data.items():\n","        ranked = sorted(entries, key=lambda x: x[0], reverse=True)\n","        top_k = ranked[:k]\n","        y_true = [label for _, label in top_k]\n","        y_pred = [1] * len(top_k)\n","        if sum(y_true) == 0:\n","            f1_list.append(0.0)\n","        else:\n","            f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","\n","    return np.mean(f1_list)"],"metadata":{"id":"MmAoRDSorhtO","executionInfo":{"status":"ok","timestamp":1744727473890,"user_tz":-480,"elapsed":29,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":["# Training and Evaluation"],"metadata":{"id":"bJfLurKFb1Sm"}},{"cell_type":"code","source":["def train_lstm_model(model, train_loader, val_loader, device, epochs, lr, clip, patience=3):\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss, train_correct, total_train = 0, 0, 0\n","\n","        for inputs, candidate, labels, user_ids, customer_vec, product_vec in train_loader:\n","            inputs = inputs.to(device)\n","            candidate = candidate.to(device)\n","            labels = labels.to(device)\n","            customer_vec = customer_vec.to(device)\n","            product_vec = product_vec.to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(inputs, candidate, customer_vec, product_vec)\n","            loss = criterion(output, labels.float())\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(model.parameters(), clip)\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            train_correct += acc(output, labels)\n","            total_train += labels.size(0)\n","\n","        train_acc = train_correct / total_train\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        # Validation\n","        model.eval()\n","        val_loss, val_correct, total_val = 0, 0, 0\n","        with torch.no_grad():\n","            for inputs, candidate, labels, user_ids, customer_vec, product_vec in val_loader:\n","                inputs = inputs.to(device)\n","                candidate = candidate.to(device)\n","                labels = labels.to(device)\n","                customer_vec = customer_vec.to(device)\n","                product_vec = product_vec.to(device)\n","\n","                output = model(inputs, candidate, customer_vec, product_vec)\n","                loss = criterion(output, labels.float())\n","                val_loss += loss.item()\n","                val_correct += acc(output, labels)\n","                total_val += labels.size(0)\n","\n","        val_acc = val_correct / total_val\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        print(f\"Epoch {epoch+1}/{epochs}\")\n","        print(f\"\\tTrain Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n","        print(f\"\\tTrain Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n","\n","        # Early stopping check, when patience = 3\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= patience:\n","                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n","                break"],"metadata":{"id":"I-Ryi-MISo1t","executionInfo":{"status":"ok","timestamp":1744727473897,"user_tz":-480,"elapsed":3,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["In this method, we evaluate the LSTM-based binary classification model that is designed to predict the probability that a user wil purchase each candidate product, given their past purchase sequence. During evaluation, the model scores candidate products for users, including both positive (purchased) and negative (not purchased) items. It then ranks these candidates by predicted probability and computes ranking-based metrics: Precision@K, Recall@K, F1@K, and NDCG@K."],"metadata":{"id":"JmbFxMGGcrPJ"}},{"cell_type":"code","source":["def evaluate_model(model, test_loader, device, k=10, threshold=0.5, set_name=\"test\"):\n","    model.eval()\n","    correct, total = 0, 0\n","    all_users, all_preds, all_labels = [], [], []\n","    user_results = defaultdict(lambda: {'preds': [], 'labels': []})\n","    criterion = nn.BCELoss()\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for inputs, candidate, labels, user_ids, customer_vec, product_vec in test_loader:\n","            inputs = inputs.to(device)\n","            candidate = candidate.to(device)\n","            labels = labels.to(device)\n","            customer_vec = customer_vec.to(device)\n","            product_vec = product_vec.to(device)\n","\n","            output = model(inputs, candidate, customer_vec, product_vec)\n","            loss = criterion(output, labels.float())\n","            total_loss += loss.item()\n","\n","            preds = output.squeeze().cpu().numpy()\n","            labs = labels.squeeze().cpu().numpy()\n","            user_ids = user_ids.squeeze().cpu().numpy()\n","\n","            preds_binary = (output.squeeze() >= 0.5).float()\n","            correct += (preds_binary == labels.squeeze()).sum().item()\n","            total += labels.size(0)\n","\n","            for u, p, l in zip(user_ids, preds, labs):\n","                user_results[int(u)]['preds'].append(p)\n","                user_results[int(u)]['labels'].append(l)\n","\n","            all_users.extend(user_ids)\n","            all_preds.extend(preds)\n","            all_labels.extend(labs)\n","\n","    # Flat metrics\n","    binarized_preds = (np.array(all_preds) >= 0.5).astype(int)\n","    flat_precision = precision_score(all_labels, binarized_preds, zero_division=0)\n","    flat_recall = recall_score(all_labels, binarized_preds, zero_division=0)\n","    flat_f1 = f1_score(all_labels, binarized_preds, zero_division=0)\n","\n","    # Ranking metrics\n","    prec_at_k = mean_precision_user_at_k(all_users, all_preds, all_labels, k=k)\n","    rec_at_k = mean_recall_user_at_k(all_users, all_preds, all_labels, k=k)\n","    f1_at_k = mean_f1_user_at_k(all_users, all_preds, all_labels, k=k)\n","    ndcg_at_k_val = mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=k)\n","\n","    # Aggregate & Log\n","    avg_loss = total_loss / len(test_loader)\n","    accuracy = correct / total\n","\n","    print(f\"Test Loss: {avg_loss:.4f}\")\n","    print(f\"Flat Precision: {flat_precision:.4f} | Recall: {flat_recall:.4f} | F1: {flat_f1:.4f}\")\n","    print(f\"Precision@{k}: {prec_at_k:.4f} | Recall@{k}: {rec_at_k:.4f} | F1@{k}: {f1_at_k:.4f} | NDCG@{k}: {ndcg_at_k_val:.4f}\")\n","\n","    metrics = {\n","        'val_acc': accuracy,\n","        'loss': avg_loss,\n","        'flat_precision': flat_precision,\n","        'flat_recall': flat_recall,\n","        'flat_f1': flat_f1,\n","        f'precision@{k}': prec_at_k,\n","        f'recall@{k}': rec_at_k,\n","        f'f1@{k}': f1_at_k,\n","        f'ndcg@{k}': ndcg_at_k_val,\n","    }\n","\n","    # Build path and save\n","    filename = f\"{set_name}_evaluation_results.csv\"\n","    output_path = os.path.join(project_dir, \"Model Results\", \"LSTM Custom Embedding\", filename)\n","    pd.DataFrame([metrics]).to_csv(output_path, index=False)\n","    print(f\"Evaluation metrics saved to {filename}\")\n","\n","    return metrics"],"metadata":{"id":"ya771HBv25xX","executionInfo":{"status":"ok","timestamp":1744727473959,"user_tz":-480,"elapsed":8,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":["# Hyperparameters"],"metadata":{"id":"lC4J99_jUZO-"}},{"cell_type":"code","source":["embedding_dim = 64\n","hidden_dim = 256\n","dropout = 0.5\n","num_layers = 2\n","clip = 5\n","epochs = 12\n","external_embed_dim = 7 + 6  # customer + product vector dim\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_items = df_reviews['item_idx'].max() + 1  # assumes item_idx is 0-based mapped\n","\n","# Initialize the model\n","model = PurchaseCandidateLSTM(\n","    num_items=num_items,\n","    embed_dim=embedding_dim,\n","    hidden_dim=hidden_dim,\n","    dropout=dropout,\n","    num_layers=num_layers,\n","    external_user_dim=7,\n","    external_product_dim=6\n",").to(device)\n","\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mg6z-wfnUZtj","executionInfo":{"status":"ok","timestamp":1744727474120,"user_tz":-480,"elapsed":157,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"d24e0cac-3592-4629-98b5-595a3ae70d19"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["PurchaseCandidateLSTM(\n","  (embedding): Embedding(73915, 64, padding_idx=0)\n","  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (candidate_embedding): Embedding(73915, 64, padding_idx=0)\n","  (fc): Linear(in_features=333, out_features=1, bias=True)\n","  (sigmoid): Sigmoid()\n",")\n"]}]},{"cell_type":"code","source":["train_lstm_model(\n","    model=model,\n","    train_loader=train_loader,\n","    val_loader=val_loader,\n","    device=device,\n","    epochs=epochs,\n","    lr=0.0003,\n","    clip=clip\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2yf1tMB0Ucin","executionInfo":{"status":"ok","timestamp":1744727572965,"user_tz":-480,"elapsed":98840,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"87961572-936d-4512-eecc-b0bc516f910d"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/12\n","\tTrain Loss: 0.5982 | Val Loss: 0.5701\n","\tTrain Acc: 69.42% | Val Acc: 73.68%\n","Epoch 2/12\n","\tTrain Loss: 0.5612 | Val Loss: 0.5526\n","\tTrain Acc: 73.25% | Val Acc: 74.05%\n","Epoch 3/12\n","\tTrain Loss: 0.5460 | Val Loss: 0.5507\n","\tTrain Acc: 74.19% | Val Acc: 73.97%\n","Epoch 4/12\n","\tTrain Loss: 0.5456 | Val Loss: 0.5491\n","\tTrain Acc: 74.78% | Val Acc: 75.51%\n","Epoch 5/12\n","\tTrain Loss: 0.5475 | Val Loss: 0.5566\n","\tTrain Acc: 75.38% | Val Acc: 74.91%\n","Epoch 6/12\n","\tTrain Loss: 0.5451 | Val Loss: 0.5523\n","\tTrain Acc: 75.63% | Val Acc: 75.95%\n","Epoch 7/12\n","\tTrain Loss: 0.5419 | Val Loss: 0.5585\n","\tTrain Acc: 75.91% | Val Acc: 76.17%\n","Early stopping triggered after 7 epochs.\n"]}]},{"cell_type":"code","source":["evaluation_results = evaluate_model(\n","    model=model,\n","    test_loader=val_loader,\n","    device=device,\n","    k=10,  # top-k\n","    set_name=\"val\"\n",")\n","\n","evaluation_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PsyihLuhmbdh","executionInfo":{"status":"ok","timestamp":1744727579873,"user_tz":-480,"elapsed":6879,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"48364467-6ab2-4082-bf24-3d1689a4ba25"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.5585\n","Flat Precision: 0.8934 | Recall: 0.5943 | F1: 0.7137\n","Precision@10: 0.3045 | Recall@10: 0.9757 | F1@10: 0.6895 | NDCG@10: 0.9464\n","Evaluation metrics saved to val_evaluation_results.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["{'val_acc': 0.7616642366173714,\n"," 'loss': 0.5584850416230996,\n"," 'flat_precision': 0.8933771723813997,\n"," 'flat_recall': 0.5942511976671527,\n"," 'flat_f1': 0.7137406967290012,\n"," 'precision@10': np.float32(0.30451438),\n"," 'recall@10': np.float32(0.975684),\n"," 'f1@10': np.float64(0.6894987041137931),\n"," 'ndcg@10': np.float64(0.9464418822439133)}"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","source":["evaluation_results = evaluate_model(\n","    model=model,\n","    test_loader=test_loader,\n","    device=device,\n","    k=10,  # top-k\n","    set_name=\"test\"\n",")\n","\n","evaluation_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYR-Rjs-U5IW","executionInfo":{"status":"ok","timestamp":1744727585135,"user_tz":-480,"elapsed":5253,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"f22bd57a-d8d4-43ae-c67f-00774256231b"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.5814\n","Flat Precision: 0.5775 | Recall: 0.5465 | F1: 0.5616\n","Precision@10: 0.1661 | Recall@10: 0.9637 | F1@10: 0.2787 | NDCG@10: 0.7926\n","Evaluation metrics saved to test_evaluation_results.csv\n"]},{"output_type":"execute_result","data":{"text/plain":["{'val_acc': 0.874021734269487,\n"," 'loss': 0.5814116349390575,\n"," 'flat_precision': 0.5774647887323944,\n"," 'flat_recall': 0.5465010602847622,\n"," 'flat_f1': 0.5615564202334631,\n"," 'precision@10': np.float32(0.16610703),\n"," 'recall@10': np.float32(0.9636674),\n"," 'f1@10': np.float64(0.27866869052598325),\n"," 'ndcg@10': np.float64(0.7925546721470978)}"]},"metadata":{},"execution_count":50}]}]}