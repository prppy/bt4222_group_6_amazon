{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNCxd23msM0q5BTLnC9y0GD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Neural Collaborative Filtering with Randomly Intialized Embeddings"],"metadata":{"id":"sXYkhGwl2pby"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J19TNojDIEkx","executionInfo":{"status":"ok","timestamp":1744605501117,"user_tz":-480,"elapsed":29325,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"b2a6d22d-8515-4444-dde0-ce22a22e2bfd"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import itertools\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import copy\n","import os\n","from collections import defaultdict\n","import torch.nn.functional as F\n","from sklearn.metrics import f1_score\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"],"metadata":{"id":"cyvfGjxo_HkB","executionInfo":{"status":"ok","timestamp":1744605508266,"user_tz":-480,"elapsed":7151,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Step 1: Load dataset"],"metadata":{"id":"dlowKi5Y-WfV"}},{"cell_type":"markdown","source":["Previously in step1_data_preprocessing.ipynb, We have split the df_reviews dataset into training, testing and validation samples for each user, following chronological order and using the early 70% of each user's interactions for training, followed by the next 15% for validation and the last 15% for testing."],"metadata":{"id":"5cbayT1uKOyM"}},{"cell_type":"code","source":["train_data = pd.read_csv(os.path.join(data_dir,\"train_data.csv\"))\n","test_data = pd.read_csv(os.path.join(data_dir,\"test_data.csv\"))\n","val_data = pd.read_csv(os.path.join(data_dir,\"val_data.csv\"))\n","\n","df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))\n","print(df_reviews.head())"],"metadata":{"id":"ew_y5LJ3J7ln","executionInfo":{"status":"ok","timestamp":1744605526111,"user_tz":-480,"elapsed":17854,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"13ae3f05-2534-47b6-dfd1-7828ed0010f1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n"]}]},{"cell_type":"markdown","source":["# Filter out Customers and Products in test and val set that do not appear in training set\n","\n","Prevent Cold Start problems during validation and testing. If a customer or product appears only in the validation or test set appears only in the validataion or test set, the model has never seen it before and cannot generate a valid prediction. Thus we will remove any rows in the validation or test sets which does not belong to any user in training set or the product is absent in the training set.\n","\n","We do not remove the rows from the training set."],"metadata":{"id":"608Mm0TBSq7z"}},{"cell_type":"code","source":["unique_customers_train = set(train_data['customer_id'].unique())\n","unique_products_train = set(train_data['product_id'].unique())\n","\n","val_data = val_data[val_data['customer_id'].isin(unique_customers_train) &\n","                    val_data['product_id'].isin(unique_products_train)].reset_index(drop=True)\n","\n","test_data = test_data[test_data['customer_id'].isin(unique_customers_train) &\n","                      test_data['product_id'].isin(unique_products_train)].reset_index(drop=True)\n","\n","print(val_data.shape)\n","print(test_data.shape)"],"metadata":{"id":"Q572_moTSr34","executionInfo":{"status":"ok","timestamp":1744605526191,"user_tz":-480,"elapsed":73,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e82c28b-878c-4552-d0ba-53cdd693b119"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["(17174, 26)\n","(32880, 26)\n"]}]},{"cell_type":"markdown","source":["# Create Data Loader"],"metadata":{"id":"NWUDdw36IuUt"}},{"cell_type":"code","source":["# Create ID to index mappings\n","user2idx = {user_id: idx for idx, user_id in enumerate(train_data['customer_id'].unique())}\n","item2idx = {item_id: idx for idx, item_id in enumerate(train_data['product_id'].unique())}\n","\n","# Map to new columns\n","train_data['user_idx'] = train_data['customer_id'].map(user2idx)\n","train_data['item_idx'] = train_data['product_id'].map(item2idx)\n","val_data['user_idx'] = val_data['customer_id'].map(user2idx)\n","val_data['item_idx'] = val_data['product_id'].map(item2idx)\n","test_data['user_idx'] = test_data['customer_id'].map(user2idx)\n","test_data['item_idx'] = test_data['product_id'].map(item2idx)\n","\n","\n","class ReviewsDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'customer_id': torch.tensor(row['user_idx'], dtype=torch.long),\n","            'product_id': torch.tensor(row['item_idx'], dtype=torch.long),\n","            'rating': torch.tensor(row['star_rating'], dtype=torch.float)\n","        }"],"metadata":{"id":"Sf5-Tjc0ItiQ","executionInfo":{"status":"ok","timestamp":1744605526394,"user_tz":-480,"elapsed":202,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation Functions\n","\n","- **ndcg_at_k**: Computes the Normalized Discounted Cumulative Gain (NDCG) at rank k for a single list of relevance. If the list contains fewer than k items, it will use actual_k = min(k, len(relevances)) to ensure fair computation.\n","\n","- **mean_ndcg_user_at_k**: Computes the mean NDCG@k across all users by grouping predicted scores and relevance labels per user, sorting by prediction, and applying ndcg_at_k. For each user, their items are sorted by predicted scores, and NDCG is computed using `ndcg_at_k` with actual_k = min(k, len(user_items)).\n","\n","- **mean_precision_user_at_k**: Computes the mean Precision@k across all users.\n","Precision@k is the proportion of relevant items (e.g., rating â‰¥ threshold) among the top-k predicted items for each user. For each user, top-k items are selected based on predicted scores. If the user has fewer than k items, actual_k = min(k, len(user_items)) is used.  \n","  Precision is calculated as:  \n","  `precision = (# of relevant items among top-k) / actual_k`  \n","  where an item is considered relevant if `rating â‰¥ threshold`.\n","\n","- **mean_recall_user_at_k**: Computes the mean Recall@k across all users.\n","Recall@k is the proportion of a user's relevant items (rating â‰¥ threshold) that are retrieved in the top-k predicted list. For each user, top-k items are selected based on predicted scores, and recall is calculated as:  \n","  `recall = (# of relevant items among top-k) / total number of relevant items for the user`  \n","  actual_k = min(k, len(user_items)) is used to handle users with fewer than k items.\n","\n","- **mean_f1_user_at_k**:  \n","  Computes the mean F1@k across all users, where F1 combines precision and recall.  \n","  For each user, top-k items are selected (using actual_k = min(k, len(user_items))), and F1 is calculated based on binarized relevance labels (`rating â‰¥ threshold`).  \n","  The predicted labels are assumed to be all 1s (e.g top-k are predicted as relevant)."],"metadata":{"id":"T-aa3NCcwrso"}},{"cell_type":"code","source":["def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)\n","    actual_k = min(k, len(relevances))\n","    if actual_k == 0:\n","        return 0.0\n","    relevances = relevances[:actual_k]\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for u, pred, rel in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, rel))\n","    ndcg_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [rel for _, rel in entries_sorted]\n","        ndcg_list.append(ndcg_at_k(relevances, k))\n","    return np.mean(ndcg_list) if ndcg_list else 0.0\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    precision_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","        rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        precision_list.append(np.sum(rels) / actual_k if actual_k > 0 else 0)\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    recall_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","\n","        all_rels = [1 if r >= threshold else 0 for _, r in entries]\n","        top_k_rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        total_relevant = np.sum(all_rels)\n","\n","        if total_relevant == 0:\n","            recall = 0.0\n","        else:\n","            recall = np.sum(top_k_rels) / total_relevant\n","        recall_list.append(recall)\n","    return np.mean(recall_list) if recall_list else 0.0\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, p, l in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((p, l))\n","\n","    f1_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        y_true = [int(l >= threshold) for _, l in entries_sorted[:actual_k]]\n","        y_pred = [1] * actual_k\n","        f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","    return np.mean(f1_list) if f1_list else 0.0"],"metadata":{"id":"R2okKje4wuBV","executionInfo":{"status":"ok","timestamp":1744605526468,"user_tz":-480,"elapsed":71,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Define the NCF model with GMF and MLP\n","\n","NCF class implements a Neural Collaborative Filtering model combining:\n","\n","- **GMF (Generalized Matrix Factorization)**: Element-wise product of user and item embeddings\n","\n","- **MLP (Multi-Layer Perceptron)**: Concatenated embeddings passed through FC layers\n","\n","- **Final prediction**: Merges GMF and MLP outputs to produce a predicted rating (1 to 5 scale)"],"metadata":{"id":"LIQJaTpPeCWj"}},{"cell_type":"markdown","source":["## Aggregate the outputs of GMF and MLP by **concatenation**"],"metadata":{"id":"KL4I1LYheKZh"}},{"cell_type":"code","source":["class NCF(nn.Module):\n","    def __init__(self, num_users, num_items, embedding_dim, dropout_rate=0.3):\n","        super(NCF, self).__init__()\n","        # Randomly initialized embedding layers\n","        self.customer_embeddings_gmf = nn.Embedding(num_users, embedding_dim)\n","        self.product_embeddings_gmf = nn.Embedding(num_items, embedding_dim)\n","\n","        self.customer_embeddings_mlp = nn.Embedding(num_users, embedding_dim)\n","        self.product_embeddings_mlp = nn.Embedding(num_items, embedding_dim)\n","\n","        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n","        self.bn1_mlp = nn.BatchNorm1d(128)\n","        self.dropout1_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc2_mlp = nn.Linear(128, 64)\n","        self.bn2_mlp = nn.BatchNorm1d(64)\n","        self.dropout2_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n","        self.bn1_combined = nn.BatchNorm1d(128)\n","        self.dropout1_combined = nn.Dropout(dropout_rate)\n","\n","        self.fc2_combined = nn.Linear(128, 1)\n","\n","    def forward(self, customer_id, product_id):\n","        customer_emb_gmf = self.customer_embeddings_gmf(customer_id)\n","        product_emb_gmf = self.product_embeddings_gmf(product_id)\n","        gmf_output = customer_emb_gmf * product_emb_gmf\n","\n","        customer_emb_mlp = self.customer_embeddings_mlp(customer_id)\n","        product_emb_mlp = self.product_embeddings_mlp(product_id)\n","        mlp_input = torch.cat([customer_emb_mlp, product_emb_mlp], dim=-1)\n","\n","        mlp_output = F.relu(self.bn1_mlp(self.fc1_mlp(mlp_input)))\n","        mlp_output = self.dropout1_mlp(mlp_output)\n","        mlp_output = F.relu(self.bn2_mlp(self.fc2_mlp(mlp_output)))\n","        mlp_output = self.dropout2_mlp(mlp_output)\n","\n","        combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n","        combined_output = F.relu(self.bn1_combined(self.fc1_combined(combined_input)))\n","        combined_output = self.dropout1_combined(combined_output)\n","\n","        output = self.fc2_combined(combined_output)\n","        return output.squeeze() * 4 + 1"],"metadata":{"id":"SIFqdHUTeIMY","executionInfo":{"status":"ok","timestamp":1744605526473,"user_tz":-480,"elapsed":33,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Model Training\n","\n","\n"],"metadata":{"id":"8UAUS1P9eRhN"}},{"cell_type":"markdown","source":["### Grid Search\n","\n","The grid search algorithm here will perform an exhaustive search to identify the best combination of hyperparameters (embedding_dim, learning_rate, batch_size, dropout_rate, num_epoches) for training the NCF model.\n","\n","For each configuration:\n","\n","1. A new NCF model is instantiated with the configuration parameters.\n","\n","2. The model is trained on the training set and evaluated on the validation set.\n","\n","3. The best model state (with lowest validation loss) is stored using early stopping.\n","\n","4. The configuration and model weights are saved if it performs better than all previous configurations.\n","\n","It will then report the best-performing configuration which we will use to train the final model on the combined training and validation data before evaluating it on our test data.\n","\n"],"metadata":{"id":"_-AdHtAYrbEI"}},{"cell_type":"markdown","source":["# NCF Model Trained on Full Dataset\n","The functions that we will be using are:\n","1. **train_full_model** : Trains one NCF model on the full dataset. It returns the trained model, best validation loss and the best model state\n","\n","2. **grid_search_full_data** : Performs grid search on the full dataset and returns the best hyperparameter configuration and the best model state.\n","\n","3. **evaluate_model**: Evaluates a trained NCF model on the test set.\n","\n","\n"],"metadata":{"id":"V4Tfp3bNecgB"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def train_full_model(train_df, val_df, config, model_state=None):\n","    loader = lambda df: DataLoader(ReviewsDataset(df), batch_size=config['batch_size'], shuffle=True)\n","    train_loader, val_loader = loader(train_df), loader(val_df)\n","    num_users = train_df['customer_id'].nunique()\n","    num_items = train_df['product_id'].nunique()\n","    model = NCF(num_users, num_items, config['embedding_dim'], config['dropout_rate']).to(device)\n","    if model_state: model.load_state_dict(model_state)\n","    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    criterion = nn.MSELoss()\n","\n","    best_loss, best_state, patience_counter = float('inf'), None, 0\n","    metrics = {}\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        for batch in train_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","\n","            optimizer.zero_grad()\n","            preds = model(user, item)\n","            loss = criterion(preds, label)\n","            loss.backward()\n","            optimizer.step()\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            val_all_preds = []\n","            val_all_labels = []\n","            val_all_users = []\n","            for batch in val_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                label = batch['rating'].to(device)\n","                preds = model(user, item)\n","                output = preds.squeeze().cpu().numpy()\n","                val_loss += criterion(preds, label).item()\n","                val_all_preds.extend(output)\n","                val_all_labels.extend(label.cpu().numpy())\n","                val_all_users.extend(user.cpu().numpy())\n","\n","        val_all_preds = np.array(val_all_preds)\n","        val_all_labels = np.array(val_all_labels)\n","        val_all_users = np.array(val_all_users)\n","\n","        rmse_val = np.sqrt(np.mean((val_all_preds - val_all_labels)**2))\n","        ndcg_val = mean_ndcg_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10)\n","        precision_val = mean_precision_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","        recall_val = mean_recall_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","        f1_val = mean_f1_user_at_k(val_all_users, val_all_preds, val_all_labels, k=10, threshold=4)\n","\n","        val_loss /= len(val_loader)\n","        if val_loss < best_loss:\n","            best_loss, best_state, patience_counter = val_loss, copy.deepcopy(model.state_dict()), 0\n","            metrics = {\n","                'rmse': rmse_val,\n","                'mse': val_loss,\n","                'ndcg@10': ndcg_val,\n","                'precision@10': precision_val,\n","                'recall@10': recall_val,\n","                'f1@10': f1_val\n","            }\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= config.get('patience', 5):\n","              break\n","    model.load_state_dict(best_state)\n","    return model, best_state, metrics\n","\n","def grid_search_full_data(train_data, val_data, param_grid):\n","    combos = list(itertools.product(*param_grid.values()))\n","    print(\"======================= GRID SEARCH =======================\")\n","    best_config, best_loss, best_state, best_metrics = None, float('inf'), None, {}\n","    for vals in combos:\n","        config = dict(zip(param_grid.keys(), vals))\n","        print(\"Running Config:\",config)\n","        model, state, metrics = train_full_model(train_data, val_data, config)\n","        val_loss = metrics['mse']\n","        if val_loss < best_loss:\n","            best_config, best_loss, best_state, best_metrics = config, val_loss, state, metrics\n","    return best_config, best_state, best_metrics\n","\n","def evaluate_model(model, test_data, batch_size=512):\n","    model.eval()\n","    loader = DataLoader(ReviewsDataset(test_data), batch_size=batch_size)\n","    criterion = nn.MSELoss()\n","    test_loss = 0\n","\n","    test_all_preds = []\n","    test_all_labels = []\n","    test_all_users = []\n","\n","    with torch.no_grad():\n","        for batch in loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","            preds = model(user, item)\n","            loss = criterion(preds, label)\n","            test_loss += loss.item()\n","\n","            test_all_preds.extend(preds.squeeze().cpu().numpy())\n","            test_all_labels.extend(label.cpu().numpy())\n","            test_all_users.extend(user.cpu().numpy())\n","    test_loss /= len(loader)\n","    # Convert lists to numpy arrays\n","    test_all_preds = np.array(test_all_preds)\n","    test_all_labels = np.array(test_all_labels)\n","    test_all_users = np.array(test_all_users)\n","\n","    # Compute evaluation metrics\n","    rmse_test = np.sqrt(np.mean((test_all_preds - test_all_labels)**2))\n","    ndcg_test = mean_ndcg_user_at_k(test_all_users, test_all_preds, test_all_labels, k=10)\n","    precision_test = mean_precision_user_at_k(test_all_users, test_all_preds, test_all_labels, k=10, threshold=4)\n","    recall_test = mean_recall_user_at_k(test_all_users, test_all_preds, test_all_labels, k=10, threshold=4)\n","    f1_test = mean_f1_user_at_k(test_all_users, test_all_preds, test_all_labels, k=10, threshold=4)\n","\n","    test_metrics = {\n","        'rmse': rmse_test,\n","        'mse': test_loss,\n","        'ndcg@10': ndcg_test,\n","        'precision@10': precision_test,\n","        'recall@10': recall_test,\n","        'f1@10': f1_test\n","    }\n","\n","    return test_metrics"],"metadata":{"id":"vB-SGqngejr6","executionInfo":{"status":"ok","timestamp":1744605526544,"user_tz":-480,"elapsed":66,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Final Model Training\n","\n","We will retrain the NCF model using the optimal hyperparameters identified through Grid Search, this time on the combined training and validation data. Lastly, the model is then evaluated on the test data."],"metadata":{"id":"3ywwIJV8tEzp"}},{"cell_type":"code","source":["# Define hyperparameter grid\n","param_grid = {\n","    'embedding_dim': [16, 64],          # keep both for low vs high capacity\n","    'learning_rate': [0.001],           # pick one reliable value\n","    'batch_size': [128, 512],           # small vs large batch\n","    'dropout_rate': [0.0, 0.3],         # low vs regular dropout\n","    'num_epochs': [20, 40]              # moderate vs longer training\n","}\n","\n","def run_final_pipeline_full_data(train_data, val_data, test_data, param_grid):\n","    output_path = os.path.join(project_dir, \"Model Results/NCF Random Embedding/Full Model/results_full_dataset.txt\")\n","    with open(output_path, \"w\") as f:\n","        best_config, best_state, best_metrics = grid_search_full_data(train_data, val_data, param_grid)\n","\n","        print(\"======Validation Metrics For Full Dataset======\\n\")\n","        print(f\"MSE: {best_metrics['mse']:.4f}, RMSE: {best_metrics['rmse']:.4f}, NDCG@10: {best_metrics['ndcg@10']}, Precision@10: {best_metrics['precision@10']}, Recall@10: {best_metrics['recall@10']}, F1@10: {best_metrics['f1@10']}\\n\")\n","        eval_results = best_metrics\n","        eval_results.update(best_config)\n","        df_val_results = pd.DataFrame([eval_results])\n","        df_val_results.to_csv(os.path.join(project_dir,\"Model Results/NCF Random Embedding/Full Model/val_results_full_dataset.csv\"), index=False)\n","\n","        train_val_data = pd.concat([train_data, val_data]).reset_index(drop=True)\n","        final_model, _, _ = train_full_model(train_val_data, val_data, best_config, best_state)\n","        test_metrics = evaluate_model(final_model, test_data)\n","\n","        test_results = {}\n","        test_results.update(test_metrics)\n","        test_results.update(best_config)\n","        df_test_results = pd.DataFrame([test_results])\n","        df_test_results.to_csv(os.path.join(project_dir,\"Model Results/NCF Random Embedding/Full Model/test_results_full_dataset.csv\"), index=False)\n","\n","        print(\"===== Test Results For Full Dataset======\\n\")\n","        print(f\"MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']:.4f}, NDCG@10: {test_metrics['ndcg@10']:.4f}, Precision@10: {test_metrics['precision@10']:.4f}, Recall@10: {test_metrics['recall@10']:.4f}, F1@10: {test_metrics['f1@10']}\")\n","\n","\n","        f.write(f\"======Validation Metrics For Full Dataset======\\n\")\n","        f.write(f\"MSE: {best_metrics['mse']:.4f}, RMSE: {best_metrics['rmse']:.4f}, NDCG@10: {best_metrics['ndcg@10']}, Precision@10: {best_metrics['precision@10']}, Recall@10: {best_metrics['recall@10']}, F1@10: {best_metrics['f1@10']}\\n\")\n","        f.write(f\"===== Test Results For Full Dataset======\\n\")\n","        f.write(f\"Full Dataset Test:  MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']:.4f}, NDCG@10: {test_metrics['ndcg@10']:.4f}, Precision@10: {test_metrics['precision@10']:.4f}, Recall@10: {test_metrics['recall@10']:.4f}, F1@10: {test_metrics['f1@10']}\\n\")"],"metadata":{"id":"R7YVplaHrdqT","executionInfo":{"status":"ok","timestamp":1744605526546,"user_tz":-480,"elapsed":18,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["run_final_pipeline_full_data(train_data, val_data, test_data, param_grid)"],"metadata":{"id":"OwxXz2oqF77Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1744623717029,"user_tz":-480,"elapsed":6876585,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"9da37e52-7855-4cc2-f063-556c6f467780"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["======================= GRID SEARCH =======================\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 16, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running Config: {'embedding_dim': 64, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","======Validation Metrics For Full Dataset======\n","\n","MSE: 0.9756, RMSE: 0.9890, NDCG@10: 0.9957900821234901, Precision@10: 0.8673749758242734, Recall@10: 0.87688927358679, F1@10: 0.8705876769228104\n","\n","===== Test Results For Full Dataset======\n","\n","MSE: 1.1621, RMSE: 1.0786, NDCG@10: 0.9718, Precision@10: 0.8597, Recall@10: 0.9344, F1@10: 0.8849569106924269\n"]}]},{"cell_type":"markdown","source":["# Check Stored Validation and Testing Results"],"metadata":{"id":"cMNiQvek9Xds"}},{"cell_type":"code","source":["#step5_2_1- NCF Model: Random Embeddings Full Dataset\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","\n","rand_emb_full_data_val_results = pd.read_csv(os.path.join(results_dir,\"NCF Random Embedding/Full Model/val_results_full_dataset.csv\"))\n","print(rand_emb_full_data_val_results.shape)\n","print(rand_emb_full_data_val_results.head())\n","\n","\n","rand_emb_by_full_data_test_results = pd.read_csv(os.path.join(results_dir,\"NCF Random Embedding/Full Model/test_results_full_dataset.csv\"))\n","print(rand_emb_by_full_data_test_results.shape)\n","print(rand_emb_by_full_data_test_results.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J7--da0B9Vdh","executionInfo":{"status":"ok","timestamp":1744625048046,"user_tz":-480,"elapsed":50,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"0df7746f-d17b-4c24-9778-72d9cfff5081"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 11)\n","       rmse       mse  ndcg@10  precision@10  recall@10     f1@10  \\\n","0  0.989009  0.975591  0.99579      0.867375   0.876889  0.870588   \n","\n","   embedding_dim  learning_rate  batch_size  dropout_rate  num_epochs  \n","0             64          0.001         128           0.3          40  \n","(1, 11)\n","       rmse       mse   ndcg@10  precision@10  recall@10     f1@10  \\\n","0  1.078569  1.162062  0.971801      0.859677   0.934431  0.884957   \n","\n","   embedding_dim  learning_rate  batch_size  dropout_rate  num_epochs  \n","0             64          0.001         128           0.3          40  \n"]}]}]}