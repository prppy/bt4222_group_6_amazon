{"cells":[{"cell_type":"markdown","metadata":{"id":"sXYkhGwl2pby"},"source":["# Neural Collaborative Filtering with Custom Embeddings"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":56620,"status":"ok","timestamp":1744707204804,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"J19TNojDIEkx","outputId":"7b4d9cc1-5536-478d-f9b3-51a2af3d3325"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"cyvfGjxo_HkB","executionInfo":{"status":"ok","timestamp":1744707212732,"user_tz":-480,"elapsed":7931,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}}},"outputs":[],"source":["import os\n","import random\n","import copy\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from tqdm import tqdm\n","from collections import defaultdict\n","from sklearn.metrics import f1_score\n","\n","# Build Custom Customer and Product Embedding\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler\n","\n","data_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/data'\n","project_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon'"]},{"cell_type":"markdown","metadata":{"id":"dlowKi5Y-WfV"},"source":["# Step 1: Load dataset"]},{"cell_type":"markdown","metadata":{"id":"5cbayT1uKOyM"},"source":["Previously in step1_data_preprocessing.ipynb, We have split the df_reviews dataset into training, testing and validation samples for each user, following chronological order and using the early 70% of each user's interactions for training, followed by the next 15% for validation and the last 15% for testing."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19332,"status":"ok","timestamp":1744605571704,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"ew_y5LJ3J7ln","outputId":"c9b00bd1-8f17-4a88-dd0e-81603e171473"},"outputs":[{"output_type":"stream","name":"stdout","text":["   customer_id  product_id  product_parent  \\\n","0        11960  B00LCJAW06       219600481   \n","1        11960  B008OTSEXY       682436048   \n","2        11960  B00KJ15KGY        32170248   \n","3        11960  B008ZL49WQ       614364353   \n","4        11960  B002WRGE5O       928204157   \n","\n","                                       product_title product_category  \\\n","0  Persian-Rugs T1007 Abstract Modern Area Rug Ca...        Furniture   \n","1  Flash Furniture High Back Black Ribbed Upholst...        Furniture   \n","2  Jackson Pollock Inspired Coffee Glass Table w/...        Furniture   \n","3                                  Eaze Lounge Chair        Furniture   \n","4         Walker Edison L-Shaped Glass Computer Desk        Furniture   \n","\n","   star_rating  helpful_votes  total_votes vine verified_purchase  ...  \\\n","0            4              1            1    N                 Y  ...   \n","1            4              0            0    N                 Y  ...   \n","2            4              1            1    N                 Y  ...   \n","3            4              0            1    N                 Y  ...   \n","4            3              0            0    N                 Y  ...   \n","\n","  time_since_last_purchase parent_product_average_rating  \\\n","0                        0                      4.500000   \n","1                        0                      4.384615   \n","2                        0                      4.500000   \n","3                       42                      4.800000   \n","4                        0                      4.000000   \n","\n","  product_id_average_rating sum_helpfulvotes sum_totalvotes  \\\n","0                  4.500000                2              3   \n","1                  4.571429                2              3   \n","2                  4.500000                2              3   \n","3                  4.000000                2              3   \n","4                  3.500000                2              3   \n","\n","                                         full_review  sentiments  \\\n","0  Quick delivery and quality Rug.. Delivered in ...    positive   \n","1  Looks great and feels nice.. Very comfortable ...    positive   \n","2  Buy from these guys. They take what they do se...    positive   \n","3  Pretty but Pricey. Chair is very pretty. A bit...    positive   \n","4  Desk quality is fine. Issues with predrilled h...    negative   \n","\n","   purchases_last_4_years  monthly_purchase_frequency  cluster  \n","0                       5                    0.104167        1  \n","1                       5                    0.104167        1  \n","2                       5                    0.104167        1  \n","3                       5                    0.104167        1  \n","4                       5                    0.104167        1  \n","\n","[5 rows x 26 columns]\n","Training Data Shape: (116086, 26)\n","Testing Data Shape: (43223, 26)\n","Validation Data Shape: (21664, 26)\n","Index(['customer_id', 'product_id', 'product_parent', 'product_title',\n","       'product_category', 'star_rating', 'helpful_votes', 'total_votes',\n","       'vine', 'verified_purchase', 'review_headline', 'review_body',\n","       'review_date', 'temporal_purchase_sequence', 'previous_purchase',\n","       'subsequent_purchase', 'time_since_last_purchase',\n","       'parent_product_average_rating', 'product_id_average_rating',\n","       'sum_helpfulvotes', 'sum_totalvotes', 'full_review', 'sentiments',\n","       'purchases_last_4_years', 'monthly_purchase_frequency', 'cluster'],\n","      dtype='object')\n","Index(['customer_id', 'product_id', 'product_parent', 'product_title',\n","       'product_category', 'star_rating', 'helpful_votes', 'total_votes',\n","       'vine', 'verified_purchase', 'review_headline', 'review_body',\n","       'review_date', 'temporal_purchase_sequence', 'previous_purchase',\n","       'subsequent_purchase', 'time_since_last_purchase',\n","       'parent_product_average_rating', 'product_id_average_rating',\n","       'sum_helpfulvotes', 'sum_totalvotes', 'full_review', 'sentiments',\n","       'purchases_last_4_years', 'monthly_purchase_frequency', 'cluster'],\n","      dtype='object')\n","sentiments\n","positive    287894\n","negative     49135\n","Name: count, dtype: int64\n"]}],"source":["train_data = pd.read_csv(os.path.join(data_dir,\"train_data.csv\"))\n","test_data = pd.read_csv(os.path.join(data_dir,\"test_data.csv\"))\n","val_data = pd.read_csv(os.path.join(data_dir,\"val_data.csv\"))\n","\n","df_reviews = pd.read_csv(os.path.join(data_dir,\"filtered_reviews_with_features_and_clusters.csv\"))\n","print(df_reviews.head())\n","\n","print(f\"Training Data Shape: {train_data.shape}\")\n","print(f\"Testing Data Shape: {test_data.shape}\")\n","print(f\"Validation Data Shape: {val_data.shape}\")\n","\n","print(train_data.columns)\n","print(test_data.columns)\n","\n","print(df_reviews['sentiments'].value_counts())\n"]},{"cell_type":"markdown","metadata":{"id":"608Mm0TBSq7z"},"source":["# Filter out Customers and Products in test and val set that do not appear in training set\n","\n","Prevent Cold Start problems during validation and testing. If a customer or product appears only in the validation or test set appears only in the validataion or test set, the model has never seen it before and cannot generate a valid prediction. Thus we will remove any rows in the validation or test sets which does not belong to any user in training set or the product is absent in the training set.\n","\n","We do not remove the rows from the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57,"status":"ok","timestamp":1744605571762,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"},"user_tz":-480},"id":"Q572_moTSr34","outputId":"f687ca74-6aa8-48c2-dfeb-59260442ae8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["(17174, 26)\n","(32880, 26)\n"]}],"source":["unique_customers_train = set(train_data['customer_id'].unique())\n","unique_products_train = set(train_data['product_id'].unique())\n","\n","val_data = val_data[val_data['customer_id'].isin(unique_customers_train) &\n","                    val_data['product_id'].isin(unique_products_train)].reset_index(drop=True)\n","\n","test_data = test_data[test_data['customer_id'].isin(unique_customers_train) &\n","                      test_data['product_id'].isin(unique_products_train)].reset_index(drop=True)\n","\n","print(val_data.shape)\n","print(test_data.shape)"]},{"cell_type":"markdown","metadata":{"id":"NWUDdw36IuUt"},"source":["# Create Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf5-Tjc0ItiQ"},"outputs":[],"source":["# Create ID to index mappings\n","user2idx = {user_id: idx for idx, user_id in enumerate(train_data['customer_id'].unique())}\n","item2idx = {item_id: idx for idx, item_id in enumerate(train_data['product_id'].unique())}\n","\n","# Map to new columns\n","train_data['user_idx'] = train_data['customer_id'].map(user2idx)\n","train_data['item_idx'] = train_data['product_id'].map(item2idx)\n","val_data['user_idx'] = val_data['customer_id'].map(user2idx)\n","val_data['item_idx'] = val_data['product_id'].map(item2idx)\n","test_data['user_idx'] = test_data['customer_id'].map(user2idx)\n","test_data['item_idx'] = test_data['product_id'].map(item2idx)\n","\n","class ReviewsDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        row = self.data.iloc[idx]\n","        return {\n","            'customer_id': torch.tensor(row['user_idx'], dtype=torch.long),\n","            'product_id': torch.tensor(row['item_idx'], dtype=torch.long),\n","            'rating': torch.tensor(row['star_rating'], dtype=torch.float)\n","        }\n","\n","# Create DataLoader\n","train_dataset = ReviewsDataset(train_data)\n","test_dataset = ReviewsDataset(test_data)\n","val_dataset = ReviewsDataset(val_data)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WpW5fl31iBx6"},"source":["# Building Customer Embeddings\n","\n","The original **df_reviews** will be used to build the custom customer and product embeddings. These custom embeddings are meant to reflect historical behaviour or characteristics of customers/products\n","\n","The customer embeddings will constructed by aggregating features within df_reviews by customer_id to find a customer's:\n"," - Purchase Frequency (Indicate how active a customer is)\n"," - Time Since Last Purchase (Indicate how active a customer is)\n"," - Average Star Rating (Overall Customer satisfaction across all of his purchases)\n"," - Total Vine Reviews (Measure of Credibility of his Reviews)\n"," - Total Helpful Votes (Measure the Credibility of his Reviews)\n"," - Total Votes (Measure the Credibility of his Reviews)\n"," - Average Sentiment (Overall Customer satisfaction across all of his purchases\n","\n","These embeddings are more informative than a randomly intialized embedding in typical recommnedation systems. The choice of specific features injects domain knowledge into the model.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uxar_lIqh6Nf"},"outputs":[],"source":["def build_customer_embeddings(df_reviews, embedding_dim):\n","    cache_path = os.path.join(project_dir, f\"Model Results/NCF Custom Embedding/Full Model/cache/cust_emb_{embedding_dim}.csv\")\n","    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","    if os.path.exists(cache_path):\n","        #print(f\"Using cached customer embeddings from {cache_path}\")\n","        return pd.read_csv(cache_path, index_col=\"customer_id\")\n","\n","    agg = df_reviews.groupby('customer_id').agg({\n","        'monthly_purchase_frequency': 'mean',\n","        'time_since_last_purchase': 'mean',\n","        'star_rating': 'mean',\n","        'vine': lambda x: (x == \"Y\").sum(),\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean()\n","    }).fillna(0).reset_index()\n","\n","    cust_ids = agg['customer_id']\n","    X = StandardScaler().fit_transform(agg.drop(columns='customer_id'))\n","    num_features = X.shape[1]\n","\n","    if embedding_dim > num_features:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features available.\")\n","\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        X = pca.fit_transform(X)\n","        if (pca.explained_variance_ratio_ > 1e-6).sum() < embedding_dim:\n","            raise ValueError(f\"PCA found fewer than {embedding_dim} meaningful components.\")\n","\n","    df = pd.DataFrame(X, index=cust_ids)\n","    df.index.name = 'customer_id'\n","    df.to_csv(cache_path)\n","    return df"]},{"cell_type":"markdown","metadata":{"id":"1JV0DADflaMj"},"source":["## Building Product Embeddings\n","\n","The product embeddings will constructed by aggregating features within df_reviews by product_id to find a product's:\n","- Mean Star Rating that it received (Customer satisfaction)\n","- Total Helpful Votes given to all its reviews (Quality of customer feedback)\n","- Total Votes given to all its reviews (Review Engagement by customers)\n","- Average sentiment (1 is Positive and 0 is Negative)\n","- Total Number of Vine Reviews (Number of Credible Reviews)\n","- Total Product Sales (Demand for Product)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbSRPfctldXM"},"outputs":[],"source":["def build_product_embeddings(df_reviews, embedding_dim):\n","    cache_path = os.path.join(project_dir, f\"Model Results/NCF Custom Embedding/Full Model/cache/prod_emb_{embedding_dim}.csv\")\n","    os.makedirs(os.path.dirname(cache_path), exist_ok=True)\n","    if os.path.exists(cache_path):\n","        #print(f\"Using cached product embeddings from {cache_path}\")\n","        return pd.read_csv(cache_path, index_col=\"product_id\")\n","\n","    agg = df_reviews.groupby('product_id').agg({\n","        'star_rating': 'mean',\n","        'helpful_votes': 'sum',\n","        'total_votes': 'sum',\n","        'sentiments': lambda x: (x == 'positive').mean(),\n","        'vine': lambda x: (x == 'Y').sum(),\n","        'product_id': 'count'  # will be renamed\n","    }).rename(columns={'product_id': 'sales_volume'}).fillna(0).reset_index()\n","\n","    prod_ids = agg['product_id']\n","    X = StandardScaler().fit_transform(agg.drop(columns='product_id'))\n","    num_features = X.shape[1]\n","\n","    if embedding_dim > num_features:\n","        raise ValueError(f\"Requested embedding_dim={embedding_dim}, but only {num_features} features available.\")\n","\n","    if embedding_dim < num_features:\n","        pca = PCA(n_components=embedding_dim)\n","        X = pca.fit_transform(X)\n","        if (pca.explained_variance_ratio_ > 1e-6).sum() < embedding_dim:\n","            raise ValueError(f\"PCA found fewer than {embedding_dim} meaningful components.\")\n","\n","    df = pd.DataFrame(X, index=prod_ids)\n","    df.index.name = 'product_id'\n","    df.to_csv(cache_path)\n","    return df\n"]},{"cell_type":"markdown","metadata":{"id":"LIQJaTpPeCWj"},"source":["# Define the NCF model with GMF and MLP\n","\n","NCF class implements a Neural Collaborative Filtering model combining:\n","\n","- **GMF (Generalized Matrix Factorization)**: Element-wise product of user and item embeddings\n","\n","- **MLP (Multi-Layer Perceptron)**: Concatenated embeddings passed through FC layers\n","\n","- **Final prediction**: Merges GMF and MLP outputs to produce a predicted rating (1 to 5 scale)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SIFqdHUTeIMY"},"outputs":[],"source":["class NCF(nn.Module):\n","    def __init__(self, customer_embedding_matrix_gmf, product_embedding_matrix_gmf,\n","                 customer_embedding_matrix_mlp, product_embedding_matrix_mlp, embedding_dim, dropout_rate = 0.3):\n","        super(NCF, self).__init__()\n","        # GMF Components\n","        self.customer_embeddings_gmf = nn.Embedding.from_pretrained(customer_embedding_matrix_gmf, freeze=False)\n","        self.product_embeddings_gmf = nn.Embedding.from_pretrained(product_embedding_matrix_gmf, freeze=False)\n","\n","        # MLP Components\n","        self.customer_embeddings_mlp = nn.Embedding.from_pretrained(customer_embedding_matrix_mlp, freeze=False)\n","        self.product_embeddings_mlp = nn.Embedding.from_pretrained(product_embedding_matrix_mlp, freeze=False)\n","\n","        self.fc1_mlp = nn.Linear(2 * embedding_dim, 128)\n","        self.bn1_mlp = nn.BatchNorm1d(128)\n","        self.dropout1_mlp = nn.Dropout(dropout_rate)\n","\n","        self.fc2_mlp = nn.Linear(128, 64)\n","        self.bn2_mlp = nn.BatchNorm1d(64)\n","        self.dropout2_mlp = nn.Dropout(dropout_rate)\n","\n","        # Final layers\n","        self.fc1_combined = nn.Linear(embedding_dim + 64, 128)\n","        self.bn1_combined = nn.BatchNorm1d(128)\n","        self.dropout1_combined = nn.Dropout(dropout_rate)\n","\n","        self.fc2_combined = nn.Linear(128, 1)\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    def forward(self, customer_id, product_id):\n","        # GMF\n","        customer_emb_gmf = self.customer_embeddings_gmf(customer_id)\n","        product_emb_gmf = self.product_embeddings_gmf(product_id)\n","        gmf_output = customer_emb_gmf * product_emb_gmf\n","\n","        # MLP\n","        customer_emb_mlp = self.customer_embeddings_mlp(customer_id)\n","        product_emb_mlp = self.product_embeddings_mlp(product_id)\n","        mlp_input = torch.cat([customer_emb_mlp, product_emb_mlp], dim=-1)\n","\n","        mlp_output = F.relu(self.bn1_mlp(self.fc1_mlp(mlp_input)))\n","        mlp_output = self.dropout1_mlp(mlp_output)\n","\n","        mlp_output = F.relu(self.bn2_mlp(self.fc2_mlp(mlp_output)))\n","        mlp_output = self.dropout2_mlp(mlp_output)\n","\n","        # Combine GMF and MLP\n","        combined_input = torch.cat([gmf_output, mlp_output], dim=-1)\n","        combined_output = F.relu(self.bn1_combined(self.fc1_combined(combined_input)))\n","        combined_output = self.dropout1_combined(combined_output)\n","\n","        # Final layer & output scaling (1–5 range)\n","        output = self.fc2_combined(combined_output)\n","        return output.squeeze() * 4 + 1"]},{"cell_type":"markdown","metadata":{"id":"8UAUS1P9eRhN"},"source":["# NCF Model Trained on Full Dataset\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4o7WFC67-ogu"},"source":["# Evaluation Functions\n","\n","- **ndcg_at_k**: Computes the Normalized Discounted Cumulative Gain (NDCG) at rank k for a single list of relevance. If the list contains fewer than k items, it will use actual_k = min(k, len(relevances)) to ensure fair computation.\n","\n","- **mean_ndcg_user_at_k**: Computes the mean NDCG@k across all users by grouping predicted scores and relevance labels per user, sorting by prediction, and applying ndcg_at_k. For each user, their items are sorted by predicted scores, and NDCG is computed using `ndcg_at_k` with actual_k = min(k, len(user_items)).\n","\n","- **mean_precision_user_at_k**: Computes the mean Precision@k across all users.\n","Precision@k is the proportion of relevant items (e.g., rating ≥ threshold) among the top-k predicted items for each user. For each user, top-k items are selected based on predicted scores. If the user has fewer than k items, actual_k = min(k, len(user_items)) is used.  \n","  Precision is calculated as:  \n","  `precision = (# of relevant items among top-k) / actual_k`  \n","  where an item is considered relevant if `rating ≥ threshold`.\n","\n","- **mean_recall_user_at_k**: Computes the mean Recall@k across all users.\n","Recall@k is the proportion of a user's relevant items (rating ≥ threshold) that are retrieved in the top-k predicted list. For each user, top-k items are selected based on predicted scores, and recall is calculated as:  \n","  `recall = (# of relevant items among top-k) / total number of relevant items for the user`  \n","  actual_k = min(k, len(user_items)) is used to handle users with fewer than k items.\n","\n","- **mean_f1_user_at_k**:  \n","  Computes the mean F1@k across all users, where F1 combines precision and recall.  \n","  For each user, top-k items are selected (using actual_k = min(k, len(user_items))), and F1 is calculated based on binarized relevance labels (`rating ≥ threshold`).  \n","  The predicted labels are assumed to be all 1s (e.g top-k are predicted as relevant)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6j0Vq7P-bTu"},"outputs":[],"source":["def ndcg_at_k(relevances, k):\n","    relevances = np.asarray(relevances, dtype=np.float64)\n","    actual_k = min(k, len(relevances))\n","    if actual_k == 0:\n","        return 0.0\n","    relevances = relevances[:actual_k]\n","    dcg = np.sum((2 ** relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    ideal_relevances = np.sort(relevances)[::-1]\n","    idcg = np.sum((2 ** ideal_relevances - 1) / np.log2(np.arange(2, actual_k + 2)))\n","    return dcg / idcg if idcg > 0 else 0.0\n","\n","def mean_ndcg_user_at_k(all_users, all_preds, all_labels, k=10):\n","    user_data = defaultdict(list)\n","    for u, pred, rel in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, rel))\n","    ndcg_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        relevances = [rel for _, rel in entries_sorted]\n","        ndcg_list.append(ndcg_at_k(relevances, k))\n","    return np.mean(ndcg_list) if ndcg_list else 0.0\n","\n","def mean_precision_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    precision_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","        rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        precision_list.append(np.sum(rels) / actual_k if actual_k > 0 else 0)\n","    return np.mean(precision_list) if precision_list else 0.0\n","\n","def mean_recall_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, pred, label in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((pred, label))\n","\n","    recall_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        top_k = entries_sorted[:actual_k]\n","\n","        all_rels = [1 if r >= threshold else 0 for _, r in entries]\n","        top_k_rels = [1 if r >= threshold else 0 for _, r in top_k]\n","        total_relevant = np.sum(all_rels)\n","\n","        if total_relevant == 0:\n","            recall = 0.0\n","        else:\n","            recall = np.sum(top_k_rels) / total_relevant\n","        recall_list.append(recall)\n","    return np.mean(recall_list) if recall_list else 0.0\n","\n","def mean_f1_user_at_k(all_users, all_preds, all_labels, k=10, threshold=4):\n","    user_data = defaultdict(list)\n","    for u, p, l in zip(all_users, all_preds, all_labels):\n","        user_data[u].append((p, l))\n","\n","    f1_list = []\n","    for entries in user_data.values():\n","        entries_sorted = sorted(entries, key=lambda x: x[0], reverse=True)\n","        actual_k = min(k, len(entries_sorted))\n","        y_true = [int(l >= threshold) for _, l in entries_sorted[:actual_k]]\n","        y_pred = [1] * actual_k\n","        f1_list.append(f1_score(y_true, y_pred, zero_division=0))\n","    return np.mean(f1_list) if f1_list else 0.0"]},{"cell_type":"markdown","metadata":{"id":"_-AdHtAYrbEI"},"source":["# Grid Search\n","\n","The grid search algorithm here will perform an exhaustive search to identify the best combination of hyperparamters (embedding_dim, learning_rate, batch_size, dropout_rate, num_epoches) for training the NCF model.\n","\n","For each configuration:\n","\n","1. Custom embeddings for users and products are generated using PCA on the training data based on the current embedding_dim.\n","\n","2. A new NCF model is instantiated with the configuration parameters.\n","\n","3. The model is trained on the training set and evaluated on the validation set.\n","\n","4. The best model state (with lowest validation loss) is stored using early stopping.\n","\n","5. The configuration and model weights are saved if it performs better than all previous configurations.\n","\n","It will then report the best-performing configuratuon which we will use to train the final model on the combined training and validation data before evaluating it on our test data.\n","\n","\n","**Embeddings**\n","\n","During grid search, both customer_embedding and prod_embedding is build using the train_data. The embedding_dim is changing in each iteration of grid search and PCA must be redone with each new embedding dimension.\n","\n","Note: The maximum embedding_dim for Grid Search is limited by the number of features that I used to form the customer_embedding and product_embedding respectively.\n"]},{"cell_type":"code","source":["def train_full_model_with_custom_embeddings(train_data, val_data, df_reviews, config, cache):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    embedding_dim = config['embedding_dim']\n","    key = f\"train-{embedding_dim}\"\n","    if key not in cache:\n","        # cust_emb = torch.tensor(build_customer_embeddings(df_reviews[df_reviews['customer_id'].isin(train_data['customer_id'])], embedding_dim).values, dtype=torch.float32)\n","        # prod_emb = torch.tensor(build_product_embeddings(df_reviews[df_reviews['product_id'].isin(train_data['product_id'])], embedding_dim).values, dtype=torch.float32)\n","        cust_emb = torch.tensor(build_customer_embeddings(train_data, embedding_dim).values, dtype=torch.float32)\n","        prod_emb = torch.tensor(build_product_embeddings(train_data, embedding_dim).values, dtype=torch.float32)\n","        cache[key] = (cust_emb, prod_emb)\n","    else:\n","        cust_emb, prod_emb = cache[key]\n","\n","    model = NCF(cust_emb, prod_emb, cust_emb.clone(), prod_emb.clone(), embedding_dim=embedding_dim, dropout_rate=config['dropout_rate']).to(device)\n","\n","    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n","    criterion = nn.MSELoss()\n","    train_loader = DataLoader(ReviewsDataset(train_data), batch_size=config['batch_size'], shuffle=True)\n","    val_loader = DataLoader(ReviewsDataset(val_data), batch_size=config['batch_size'])\n","\n","    best_val_loss = float('inf')\n","    best_model_state = None\n","    best_metrics = {}\n","    patience_counter = 0\n","\n","    for epoch in range(config['num_epochs']):\n","        model.train()\n","        for batch in train_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(user, item)\n","            loss = criterion(output, label)\n","            loss.backward()\n","            optimizer.step()\n","\n","        model.eval()\n","        val_loss, preds, labels, users = 0, [], [], []\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                user = batch['customer_id'].to(device)\n","                item = batch['product_id'].to(device)\n","                label = batch['rating'].to(device)\n","\n","                output = model(user, item)\n","                val_loss += criterion(output, label).item()\n","\n","                preds.extend(output.detach().cpu().numpy())\n","                labels.extend(label.detach().cpu().numpy())\n","                users.extend(user.detach().cpu().numpy())\n","\n","        val_loss /= len(val_loader)\n","        preds, labels, users = np.array(preds), np.array(labels), np.array(users)\n","        rmse = np.sqrt(np.mean((preds - labels) ** 2))\n","        ndcg = mean_ndcg_user_at_k(users, preds, labels, k=10)\n","        precision = mean_precision_user_at_k(users, preds, labels, k=10, threshold=4)\n","        recall = mean_recall_user_at_k(users, preds, labels, k=10, threshold=4)\n","        f1_val = mean_f1_user_at_k(users, preds, labels, k=10, threshold=4)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_model_state = copy.deepcopy(model.state_dict())\n","            best_metrics = {\n","                'mse': val_loss,\n","                'rmse': rmse,\n","                'ndcg@10': ndcg,\n","                'precision@10': precision,\n","                'recall@10': recall,\n","                'f1@10': f1_val\n","            }\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= config.get('patience', 5):\n","                break\n","\n","    model.load_state_dict(best_model_state)\n","    return model, best_model_state, best_metrics\n","\n","def evaluate_model(model, test_data, batch_size=512):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.eval()\n","    criterion = nn.MSELoss()\n","\n","    test_loader = DataLoader(ReviewsDataset(test_data), batch_size=batch_size)\n","    preds, labels, users = [], [], []\n","    total_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            user = batch['customer_id'].to(device)\n","            item = batch['product_id'].to(device)\n","            label = batch['rating'].to(device)\n","            output = model(user, item)\n","            total_loss += criterion(output, label).item()\n","\n","            preds.extend(output.cpu().numpy())\n","            labels.extend(label.cpu().numpy())\n","            users.extend(user.cpu().numpy())\n","\n","    preds, labels, users = np.array(preds), np.array(labels), np.array(users)\n","    rmse = np.sqrt(np.mean((preds - labels) ** 2))\n","    ndcg = mean_ndcg_user_at_k(users, preds, labels, k=10)\n","    precision = mean_precision_user_at_k(users, preds, labels, k=10, threshold=4)\n","    recall = mean_recall_user_at_k(users, preds, labels, k=10, threshold=4)\n","    f1_val = mean_f1_user_at_k(users, preds, labels, k=10, threshold=4)\n","\n","    return {\n","        'mse': total_loss / len(test_loader),\n","        'rmse': rmse,\n","        'ndcg@10': ndcg,\n","        'precision@10': precision,\n","        'recall@10': recall,\n","        'f1@10': f1_val\n","    }\n","\n"],"metadata":{"id":"7ZxhVdwP-9Wn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ywwIJV8tEzp"},"source":["### Final Model Training + Evaluation\n","\n","We will retrain the NCF model using the optimal hyperparameters identified through Grid Search, this time on the combined training and validation data. Lastly, the model is then evaluated on the test data.\n","\n","**Embeddings**\n","\n","The final model will be intialized using customer and product embeddings that are forming using the (train_data + val_data) as we want our final model to have the most informed embeddings for evaluation on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5WzovdIbtO9e"},"outputs":[],"source":["def run_final_pipeline_full_data_custom_embeddings(train_data, val_data, test_data, df_reviews, param_grid, project_dir):\n","    from itertools import product\n","\n","    cache = {}\n","    param_keys = list(param_grid.keys())\n","    grid_log = []\n","    best_loss = float('inf')\n","    best_config = None\n","    best_state = None\n","    best_metrics = None\n","\n","    print(\"\\n======================= GRID SEARCH =======================\")\n","    for values in product(*param_grid.values()):\n","        config = dict(zip(param_keys, values))\n","        print(f\"Running config: {config}\")\n","        model, state, metrics = train_full_model_with_custom_embeddings(train_data, val_data, df_reviews, config, cache)\n","        row = metrics.copy()\n","        row.update(config)\n","        grid_log.append(row)\n","\n","        if metrics['mse'] < best_loss:\n","            best_loss = metrics['mse']\n","            best_config = config\n","            best_state = state\n","            best_metrics = metrics\n","\n","    pd.DataFrame(grid_log).to_csv(os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/grid_search_log.csv\"), index=False)\n","    pd.DataFrame([{**best_metrics, **best_config}]).to_csv(os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/full_model_validation_results.csv\"), index=False)\n","\n","    final_train = pd.concat([train_data, val_data]).reset_index(drop=True)\n","    embedding_dim = best_config['embedding_dim']\n","    cust_final = torch.tensor(build_customer_embeddings(final_train, embedding_dim).values, dtype=torch.float32)\n","    prod_final = torch.tensor(build_product_embeddings(final_train, embedding_dim).values, dtype=torch.float32)\n","\n","    torch.save(cust_final, os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/best_customer_embedding.pt\"))\n","    torch.save(prod_final, os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/best_product_embedding.pt\"))\n","\n","    model = NCF(cust_final, prod_final, cust_final.clone(), prod_final.clone(), embedding_dim, best_config['dropout_rate']).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n","    optimizer = torch.optim.Adam(model.parameters(), lr=best_config['learning_rate'])\n","    criterion = nn.MSELoss()\n","    loader = DataLoader(ReviewsDataset(final_train), batch_size=best_config['batch_size'], shuffle=True)\n","\n","    model.train()\n","    for epoch in range(best_config['num_epochs']):\n","        epoch_loss = 0\n","        for batch in loader:\n","            user = batch['customer_id'].to(model.device)\n","            item = batch['product_id'].to(model.device)\n","            label = batch['rating'].to(model.device)\n","            optimizer.zero_grad()\n","            loss = criterion(model(user, item), label)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","    test_metrics = evaluate_model(model, test_data)\n","    pd.DataFrame([{**test_metrics, **best_config}]).to_csv(\n","        os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/full_model_testing_results.csv\"), index=False\n","    )\n","\n","    print(\"====== Validation Metrics For Full Dataset with Custom Embeddings ======\")\n","    print(f\"Best Config: {best_config}\")\n","    print(f\"MSE: {best_metrics['mse']:.4f}, RMSE: {best_metrics['rmse']:.4f}, NDCG@10: {best_metrics['ndcg@10']:.4f}, Precision@10: {best_metrics['precision@10']:.4f}, Recall@10: {best_metrics['recall@10']:.4f}, F1@10: {best_metrics['f1@10']}\\n\")\n","\n","    print(\"====== Test Metrics After Final Retraining ======\")\n","    print(f\"MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']:.4f}, NDCG@10: {test_metrics['ndcg@10']:.4f}, Precision@10: {test_metrics['precision@10']:.4f}, Recall@10: {test_metrics['recall@10']:.4f}, F1@10: {test_metrics['f1@10']}\\n\")\n","\n","    with open(os.path.join(project_dir, \"Model Results/NCF Custom Embedding/Full Model/results_full_dataset.txt\"), \"w\") as f:\n","        f.write(\"====== Validation Metrics For Full Dataset with Custom Embeddings ======\\n\")\n","        f.write(f\"Best Config: {best_config}\\n\")\n","        f.write(f\"MSE: {best_metrics['mse']:.4f}, RMSE: {best_metrics['rmse']:.4f}, NDCG@10: {best_metrics['ndcg@10']:.4f}, Precision@10: {best_metrics['precision@10']:.4f}, Recall@10: {best_metrics['recall@10']:.4f}, F1@10: {best_metrics['f1@10']}\\n\")\n","        f.write(\"====== Test Metrics After Final Retraining ======\\n\")\n","        f.write(f\"MSE: {test_metrics['mse']:.4f}, RMSE: {test_metrics['rmse']:.4f}, NDCG@10: {test_metrics['ndcg@10']:.4f}, Precision@10: {test_metrics['precision@10']:.4f}, Recall@10: {test_metrics['recall@10']:.4f}, F1@10: {test_metrics['f1@10']}\\n\")\n"]},{"cell_type":"code","source":["run_final_pipeline_full_data_custom_embeddings(\n","    train_data=train_data,\n","    val_data=val_data,\n","    test_data=test_data,\n","    df_reviews=df_reviews,\n","    param_grid = {\n","        'embedding_dim': [5,6],             # keep both for low vs high capacity\n","        'learning_rate': [0.001],           # pick one reliable value\n","        'batch_size': [128, 512],           # small vs large batch\n","        'dropout_rate': [0.0, 0.3],         # low vs regular dropout\n","        'num_epochs': [20, 40]              # moderate vs longer training\n","    },\n","    project_dir = project_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JMH55mqB1yE","executionInfo":{"status":"ok","timestamp":1744615017360,"user_tz":-480,"elapsed":9445276,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"e88c0ad4-4e6b-4c4d-e96e-5093904de4c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================= GRID SEARCH =======================\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 5, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.3, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.0, 'num_epochs': 40}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 20}\n","Running config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 512, 'dropout_rate': 0.3, 'num_epochs': 40}\n","====== Validation Metrics For Full Dataset with Custom Embeddings ======\n","Best Config: {'embedding_dim': 6, 'learning_rate': 0.001, 'batch_size': 128, 'dropout_rate': 0.0, 'num_epochs': 20}\n","MSE: 0.8928, RMSE: 0.9448, NDCG@10: 0.9964, Precision@10: 0.8674, Recall@10: 0.8769, F1@10: 0.8705876769228104\n","\n","====== Test Metrics After Final Retraining ======\n","MSE: 1.8600, RMSE: 1.1922, NDCG@10: 0.9716, Precision@10: 0.8597, Recall@10: 0.9344, F1@10: 0.8849535203780803\n","\n"]}]},{"cell_type":"markdown","source":["# Check Stored Validation and Testing Results"],"metadata":{"id":"g3CHh_Bj8lhM"}},{"cell_type":"code","source":["#step5_1_1- NCF Model: Custom Embedding Full Dataset\n","results_dir = '/content/drive/MyDrive/bt4222_group_6/bt4222_group_6_amazon/Model Results'\n","cust_emb_full_data_val_results = pd.read_csv(os.path.join(results_dir,\"NCF Custom Embedding/Full Model/full_model_validation_results.csv\"))\n","print(cust_emb_full_data_val_results.shape)\n","print(\"==============step5_1_1- NCF Model: Custom Embedding Full Dataset Validation Results===============\")\n","display(cust_emb_full_data_val_results.head())\n","\n","\n","cust_emb_by_full_data_test_results = pd.read_csv(os.path.join(results_dir,\"NCF Custom Embedding/Full Model/full_model_testing_results.csv\"))\n","print(cust_emb_by_full_data_test_results.shape)\n","print(\"==============step5_1_1- NCF Model: Custom Embedding Full Dataset Testing Results===============\")\n","display(cust_emb_by_full_data_test_results.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"Pe4qe_NmWNYj","executionInfo":{"status":"ok","timestamp":1744707360157,"user_tz":-480,"elapsed":2581,"user":{"displayName":"BT4222Group6","userId":"06225424043284784520"}},"outputId":"38d3212f-6f16-4e9a-80d4-fc963b0df694"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 11)\n","==============step5_1_1- NCF Model: Custom Embedding Full Dataset Validation Results===============\n"]},{"output_type":"display_data","data":{"text/plain":["        mse     rmse   ndcg@10  precision@10  recall@10     f1@10  \\\n","0  0.892829  0.94476  0.996364      0.867375   0.876889  0.870588   \n","\n","   embedding_dim  learning_rate  batch_size  dropout_rate  num_epochs  \n","0              6          0.001         128           0.0          20  "],"text/html":["\n","  <div id=\"df-84147b58-991c-4437-a7b2-8c75f189e830\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mse</th>\n","      <th>rmse</th>\n","      <th>ndcg@10</th>\n","      <th>precision@10</th>\n","      <th>recall@10</th>\n","      <th>f1@10</th>\n","      <th>embedding_dim</th>\n","      <th>learning_rate</th>\n","      <th>batch_size</th>\n","      <th>dropout_rate</th>\n","      <th>num_epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.892829</td>\n","      <td>0.94476</td>\n","      <td>0.996364</td>\n","      <td>0.867375</td>\n","      <td>0.876889</td>\n","      <td>0.870588</td>\n","      <td>6</td>\n","      <td>0.001</td>\n","      <td>128</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84147b58-991c-4437-a7b2-8c75f189e830')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-84147b58-991c-4437-a7b2-8c75f189e830 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-84147b58-991c-4437-a7b2-8c75f189e830');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(cust_emb_by_full_data_test_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"mse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8928293709401731,\n        \"max\": 0.8928293709401731,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8928293709401731\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.94475996,\n        \"max\": 0.94475996,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.94475996\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ndcg@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9963641722515264,\n        \"max\": 0.9963641722515264,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9963641722515264\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8673749758242733,\n        \"max\": 0.8673749758242733,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8673749758242733\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.87688927358679,\n        \"max\": 0.87688927358679,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.87688927358679\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8705876769228104,\n        \"max\": 0.8705876769228104,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8705876769228104\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 20,\n        \"max\": 20,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["(1, 11)\n","==============step5_1_1- NCF Model: Custom Embedding Full Dataset Testing Results===============\n"]},{"output_type":"display_data","data":{"text/plain":["        mse      rmse   ndcg@10  precision@10  recall@10     f1@10  \\\n","0  1.859998  1.192249  0.971615      0.859671   0.934425  0.884954   \n","\n","   embedding_dim  learning_rate  batch_size  dropout_rate  num_epochs  \n","0              6          0.001         128           0.0          20  "],"text/html":["\n","  <div id=\"df-39d56597-5f31-45be-94cb-abbdc1b06df8\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>mse</th>\n","      <th>rmse</th>\n","      <th>ndcg@10</th>\n","      <th>precision@10</th>\n","      <th>recall@10</th>\n","      <th>f1@10</th>\n","      <th>embedding_dim</th>\n","      <th>learning_rate</th>\n","      <th>batch_size</th>\n","      <th>dropout_rate</th>\n","      <th>num_epochs</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.859998</td>\n","      <td>1.192249</td>\n","      <td>0.971615</td>\n","      <td>0.859671</td>\n","      <td>0.934425</td>\n","      <td>0.884954</td>\n","      <td>6</td>\n","      <td>0.001</td>\n","      <td>128</td>\n","      <td>0.0</td>\n","      <td>20</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39d56597-5f31-45be-94cb-abbdc1b06df8')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-39d56597-5f31-45be-94cb-abbdc1b06df8 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-39d56597-5f31-45be-94cb-abbdc1b06df8');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"display(cust_emb_by_full_data_test_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"mse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.859997579684624,\n        \"max\": 1.859997579684624,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.859997579684624\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rmse\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1.1922495,\n        \"max\": 1.1922495,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.1922495\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ndcg@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9716145157495414,\n        \"max\": 0.9716145157495414,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9716145157495414\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"precision@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8596708159755292,\n        \"max\": 0.8596708159755292,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8596708159755292\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"recall@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.9344245399089158,\n        \"max\": 0.9344245399089158,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.9344245399089158\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"f1@10\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.8849535203780803,\n        \"max\": 0.8849535203780803,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.8849535203780803\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding_dim\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"learning_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.001,\n        \"max\": 0.001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"batch_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 128,\n        \"max\": 128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dropout_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_epochs\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 20,\n        \"max\": 20,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          20\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{}}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPqyUNfmucogI3IIU8C5k/g"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}